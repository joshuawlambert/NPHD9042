---
title: "Guide to Writing Methods and Results for Statistical Analyses"
author: "Dr. Joshua Lambert"
output: html
format:
  html:
    toc: true
    toc-title: "Table of Contents"
    code-fold: true
    code-tools: true 
    css: styles.css
---

# Guide to Writing Methods and Results for Statistical Analyses

## Introduction

This guide provides general instructions on how to write the Methods and Results sections for various statistical methods used in academic research and provides a generic example of each. It includes tips and suggestions for Discriminant Analysis, MANOVA, Principal Component Analysis (PCA), Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), Path Analysis (PA), Structural Equation Modeling (SEM), Classification and Regression Trees (CART), and different variable selection techniques in regression.

## Discriminant Analysis

### Methods

Describe the sample and how data were collected, specifying the dependent (grouping) variable and the predictor variables.

#### Example

**Participants and Data Collection**: A total of 200 patients from the cardiology department were included in this study. Data were collected through structured interviews and medical records.

**Variables**: The dependent variable was patient risk category (low, medium, high). Predictor variables included age, blood pressure, cholesterol levels, and BMI.

**Procedure**: Discriminant analysis was conducted using SPSS to classify patients into risk categories. The stepwise method was used to select the most significant predictors.

### Results

Summarize the main findings, report the statistical output, and interpret the implications.

#### Example

**Summary of Findings**: The discriminant analysis revealed that blood pressure and cholesterol levels were significant predictors of patient risk category.

**Statistical Output**: The first canonical discriminant function had an eigenvalue of 2.34 and explained 75% of the variance. Wilks' Lambda was significant (Λ = 0.65, χ² = 42.56, p \< 0.001). The classification results showed that 85% of the patients were correctly classified.

**Interpretation**: These results suggest that blood pressure and cholesterol levels are critical factors in determining patient risk categories. This information can be used to improve risk assessment protocols.

**Limitations**: Discriminant analysis assumes that the predictor variables are normally distributed within each group, which may not always be the case with clinical data, potentially affecting the accuracy of the classification.

## MANOVA

### Methods

Describe the sample and how data were collected, specifying the dependent and independent variables.

#### Example

**Participants and Data Collection**: The study included 150 participants from three different treatment groups. Data were collected through clinical measurements and self-reported questionnaires.

**Variables**: The dependent variables were blood pressure, cholesterol levels, and heart rate. The independent variable was the treatment group.

**Procedure**: MANOVA was conducted using R to compare the multivariate means of the three treatment groups. Pillai's trace was used as the test statistic.

### Results

Summarize the main findings, report the test statistics, degrees of freedom, and p-values, and interpret the implications.

#### Example

**Summary of Findings**: The MANOVA revealed significant differences in the combined dependent variables across the treatment groups.

**Statistical Output**: Pillai's trace was significant (V = 0.45, F(6, 288) = 5.67, p \< 0.001). Follow-up ANOVAs indicated significant differences in blood pressure (F(2, 147) = 8.45, p \< 0.01), cholesterol levels (F(2, 147) = 6.78, p \< 0.01), and heart rate (F(2, 147) = 5.34, p \< 0.05).

**Interpretation**: These results suggest that the treatment groups differ significantly in their health outcomes. Further post-hoc analyses are needed to determine which groups differ from each other.

**Limitations**: MANOVA assumes homogeneity of variance-covariance matrices, which might not hold true for the clinical measurements, potentially affecting the validity of the results.

## Principal Component Analysis (PCA)

### Methods

Describe the sample and how data were collected, specifying the variables included in the PCA.

#### Example

**Participants and Data Collection**: Data were collected from 300 nursing students through a survey measuring various health indicators.

**Variables**: The variables included in the PCA were 20 health indicators such as physical activity, diet, stress levels, and sleep quality.

**Procedure**: PCA was conducted using Python to reduce the dimensionality of the dataset. The number of components was determined based on eigenvalues greater than 1 and the scree plot.

### Results

Summarize the main findings, report the explained variance, eigenvalues, and component loadings, and interpret the implications.

#### Example

**Summary of Findings**: The PCA identified four principal components that explained 65% of the total variance.

**Statistical Output**: The first component had an eigenvalue of 4.5 and explained 25% of the variance. The component loadings indicated that physical activity, diet, and sleep quality loaded highly on the first component.

**Interpretation**: These results suggest that physical activity, diet, and sleep quality are key factors influencing the health indicators of nursing students. Interventions targeting these areas may be effective in improving overall health.

**Limitations**: PCA assumes linear relationships among variables and may not capture complex, non-linear interactions present in health data, potentially oversimplifying the underlying structure.

## Exploratory Factor Analysis (EFA)

### Methods

Describe the sample and how data were collected, specifying the variables included in the EFA.

#### Example

**Participants and Data Collection**: The study included 250 patients from a mental health clinic. Data were collected through a comprehensive survey on mental health and well-being.

**Variables**: The variables included in the EFA were 15 survey items related to mental health.

**Procedure**: EFA was conducted using R with varimax rotation to identify the underlying factor structure. The number of factors was determined based on eigenvalues greater than 1 and the scree plot.

### Results

Summarize the main findings, report the factor loadings, eigenvalues, and explained variance, and interpret the implications.

#### Example

**Summary of Findings**: The EFA identified three factors that explained 70% of the total variance.

**Statistical Output**: The first factor had an eigenvalue of 5.2 and explained 35% of the variance. The factor loadings indicated that anxiety, depression, and stress loaded highly on the first factor.

**Interpretation**: These results suggest that anxiety, depression, and stress are closely related constructs in this patient population. Interventions targeting these factors may be beneficial in improving mental health outcomes.

**Limitations**: EFA requires a large sample size for stable results, which may be difficult to achieve in a clinical setting, potentially affecting the reliability of the factor structure identified.

## Confirmatory Factor Analysis (CFA)

### Methods

Describe the sample and how data were collected, specifying the variables included in the CFA.

#### Example

**Participants and Data Collection**: Data were collected from 400 respondents through a structured survey assessing various psychological constructs.

**Variables**: The variables included in the CFA were items from standardized questionnaires on anxiety, depression, and stress.

**Procedure**: CFA was conducted using R with the lavaan package. The factor structure was specified based on theoretical expectations and prior research.

### Results

Summarize the main findings, report the goodness-of-fit indices, and interpret the implications.

#### Example

**Summary of Findings**: The CFA model showed an acceptable fit to the data.

**Statistical Output**: The model fit indices were as follows: CFI = 0.95, RMSEA = 0.05, and χ²(120) = 250.34, p \< 0.001. The standardized factor loadings ranged from 0.60 to 0.85.

**Interpretation**: These results indicate that the specified factor structure is a good representation of the underlying constructs. The high factor loadings suggest that the observed variables are good indicators of their respective latent variables.

**Limitations**: CFA requires a large sample size for reliable estimates, which may not always be feasible in psychological research, potentially affecting the robustness of the model fit.

## Path Analysis (PA)

### Methods

Describe the sample and how data were collected, specifying the variables included in the path analysis.

#### Example

**Participants and Data Collection**: The study included 350 participants from a longitudinal health study. Data were collected through annual health assessments and surveys.

**Variables**: The variables included in the path analysis were health behaviors (diet, exercise), psychological factors (stress, anxiety), and health outcomes (BMI, blood pressure).

**Procedure**: Path analysis was conducted using Mplus to examine the direct and indirect effects of health behaviors and psychological factors on health outcomes.

### Results

Summarize the main findings, report the path coefficients, and interpret the implications.

#### Example

**Summary of Findings**: The path analysis revealed significant direct and indirect effects of health behaviors and psychological factors on health outcomes.

**Statistical Output**: The standardized path coefficients were as follows: diet → BMI (β = -0.25, p \< 0.01), exercise → blood pressure (β = -0.20, p \< 0.05), stress → anxiety (β = 0.30, p \< 0.001). The model fit indices indicated a good fit: CFI = 0.97, RMSEA = 0.04.

**Interpretation**: These results suggest that health behaviors and psychological factors have significant impacts on health outcomes. Interventions targeting these factors may help improve health metrics in the population.

**Limitations**: Path analysis assumes linear relationships between variables, which may not adequately capture the complexity of health behaviors and outcomes, potentially limiting the interpretation of direct and indirect effects.

## Structural Equation Modeling (SEM)

### Methods

Describe the sample and how data were collected, specifying the variables included in the SEM.

#### Example

**Participants and Data Collection**: The sample consisted of 500 participants from a community health survey. Data were collected through self-reported questionnaires and clinical measurements.

**Variables**: The SEM included observed variables such as diet, exercise, stress, and health outcomes (BMI, blood pressure) as well as latent variables representing overall health and psychological well-being.

**Procedure**: SEM was conducted using R with the lavaan package. The model was specified based on theoretical expectations and prior research.

### Results

Summarize the main findings, report the goodness-of-fit indices, and interpret the implications.

#### Example

**Summary of Findings**: The SEM provided a good fit to the data, with significant path coefficients indicating the relationships among the variables.

**Statistical Output**: The model fit indices were as follows: CFI = 0.96, RMSEA = 0.05, and χ²(200) = 300.45, p \< 0.001. The standardized path coefficients ranged from 0.20 to 0.45.

**Interpretation**: These results support the hypothesized model, suggesting that both health behaviors and psychological well-being significantly influence health outcomes. These findings highlight the importance of integrated health interventions.

**Limitations**: SEM requires large sample sizes for accurate estimates, which may not always be available in community health surveys, potentially impacting the stability of the parameter estimates.

## Classification and Regression Trees (CART)

### Methods

Describe the sample and how data were collected, specifying the variables included in the CART analysis.

#### Example

**Participants and Data Collection**: The study involved 600 patients from a diabetes clinic. Data were collected through clinical records and follow-up visits.

**Variables**: The CART analysis included variables such as age, BMI, blood sugar levels, and lifestyle factors (diet, physical activity).

**Procedure**: CART was conducted using Python with the scikit-learn library. The decision tree was trained to predict diabetes status based on the predictor variables.

### Results

Summarize the main findings, report the structure of the decision tree, and interpret the implications.

#### Example

**Summary of Findings**: The CART model identified key predictors of diabetes status and provided a decision tree for classification.

**Statistical Output**: The decision tree had a depth of 4 and included variables such as blood sugar levels, BMI, and age as primary split criteria. The model accuracy was 85%.

**Interpretation**: These results suggest that blood sugar levels, BMI, and age are significant predictors of diabetes status. The decision tree model can be used for early identification and intervention in at-risk patients.

**Limitations**: CART can be prone to overfitting, especially with small datasets, which may limit its generalizability to new data. Pruning is required to avoid overly complex trees.

## Variable Selection Techniques in Regression

### Methods

Describe the sample and how data were collected, specifying the dependent and independent variables included in the regression analysis.

#### Example

**Participants and Data Collection**: The sample included 400 participants from a cardiovascular health study. Data were collected through medical records and lifestyle questionnaires.

**Variables**: The dependent variable was recovery time. The independent variables included age, severity of condition, treatment type, gender, and comorbidities.

**Procedure**: The variable selection technique (forward selection, backward selection, and stepwise selection) was used to identify the most significant predictors of recovery time.

### Results

Summarize the main findings, report the selected variables, and interpret the implications for each technique.

#### Examples

##### Forward Selection

**Summary of Findings**: Forward selection identified age, severity of condition, and treatment type as significant predictors.

**Statistical Output**: The final model included age (β = 0.30, p \< 0.01), severity (β = 0.25, p \< 0.01), and treatment (β = -0.20, p \< 0.05). The model R² was 0.60.

**Interpretation**: These results indicate that age, severity of condition, and treatment type are important predictors of recovery time.

**Limitations**: Forward selection can miss important variables that only show their effect in combination with others. It may also include variables that are not truly significant due to random chance, leading to potential overfitting.

##### Backward Selection

**Summary of Findings**: Backward selection identified severity of condition, age, and comorbidities as significant predictors.

**Statistical Output**: The final model included severity (β = 0.35, p \< 0.001), age (β = 0.20, p \< 0.05), and comorbidities (β = 0.15, p \< 0.05). The model R² was 0.65.

**Interpretation**: These results highlight the importance of severity, age, and comorbidities in predicting recovery time.

**Limitations**: Backward selection can be computationally intensive and may remove variables that are important in combination with others. It can also be sensitive to the initial set of predictors, leading to different final models if starting with a different set.

##### Stepwise Selection

**Summary of Findings**: Stepwise selection identified age, severity, treatment, and comorbidities as significant predictors.

**Statistical Output**: The final model included age (β = 0.25, p \< 0.01), severity (β = 0.30, p \< 0.01), treatment (β = -0.15, p \< 0.05), and comorbidities (β = 0.10, p \< 0.05). The model R² was 0.70.

**Interpretation**: These results suggest that a combination of age, severity, treatment type, and comorbidities are important predictors of recovery time.

**Limitations**: Stepwise selection can be prone to overfitting, especially in small datasets. It may also be unstable, meaning small changes in the data can result in different models. Additionally, it can be computationally intensive and may not always provide the most parsimonious model.

##### LASSO

**Summary of Findings**: LASSO regression identified age, severity of condition, and treatment type as significant predictors.

**Statistical Output**: The final model included age (β = 0.28), severity (β = 0.35), and treatment (β = -0.22). The model's cross-validated mean squared error (MSE) was 0.12.

**Interpretation**: These results suggest that age, severity of condition, and treatment type are important predictors of recovery time. The LASSO regression model's ability to shrink less important variable coefficients to zero highlights its usefulness in handling high-dimensional data with many predictors.

**Limitations**: LASSO regression may not perform well if the true relationship between the predictors and the outcome is not linear. It can also be sensitive to the choice of the regularization parameter (lambda), which requires careful selection. Additionally, LASSO may struggle with highly correlated predictors, as it tends to select one and ignore the others.
