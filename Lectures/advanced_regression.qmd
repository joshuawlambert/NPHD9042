---
title: "Model Building and Advanced Linear and Logistic Regression"
author: "Dr. Joshua Lambert"
output: html
format:
  html:
    toc: true
    toc-title: "Table of Contents"
    code-fold: true
    code-tools: true 
    css: styles.css
---

{{< video https://youtu.be/QpMHKd6vlm0 >}}

{{< video https://youtu.be/oSSbnchCcpE >}}

# Model Building and Advanced Linear and Logistic Regression

Understanding the fundamentals and advanced topics of multiple linear and logistic regression analysis is crucial for nursing research. This page will cover the definitions, key concepts, assumptions, and techniques required to build robust models in these contexts.

## Definition and Purpose of Multiple Linear and Logistic Regression Analysis

### Multiple Linear Regression

-   **Definition**: A statistical technique that models the relationship between a dependent variable and two or more independent variables by fitting a linear equation.
-   **Purpose**: To predict the value of the dependent variable based on the values of the independent variables and to understand the relationship between them.

### Logistic Regression

-   **Definition**: A regression model where the dependent variable is categorical, typically binary.
-   **Purpose**: To model the probability of a binary outcome based on one or more predictor variables.

## Key Assumptions and Limitations

### Multiple Linear Regression Assumptions

-   **Linearity**: The relationship between the dependent and independent variables should be linear.
-   **Independence**: Observations should be independent of each other.
-   **Homoscedasticity**: The variance of error terms should be constant across all levels of the independent variables.
-   **Normality**: The residuals (errors) should be approximately normally distributed.
-   **Absence of Multicollinearity**: Predictors should not be too highly correlated with each other.
-   **Independence**: Observations should be independent.
### Logistic Regression Assumptions

-   **Linearity of Logit**: The logit of the outcome should have a linear relationship with the predictor variables.
-   **Independence**: Observations should be independent.
-   **Absence of Multicollinearity**: Predictors should not be too highly correlated with each other.

## Dependent Variable

-   **Multiple Linear Regression**: A continuous variable (e.g., blood pressure, cholesterol level).
-   **Logistic Regression**: A categorical variable (e.g., presence or absence of a disease).

## Independent Variables

-   **Definition**: Variables that predict or explain the dependent variable. Can be continuous or categorical.
-   **Examples in Nursing Research**: Age, weight, treatment type, comorbidities.

## Variable Selection Techniques

### Forward Selection

![](https://quantifyinghealth.com/wp-content/uploads/2019/10/forward-stepwise-algorithm.png)

-   **Method**: Starts with no predictors and adds them one by one based on a specified criterion (e.g., p-value).
-   **Process**:
    -   Begin with an empty model.
    -   Add the predictor with the best criterion (e.g. lowest p-value).
    -   Continue adding predictors one at a time, based on criterion, until no additional predictors meet the criterion.
-   **Advantages**: Simple and easy to understand.
-   **Disadvantages**: Can miss important variables that only show their effect in combination with others. Using p-values to add variables can lead to incorrect model specification. Other methods like AIC, or BIC can perform better.

### Backward Selection

![](https://quantifyinghealth.com/wp-content/uploads/2019/10/backward-stepwise-algorithm.png)

-   **Method**: Starts with all candidate predictors and removes them one by one based on a specified criterion.
-   **Process**:
    -   Begin with the full model.
    -   Remove the predictor with the worst criterion.
    -   Continue removing predictors until all remaining predictors meet criterion.
-   **Advantages**: Considers the full model from the start.
-   **Disadvantages**: Computationally intensive for large set of candidate predictors.

### Stepwise Selection

-   **Method**: A combination of forward and backward selection.
-   **Process**:
    -   Begin with an empty model or a model with a subset of predictors.
    -   Add predictors based on criterion and remove predictors whose criterion no longer meets standard.
    -   Continue until no predictors can be added or removed.
-   **Advantages**: More flexible and can result in a better model.
-   **Disadvantages**: Prone to overfitting and can be unstable.

### Lasso (Least Absolute Shrinkage and Selection Operator)

![](https://statmodeling.stat.columbia.edu/wp-content/uploads/2013/03/Screen-Shot-2013-03-17-at-10.43.11-PM.png)

-   **Method**: Performs both variable selection and regularization to enhance the prediction accuracy and interpretability.
-   **Process**:
    -   Adds a penalty to the size of the coefficients, shrinking some to zero and thus performing variable selection.
    -   The amount of shrinkage is controlled by a tuning parameter (lambda).
-   **Advantages**: Can handle large sets of predictors and reduces overfitting.
-   **Disadvantages**: Requires careful selection of the regularization parameter. Use cross validation to select.

### Multi-Step (Hierarchical Regression)

![](https://www.researchgate.net/publication/287965810/figure/tbl1/AS:647170898284545@1531308974339/Hierarchical-regression-analysis-table.png)

-   **Method**: Involves entering predictors into the regression model in steps based on theoretical justification.
-   **Process**:
    -   Enter variables in a pre-specified order, typically based on theoretical importance.
    -   Assess the contribution of each variable or set of variables at each step.
-   **Advantages**: Allows testing the incremental value of adding new predictors.
-   **Disadvantages**: Can be complex to implement and interpret.
-   **Confusion with Hierarchical Models**:
    -   Hierarchical *regression* involves adding variables in steps to assess their incremental value.
    -   Hierarchical *models* (also known as multilevel models) account for data that is nested within higher-level units (e.g., patients within hospitals).
    -   Despite similar names, hierarchical regression and hierarchical models address different analytical needs.

## Understanding Coefficients

-   **Interpretation**: Coefficients represent the change in the dependent variable for a one-unit change in the predictor variable, holding other variables constant.
-   **Logistic Regression**: Coefficients are in terms of the log odds of the outcome. Once exponentiated, the coefficientsare in terms of change in odds of outcome.

## Significance and Confidence Intervals

-   **P-Values**: Indicate whether there is statistical evidence that the population parameter is significantly different from zero.
-   **Confidence Intervals**: Provide a range of values within which the true population parameter is expected to fall. Based on a defined confidence level (e.g. 95%)

## Common Criterion: R-Squared and Adjusted R-Squared, and Other Model Fit Statistics

### Multiple Linear Regression

-   **R-Squared**: Measures the proportion of variability in the dependent variable that is explained by the independent variables. Problem is that as the number of variables increase the R-Squared will always increase.
-   **Adjusted R-Squared**: Adjusts R-squared for the number of predictors in the model. Addresses problem previously mentioned.
-   **AIC/BIC (Akaike Information Criterion/Bayesian Information Criterion)**: Used for model comparison, with lower values indicating a better fit and quality.

### Logistic Regression

-   **Pseudo R-Squared**: Analogous to R-squared in linear regression, but measures the goodness of fit for logistic models. Problem is that as the number of variables increase the R-Squared will always increase. Adjusted Pseudo R-Squared do exist.
-   **AIC/BIC (Akaike Information Criterion/Bayesian Information Criterion)**: Used for model comparison, with lower values indicating a better fit and quality.

## Residual Analysis

### Normality of Residuals

-   **Importance**: Ensures the validity of hypothesis tests and confidence intervals.
-   **Assessment**: Histogram, Q-Q plot.

### Homoscedasticity

-   **Importance**: Ensures that the residuals have constant variance, which is an assumption of linear regression.
-   **Assessment**: Residuals vs. fitted values plot.

## Diagnostic Plots

-   **Residual Plots**: Help assess the assumptions of linearity, homoscedasticity, and independence.
-   **Influence Plots**: Identify influential data points that can disproportionately affect the model.

## Examples of Model Building in Multiple Linear and Logistic Regression

### Multiple Linear Regression Example

-   **Scenario**: Predicting calories of common breakfast cereal based on nutritional information.
-   Use the <a href="Cereal.csv" download>Cereal datatset to run the multiple linear regression.</a>

### Logistic Regression Example

-   **Scenario**: Predicting the likelihood of domestic vehicle manufacture based on vehicle characteristics.
-   Use the <a href="Cars 1993.csv" download> Cars 1993.csv</a> dataset to run the logistic regression. This dataset contains information on 93 cars from the year 1993.

## Software Examples

### Forward Selection

<details>

<summary>JMP Instructions:</summary>

<ul>

<li>Go to <code>Analyze</code> \> <code>Fit Model</code>.</li>

<li>Select your dependent variable and move it to the <code>Y</code> box.</li>

<li>Select your independent variables and move them to the <code>Construct Model Effects</code> box.</li>

<li>Click on the <code>Stepwise</code> button.</li>

<li>In the Stepwise Options dialog, set the <code>Direction</code> to <code>Forward</code>.</li>

<li>Specify the stopping criterion (e.g., p-value threshold.</li>

<li>Click <code>Run Model</code> to perform the forward selection.</li>

</ul>

</details>

<details>

<summary>R Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
        #Forward selection for MLR
# Load necessary libraries
library(MASS)  

# Load dataset
cars1993<-read_csv("Cars 1993.csv")

#Build the full model
full_model <- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`+ `Passenger Capacity`, data = cars1993)

# Forward stepwise selection
step_model <- stepAIC(lm(`Minimum Price ($1000)` ~ 1, data = cars1993), 
                      direction = "forward", 
                      scope = list(lower = ~1, upper = full_model))

# Print summary of the final model selected by forward selection
summary(step_model)
      </pre>
```
</li>

</ul>

</details>

<details>

<summary>Python Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LinearRegression
import numpy as np

# Load iris data
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Feature selection using Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=0)
sfm = SelectFromModel(rf, threshold='median')
X_train_selected = sfm.fit_transform(X_train, y_train)

# Get the selected feature indices
selected_feature_indices = sfm.get_support(indices=True)

# Get the selected feature names
selected_features = X_train.columns[selected_feature_indices]

def forward_selection(X_train, y_train, selected_features):
    remaining_features = set(selected_features)
    current_score = float('inf')
    best_new_score = float('inf')
    selected_features_list = [] # Use a list to store selected feature names
    
    while remaining_features:
        scores_with_candidates = []
        for candidate in remaining_features:
            # Add the candidate feature to the selected features
            selected_features_list.append(candidate)
            
            # Fit a model with the selected features
            # Use the list of selected feature names to index X_train
            model = LinearRegression().fit(X_train[selected_features_list], y_train) 
            
            # Calculate the AIC score
            RSS = sum((model.predict(X_train[selected_features_list]) - y_train) ** 2)
            n = len(X_train)
            p = len(selected_features_list)
            score = n * np.log(RSS / n) + 2 * p
            
            # Store the score and the candidate feature
            scores_with_candidates.append((score, candidate))
            
            # Remove the candidate feature from the selected features
            selected_features_list.pop()
        
        # Sort the scores and select the best candidate
        scores_with_candidates.sort()
        best_new_score, best_candidate = scores_with_candidates[0]
        
        # If the new score is better than the current score, update the current score and add the best candidate to the selected features
        if current_score > best_new_score:
            current_score = best_new_score
            selected_features_list.append(best_candidate) # Append to the list
            remaining_features.remove(best_candidate)
    
    # Fit a model with the selected features
    # Index X_train with the list of selected feature names
    model = LinearRegression().fit(X_train[selected_features_list], y_train) 
    
    return model, selected_features_list # Return both the model and the list of selected features

# Run forward selection
# Pass the list of selected feature names to the function
model, selected_features_list = forward_selection(X_train, y_train, selected_features) 

# Print the summary of the model
print("Selected Features:", model.coef_)

# Print the selected feature column names
print("Selected Features:")
for feature in selected_features_list: # Iterate over the list of selected features
    print(feature)
      </pre>
```
</li>

</ul>

</details>

<details>

<summary>STATA Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
        // Forward Selection in STATA
        stepwise, pr(0.05): regress recovery_time age severity treatment gender comorbidities
      </pre>
```
</li>

</ul>

</details>

### Backward Selection

<details>

<summary>JMP Instructions:</summary>

<ul>

<li>Go to <code>Analyze</code> \> <code>Fit Model</code>.</li>

<li>Select your dependent variable and move it to the <code>Y</code> box.</li>

<li>Select your independent variables and move them to the <code>Construct Model Effects</code> box.</li>

<li>Click on the <code>Stepwise</code> button.</li>

<li>In the Stepwise Options dialog, set the <code>Direction</code> to <code>Backward</code>.</li>

<li>Specify the stopping criterion (e.g., p-value threshold) and click <code>OK</code>.</li>

<li>Click <code>Run</code> to perform the backward selection.</li>

</ul>

</details>

<details>

<summary>R Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
        #Backward selection with MLR
# Load necessary libraries
library(MASS)  

# Load dataset
cars1993<-read_csv("Cars 1993.csv")

#Build the full model
full_model <- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`+ `Passenger Capacity`, data = cars1993)

# Backward stepwise selection
step_model <- stepAIC(full_model, direction = "backward")

# Print summary of the final model selected by backward selection
summary(step_model)
  
      </pre>
```
</li>

</ul>

</details>

<details>

<summary>Python Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LinearRegression
import numpy as np
from sklearn.linear_model import Lasso

# Load iris data
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)


def backward_selection(X_train, y_train):
    remaining_features = set(X_train.columns)
    current_score = float('inf')
    selected_features_list = list(X_train.columns)
    
    # Change the model to Lasso for feature selection
    model = Lasso(alpha=0.01)
    
    while len(remaining_features) > 1:
        scores_with_candidates = []
        for candidate in remaining_features:
            # Temporarily remove the candidate feature from the selected features
            selected_features_list.remove(candidate)
            
            # Fit the Lasso model with the remaining features
            model.fit(X_train[selected_features_list], y_train)
            
            # Calculate the AIC score
            RSS = sum((model.predict(X_train[selected_features_list]) - y_train) ** 2)
            n = len(X_train)
            p = len(selected_features_list)
            score = n * np.log(RSS / n) + 2 * p
            
            # Store the score and the candidate feature
            scores_with_candidates.append((score, candidate))
            
            # Add the candidate feature back to the selected features
            selected_features_list.append(candidate)
        
        # Sort the scores in ascending order
        scores_with_candidates.sort()
        
        # Select the candidate feature with the highest score
        _, worst_candidate = scores_with_candidates.pop(0)
        
        # If removing the worst candidate improves the score, remove it
        if scores_with_candidates[-1][0] < current_score: 
            selected_features_list.remove(worst_candidate)
            remaining_features.remove(worst_candidate)
            current_score = scores_with_candidates[-1][0]
        else:
            break  # Early stopping if improvement becomes negligible
    
    # Fit the final Lasso model with the remaining features
    model.fit(X_train[selected_features_list], y_train)
    
    return model, selected_features_list

# Run backward selection
model, selected_features_list = backward_selection(X_train, y_train)

# Get the names of the selected features
selected_feature_names = selected_features_list

# Get the coefficients of the selected features
selected_feature_coefficients = model.coef_

# Print the summary of the selected features and their coefficients
for feature_name, coefficient in zip(selected_feature_names, selected_feature_coefficients):
    print("Feature:", feature_name, "| Coefficient:", coefficient)
      </pre>
```
</li>

</ul>

</details>

<details>

<summary>STATA Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
        // Backward Selection in STATA
        stepwise, pr(0.05) backward: regress recovery_time age severity treatment gender comorbidities
      </pre>
```
</li>

</ul>

</details>

### Stepwise Selection

<details>

<summary>JMP Instructions:</summary>

<ul>

<li>Go to <code>Analyze</code> \> <code>Fit Model</code>.</li>

<li>Select your dependent variable and move it to the <code>Y</code> box.</li>

<li>Select your independent variables and move them to the <code>Construct Model Effects</code> box.</li>

<li>Click on the <code>Stepwise</code> button.</li>

<li>In the Stepwise Options dialog, set the <code>Direction</code> to <code>Mixed</code> (for both forward and backward steps).</li>

<li>Specify the stopping criterion (e.g., p-value threshold) and click <code>OK</code>.</li>

<li>Click <code>Run</code> to perform the stepwise selection.</li>

</ul>

</details>

<details>

<summary>R Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
        # Stepwise Selection in R
        library(MASS)
        
        # Full model
        full_model <- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`+ `Passenger Capacity`, data = cars1993)
        
        # Stepwise selection
        step_model <- stepAIC(full_model, direction = "both")
        
        summary(step_model)
      </pre>
```
</li>

</ul>

</details>

<details>

<summary>Python Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
      
 # Stepwise Selection in Python
import pandas as pd
import statsmodels.api as sm
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load Iris data
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Add the target variable to the training data
train_data = X_train.copy()
train_data['target'] = y_train

# Stepwise selection
def stepwise_selected(data, response):
    def calculate_aic(model):
        return sm.OLS(data[response], model).fit().aic

    predictors = list(data.columns)
    predictors.remove(response)
    selected = []
    aic_current = float('inf')
    while predictors:
        aic_with_candidates = []
        for candidate in predictors:
            model = sm.add_constant(data[selected + [candidate]])
            aic = calculate_aic(model)
            aic_with_candidates.append((aic, candidate))
        aic_with_candidates.sort()
        best_aic, best_candidate = aic_with_candidates[0]
        if aic_current > best_aic:
            predictors.remove(best_candidate)
            selected.append(best_candidate)
            aic_current = best_aic
        else:
            break

        # Check if there are selected features before attempting to remove any
        if selected:
            remaining = list(set(data.columns) - set(selected) - {response})
            for candidate in remaining:
                # Check if the candidate is still in the selected list
                if candidate in selected: 
                    model = sm.add_constant(data[selected].drop([candidate], axis=1))
                    aic = calculate_aic(model)
                    if aic < aic_current:
                        selected.remove(candidate)
                        aic_current = aic

    final_model = sm.OLS(data[response], sm.add_constant(data[selected])).fit()
    return final_model

# Apply stepwise selection to Iris dataset
selected_model = stepwise_selected(train_data, 'target')
print(selected_model.summary())
      </pre>
```
</li>

</ul>

</details>

<details>

<summary>STATA Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
        // Stepwise Selection in STATA
        stepwise, pr(0.05): regress recovery_time age severity treatment gender comorbidities
      </pre>
```
</li>

</ul>

</details>

### Lasso

<details>

<summary>JMP Instructions:</summary>

<ul>

<li>Go to <code>Analyze</code> \> <code>Fit Model</code>.</li>

<li>Select your dependent variable and move it to the <code>Y</code> box.</li>

<li>Select your independent variables and move them to the <code>Construct Model Effects</code> box.</li>

<li>Click on the <code>Personality</code> dropdown and select <code>Lasso</code>.</li>

<li>Specify the tuning parameter (lambda) or use cross-validation to select it.</li>

<li>Click <code>Run</code> to fit the lasso model.</li>

</ul>

</details>

<details>

<summary>R Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
       # Install glmnet package if not already installed
install.packages("glmnet")

# Lasso in R
library(glmnet)
        
# Prepare data
  data_2<-read.csv("Cars 1993.csv")
X <- model.matrix(`Minimum.Price...1000.` ~ `Maximum.Horsepower` + `Fuel.Tank.Capacity` + `Passenger.Capacity`, data = data_2)[, -1]
y <- data_2$`Minimum.Price...1000.`
        
#Fit LASSO using cross-validation
set.seed(123) #For reproducibility
cv_fit <- cv.glmnet(X, y, alpha = 1)

#Plot the cross-validation
plot(cv_fit)

#Extract the coefficients of the best model
best_lambda <- cv_fit$lambda.min
lasso_coefficients <- coef(cv_fit, s = "lambda.min")
print(lasso_coefficients)

      </pre>
```
</li>

</ul>

</details>

<details>

<summary>Python Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
        
        
        # Lasso in Python
       import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import Lasso

# Load Iris data
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Perform LASSO with cross-validation to find the best alpha
lasso = Lasso(max_iter=10000)
param_grid = {'alpha': np.logspace(-4, 0, 100)}  # Test alpha values from 0.0001 to 1
lasso_cv = GridSearchCV(lasso, param_grid, cv=5)
lasso_cv.fit(X_train, y_train)

# Get the best alpha value and the corresponding model
best_alpha = lasso_cv.best_params_['alpha']
best_model = lasso_cv.best_estimator_

# Get the selected features based on the best model
selected_features = X_train.columns[best_model.coef_ != 0]

# Print the best alpha value and selected features
print("Best alpha:", best_alpha)
print("Selected features:", selected_features)
      </pre>
```
</li>

</ul>

</details>

<details>

<summary>STATA Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
        // Lasso in STATA
        lassoreg recovery_time age severity treatment gender comorbidities
      </pre>
```
</li>

</ul>

</details>

### Multi-Step (Hierarchical Regression)

<details>

<summary>JMP Instructions:</summary>

<ul>

<li>Go to <code>Analyze</code> \> <code>Fit Model</code>.</li>

<li>Select your dependent variable and move it to the <code>Y</code> box.</li>

<li>Select the first set of independent variables and move them to the <code>Construct Model Effects</code> box.</li>

<li>Click <code>Run</code> to fit the first model.</li>

<li>To add more variables, go to <code>Redo</code> \> <code>Modeling</code> \> <code>Add Predictors</code>.</li>

<li>Add the next set of variables and click <code>Run</code> again.</li>

<li>Repeat the process until all variables are included, fitting the model in steps.</li>

</ul>

</details>

<details>

<summary>R Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
        # Multi-Step (Hierarchical Regression) in R
        # Step 1
        model1<- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower`, data = cars1993)
        
        # Step 2
        model2 <- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`, data = cars1993)
        
        # Step 3
        model3 <- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`+ `Passenger Capacity`, data = cars1993)
        
        # Compare models
        summary(model1)
        summary(model2)
        summary(model3)
        
        #ANOVA of models
        anova(model1, model2, model3)

      </pre>
```
</li>

</ul>

</details>

<details>

<summary>Python Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
    
# Multi-Step (Hierarchical Regression) in Python
import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Load Iris data
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Step 1: Regression on a subset of predictors
predictors_step1 = ['sepal length (cm)', 'sepal width (cm)']
X_train_step1 = sm.add_constant(X_train[predictors_step1])
model_step1 = sm.OLS(y_train, X_train_step1)
results_step1 = model_step1.fit()
y_pred_step1 = results_step1.predict(sm.add_constant(X_test[predictors_step1]))
mse_step1 = mean_squared_error(y_test, y_pred_step1)
print("Model Summary - Step 1:")
print(results_step1.summary())
print("Mean Squared Error - Step 1:", mse_step1)

# Step 2: Regression with additional predictors
predictors_step2 = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)']
X_train_step2 = sm.add_constant(X_train[predictors_step2])
model_step2 = sm.OLS(y_train, X_train_step2)
results_step2 = model_step2.fit()
y_pred_step2 = results_step2.predict(sm.add_constant(X_test[predictors_step2]))
mse_step2 = mean_squared_error(y_test, y_pred_step2)
print("Model Summary - Step 2:")
print(results_step2.summary())
print("Mean Squared Error - Step 2:", mse_step2)

# Step 3: Regression with all predictors
X_train_step3 = sm.add_constant(X_train)
model_step3 = sm.OLS(y_train, X_train_step3)
results_step3 = model_step3.fit()
y_pred_step3 = results_step3.predict(sm.add_constant(X_test))
mse_step3 = mean_squared_error(y_test, y_pred_step3)
print("Model Summary - Step 3:")
print(results_step3.summary())
print("Mean Squared Error - Step 3:", mse_step3)


# COMPARING MSE
print("Mean Squared Error - Step 1:", mse_step1)
print("Mean Squared Error - Step 2:", mse_step2)
print("Mean Squared Error - Step 3:", mse_step3)
      </pre>
```
</li>

</ul>

</details>

<details>

<summary>STATA Code Example:</summary>

<ul>

<li>

```{=html}
<pre>
        // Multi-Step (Hierarchical Regression) in STATA
        // Step 1
        regress recovery_time age gender
        
        // Step 2
        regress recovery_time age gender severity
        
        // Step 3
        regress recovery_time age gender severity treatment comorbidities
      </pre>
```
</li>

</ul>

</details>
