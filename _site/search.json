[
  {
    "objectID": "Lectures/writing.html",
    "href": "Lectures/writing.html",
    "title": "Guide to Writing Methods and Results for Statistical Analyses",
    "section": "",
    "text": "This guide provides general instructions on how to write the Methods and Results sections for various statistical methods used in academic research and provides a generic example of each. It includes tips and suggestions for Discriminant Analysis, MANOVA, Principal Component Analysis (PCA), Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), Path Analysis (PA), Structural Equation Modeling (SEM), Classification and Regression Trees (CART), and different variable selection techniques in regression.\n\n\n\n\n\nDescribe the sample and how data were collected, specifying the dependent (grouping) variable and the predictor variables.\n\n\nParticipants and Data Collection: A total of 200 patients from the cardiology department were included in this study. Data were collected through structured interviews and medical records.\nVariables: The dependent variable was patient risk category (low, medium, high). Predictor variables included age, blood pressure, cholesterol levels, and BMI.\nProcedure: Discriminant analysis was conducted using SPSS to classify patients into risk categories. The stepwise method was used to select the most significant predictors.\n\n\n\n\nSummarize the main findings, report the statistical output, and interpret the implications.\n\n\nSummary of Findings: The discriminant analysis revealed that blood pressure and cholesterol levels were significant predictors of patient risk category.\nStatistical Output: The first canonical discriminant function had an eigenvalue of 2.34 and explained 75% of the variance. Wilks’ Lambda was significant (Λ = 0.65, χ² = 42.56, p &lt; 0.001). The classification results showed that 85% of the patients were correctly classified.\nInterpretation: These results suggest that blood pressure and cholesterol levels are critical factors in determining patient risk categories. This information can be used to improve risk assessment protocols.\nLimitations: Discriminant analysis assumes that the predictor variables are normally distributed within each group, which may not always be the case with clinical data, potentially affecting the accuracy of the classification.\n\n\n\n\n\n\n\nDescribe the sample and how data were collected, specifying the dependent and independent variables.\n\n\nParticipants and Data Collection: The study included 150 participants from three different treatment groups. Data were collected through clinical measurements and self-reported questionnaires.\nVariables: The dependent variables were blood pressure, cholesterol levels, and heart rate. The independent variable was the treatment group.\nProcedure: MANOVA was conducted using R to compare the multivariate means of the three treatment groups. Pillai’s trace was used as the test statistic.\n\n\n\n\nSummarize the main findings, report the test statistics, degrees of freedom, and p-values, and interpret the implications.\n\n\nSummary of Findings: The MANOVA revealed significant differences in the combined dependent variables across the treatment groups.\nStatistical Output: Pillai’s trace was significant (V = 0.45, F(6, 288) = 5.67, p &lt; 0.001). Follow-up ANOVAs indicated significant differences in blood pressure (F(2, 147) = 8.45, p &lt; 0.01), cholesterol levels (F(2, 147) = 6.78, p &lt; 0.01), and heart rate (F(2, 147) = 5.34, p &lt; 0.05).\nInterpretation: These results suggest that the treatment groups differ significantly in their health outcomes. Further post-hoc analyses are needed to determine which groups differ from each other.\nLimitations: MANOVA assumes homogeneity of variance-covariance matrices, which might not hold true for the clinical measurements, potentially affecting the validity of the results.\n\n\n\n\n\n\n\nDescribe the sample and how data were collected, specifying the variables included in the PCA.\n\n\nParticipants and Data Collection: Data were collected from 300 nursing students through a survey measuring various health indicators.\nVariables: The variables included in the PCA were 20 health indicators such as physical activity, diet, stress levels, and sleep quality.\nProcedure: PCA was conducted using Python to reduce the dimensionality of the dataset. The number of components was determined based on eigenvalues greater than 1 and the scree plot.\n\n\n\n\nSummarize the main findings, report the explained variance, eigenvalues, and component loadings, and interpret the implications.\n\n\nSummary of Findings: The PCA identified four principal components that explained 65% of the total variance.\nStatistical Output: The first component had an eigenvalue of 4.5 and explained 25% of the variance. The component loadings indicated that physical activity, diet, and sleep quality loaded highly on the first component.\nInterpretation: These results suggest that physical activity, diet, and sleep quality are key factors influencing the health indicators of nursing students. Interventions targeting these areas may be effective in improving overall health.\nLimitations: PCA assumes linear relationships among variables and may not capture complex, non-linear interactions present in health data, potentially oversimplifying the underlying structure.\n\n\n\n\n\n\n\nDescribe the sample and how data were collected, specifying the variables included in the EFA.\n\n\nParticipants and Data Collection: The study included 250 patients from a mental health clinic. Data were collected through a comprehensive survey on mental health and well-being.\nVariables: The variables included in the EFA were 15 survey items related to mental health.\nProcedure: EFA was conducted using R with varimax rotation to identify the underlying factor structure. The number of factors was determined based on eigenvalues greater than 1 and the scree plot.\n\n\n\n\nSummarize the main findings, report the factor loadings, eigenvalues, and explained variance, and interpret the implications.\n\n\nSummary of Findings: The EFA identified three factors that explained 70% of the total variance.\nStatistical Output: The first factor had an eigenvalue of 5.2 and explained 35% of the variance. The factor loadings indicated that anxiety, depression, and stress loaded highly on the first factor.\nInterpretation: These results suggest that anxiety, depression, and stress are closely related constructs in this patient population. Interventions targeting these factors may be beneficial in improving mental health outcomes.\nLimitations: EFA requires a large sample size for stable results, which may be difficult to achieve in a clinical setting, potentially affecting the reliability of the factor structure identified.\n\n\n\n\n\n\n\nDescribe the sample and how data were collected, specifying the variables included in the CFA.\n\n\nParticipants and Data Collection: Data were collected from 400 respondents through a structured survey assessing various psychological constructs.\nVariables: The variables included in the CFA were items from standardized questionnaires on anxiety, depression, and stress.\nProcedure: CFA was conducted using R with the lavaan package. The factor structure was specified based on theoretical expectations and prior research.\n\n\n\n\nSummarize the main findings, report the goodness-of-fit indices, and interpret the implications.\n\n\nSummary of Findings: The CFA model showed an acceptable fit to the data.\nStatistical Output: The model fit indices were as follows: CFI = 0.95, RMSEA = 0.05, and χ²(120) = 250.34, p &lt; 0.001. The standardized factor loadings ranged from 0.60 to 0.85.\nInterpretation: These results indicate that the specified factor structure is a good representation of the underlying constructs. The high factor loadings suggest that the observed variables are good indicators of their respective latent variables.\nLimitations: CFA requires a large sample size for reliable estimates, which may not always be feasible in psychological research, potentially affecting the robustness of the model fit.\n\n\n\n\n\n\n\nDescribe the sample and how data were collected, specifying the variables included in the path analysis.\n\n\nParticipants and Data Collection: The study included 350 participants from a longitudinal health study. Data were collected through annual health assessments and surveys.\nVariables: The variables included in the path analysis were health behaviors (diet, exercise), psychological factors (stress, anxiety), and health outcomes (BMI, blood pressure).\nProcedure: Path analysis was conducted using Mplus to examine the direct and indirect effects of health behaviors and psychological factors on health outcomes.\n\n\n\n\nSummarize the main findings, report the path coefficients, and interpret the implications.\n\n\nSummary of Findings: The path analysis revealed significant direct and indirect effects of health behaviors and psychological factors on health outcomes.\nStatistical Output: The standardized path coefficients were as follows: diet → BMI (β = -0.25, p &lt; 0.01), exercise → blood pressure (β = -0.20, p &lt; 0.05), stress → anxiety (β = 0.30, p &lt; 0.001). The model fit indices indicated a good fit: CFI = 0.97, RMSEA = 0.04.\nInterpretation: These results suggest that health behaviors and psychological factors have significant impacts on health outcomes. Interventions targeting these factors may help improve health metrics in the population.\nLimitations: Path analysis assumes linear relationships between variables, which may not adequately capture the complexity of health behaviors and outcomes, potentially limiting the interpretation of direct and indirect effects.\n\n\n\n\n\n\n\nDescribe the sample and how data were collected, specifying the variables included in the SEM.\n\n\nParticipants and Data Collection: The sample consisted of 500 participants from a community health survey. Data were collected through self-reported questionnaires and clinical measurements.\nVariables: The SEM included observed variables such as diet, exercise, stress, and health outcomes (BMI, blood pressure) as well as latent variables representing overall health and psychological well-being.\nProcedure: SEM was conducted using R with the lavaan package. The model was specified based on theoretical expectations and prior research.\n\n\n\n\nSummarize the main findings, report the goodness-of-fit indices, and interpret the implications.\n\n\nSummary of Findings: The SEM provided a good fit to the data, with significant path coefficients indicating the relationships among the variables.\nStatistical Output: The model fit indices were as follows: CFI = 0.96, RMSEA = 0.05, and χ²(200) = 300.45, p &lt; 0.001. The standardized path coefficients ranged from 0.20 to 0.45.\nInterpretation: These results support the hypothesized model, suggesting that both health behaviors and psychological well-being significantly influence health outcomes. These findings highlight the importance of integrated health interventions.\nLimitations: SEM requires large sample sizes for accurate estimates, which may not always be available in community health surveys, potentially impacting the stability of the parameter estimates.\n\n\n\n\n\n\n\nDescribe the sample and how data were collected, specifying the variables included in the CART analysis.\n\n\nParticipants and Data Collection: The study involved 600 patients from a diabetes clinic. Data were collected through clinical records and follow-up visits.\nVariables: The CART analysis included variables such as age, BMI, blood sugar levels, and lifestyle factors (diet, physical activity).\nProcedure: CART was conducted using Python with the scikit-learn library. The decision tree was trained to predict diabetes status based on the predictor variables.\n\n\n\n\nSummarize the main findings, report the structure of the decision tree, and interpret the implications.\n\n\nSummary of Findings: The CART model identified key predictors of diabetes status and provided a decision tree for classification.\nStatistical Output: The decision tree had a depth of 4 and included variables such as blood sugar levels, BMI, and age as primary split criteria. The model accuracy was 85%.\nInterpretation: These results suggest that blood sugar levels, BMI, and age are significant predictors of diabetes status. The decision tree model can be used for early identification and intervention in at-risk patients.\nLimitations: CART can be prone to overfitting, especially with small datasets, which may limit its generalizability to new data. Pruning is required to avoid overly complex trees.\n\n\n\n\n\n\n\nDescribe the sample and how data were collected, specifying the dependent and independent variables included in the regression analysis.\n\n\nParticipants and Data Collection: The sample included 400 participants from a cardiovascular health study. Data were collected through medical records and lifestyle questionnaires.\nVariables: The dependent variable was recovery time. The independent variables included age, severity of condition, treatment type, gender, and comorbidities.\nProcedure: The variable selection technique (forward selection, backward selection, and stepwise selection) was used to identify the most significant predictors of recovery time.\n\n\n\n\nSummarize the main findings, report the selected variables, and interpret the implications for each technique.\n\n\n\n\nSummary of Findings: Forward selection identified age, severity of condition, and treatment type as significant predictors.\nStatistical Output: The final model included age (β = 0.30, p &lt; 0.01), severity (β = 0.25, p &lt; 0.01), and treatment (β = -0.20, p &lt; 0.05). The model R² was 0.60.\nInterpretation: These results indicate that age, severity of condition, and treatment type are important predictors of recovery time.\nLimitations: Forward selection can miss important variables that only show their effect in combination with others. It may also include variables that are not truly significant due to random chance, leading to potential overfitting.\n\n\n\nSummary of Findings: Backward selection identified severity of condition, age, and comorbidities as significant predictors.\nStatistical Output: The final model included severity (β = 0.35, p &lt; 0.001), age (β = 0.20, p &lt; 0.05), and comorbidities (β = 0.15, p &lt; 0.05). The model R² was 0.65.\nInterpretation: These results highlight the importance of severity, age, and comorbidities in predicting recovery time.\nLimitations: Backward selection can be computationally intensive and may remove variables that are important in combination with others. It can also be sensitive to the initial set of predictors, leading to different final models if starting with a different set.\n\n\n\nSummary of Findings: Stepwise selection identified age, severity, treatment, and comorbidities as significant predictors.\nStatistical Output: The final model included age (β = 0.25, p &lt; 0.01), severity (β = 0.30, p &lt; 0.01), treatment (β = -0.15, p &lt; 0.05), and comorbidities (β = 0.10, p &lt; 0.05). The model R² was 0.70.\nInterpretation: These results suggest that a combination of age, severity, treatment type, and comorbidities are important predictors of recovery time.\nLimitations: Stepwise selection can be prone to overfitting, especially in small datasets. It may also be unstable, meaning small changes in the data can result in different models. Additionally, it can be computationally intensive and may not always provide the most parsimonious model.\n\n\n\nSummary of Findings: LASSO regression identified age, severity of condition, and treatment type as significant predictors.\nStatistical Output: The final model included age (β = 0.28), severity (β = 0.35), and treatment (β = -0.22). The model’s cross-validated mean squared error (MSE) was 0.12.\nInterpretation: These results suggest that age, severity of condition, and treatment type are important predictors of recovery time. The LASSO regression model’s ability to shrink less important variable coefficients to zero highlights its usefulness in handling high-dimensional data with many predictors.\nLimitations: LASSO regression may not perform well if the true relationship between the predictors and the outcome is not linear. It can also be sensitive to the choice of the regularization parameter (lambda), which requires careful selection. Additionally, LASSO may struggle with highly correlated predictors, as it tends to select one and ignore the others."
  },
  {
    "objectID": "Lectures/writing.html#introduction",
    "href": "Lectures/writing.html#introduction",
    "title": "Guide to Writing Methods and Results for Statistical Analyses",
    "section": "",
    "text": "This guide provides general instructions on how to write the Methods and Results sections for various statistical methods used in academic research and provides a generic example of each. It includes tips and suggestions for Discriminant Analysis, MANOVA, Principal Component Analysis (PCA), Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), Path Analysis (PA), Structural Equation Modeling (SEM), Classification and Regression Trees (CART), and different variable selection techniques in regression."
  },
  {
    "objectID": "Lectures/writing.html#discriminant-analysis",
    "href": "Lectures/writing.html#discriminant-analysis",
    "title": "Guide to Writing Methods and Results for Statistical Analyses",
    "section": "",
    "text": "Describe the sample and how data were collected, specifying the dependent (grouping) variable and the predictor variables.\n\n\nParticipants and Data Collection: A total of 200 patients from the cardiology department were included in this study. Data were collected through structured interviews and medical records.\nVariables: The dependent variable was patient risk category (low, medium, high). Predictor variables included age, blood pressure, cholesterol levels, and BMI.\nProcedure: Discriminant analysis was conducted using SPSS to classify patients into risk categories. The stepwise method was used to select the most significant predictors.\n\n\n\n\nSummarize the main findings, report the statistical output, and interpret the implications.\n\n\nSummary of Findings: The discriminant analysis revealed that blood pressure and cholesterol levels were significant predictors of patient risk category.\nStatistical Output: The first canonical discriminant function had an eigenvalue of 2.34 and explained 75% of the variance. Wilks’ Lambda was significant (Λ = 0.65, χ² = 42.56, p &lt; 0.001). The classification results showed that 85% of the patients were correctly classified.\nInterpretation: These results suggest that blood pressure and cholesterol levels are critical factors in determining patient risk categories. This information can be used to improve risk assessment protocols.\nLimitations: Discriminant analysis assumes that the predictor variables are normally distributed within each group, which may not always be the case with clinical data, potentially affecting the accuracy of the classification."
  },
  {
    "objectID": "Lectures/writing.html#manova",
    "href": "Lectures/writing.html#manova",
    "title": "Guide to Writing Methods and Results for Statistical Analyses",
    "section": "",
    "text": "Describe the sample and how data were collected, specifying the dependent and independent variables.\n\n\nParticipants and Data Collection: The study included 150 participants from three different treatment groups. Data were collected through clinical measurements and self-reported questionnaires.\nVariables: The dependent variables were blood pressure, cholesterol levels, and heart rate. The independent variable was the treatment group.\nProcedure: MANOVA was conducted using R to compare the multivariate means of the three treatment groups. Pillai’s trace was used as the test statistic.\n\n\n\n\nSummarize the main findings, report the test statistics, degrees of freedom, and p-values, and interpret the implications.\n\n\nSummary of Findings: The MANOVA revealed significant differences in the combined dependent variables across the treatment groups.\nStatistical Output: Pillai’s trace was significant (V = 0.45, F(6, 288) = 5.67, p &lt; 0.001). Follow-up ANOVAs indicated significant differences in blood pressure (F(2, 147) = 8.45, p &lt; 0.01), cholesterol levels (F(2, 147) = 6.78, p &lt; 0.01), and heart rate (F(2, 147) = 5.34, p &lt; 0.05).\nInterpretation: These results suggest that the treatment groups differ significantly in their health outcomes. Further post-hoc analyses are needed to determine which groups differ from each other.\nLimitations: MANOVA assumes homogeneity of variance-covariance matrices, which might not hold true for the clinical measurements, potentially affecting the validity of the results."
  },
  {
    "objectID": "Lectures/writing.html#principal-component-analysis-pca",
    "href": "Lectures/writing.html#principal-component-analysis-pca",
    "title": "Guide to Writing Methods and Results for Statistical Analyses",
    "section": "",
    "text": "Describe the sample and how data were collected, specifying the variables included in the PCA.\n\n\nParticipants and Data Collection: Data were collected from 300 nursing students through a survey measuring various health indicators.\nVariables: The variables included in the PCA were 20 health indicators such as physical activity, diet, stress levels, and sleep quality.\nProcedure: PCA was conducted using Python to reduce the dimensionality of the dataset. The number of components was determined based on eigenvalues greater than 1 and the scree plot.\n\n\n\n\nSummarize the main findings, report the explained variance, eigenvalues, and component loadings, and interpret the implications.\n\n\nSummary of Findings: The PCA identified four principal components that explained 65% of the total variance.\nStatistical Output: The first component had an eigenvalue of 4.5 and explained 25% of the variance. The component loadings indicated that physical activity, diet, and sleep quality loaded highly on the first component.\nInterpretation: These results suggest that physical activity, diet, and sleep quality are key factors influencing the health indicators of nursing students. Interventions targeting these areas may be effective in improving overall health.\nLimitations: PCA assumes linear relationships among variables and may not capture complex, non-linear interactions present in health data, potentially oversimplifying the underlying structure."
  },
  {
    "objectID": "Lectures/writing.html#exploratory-factor-analysis-efa",
    "href": "Lectures/writing.html#exploratory-factor-analysis-efa",
    "title": "Guide to Writing Methods and Results for Statistical Analyses",
    "section": "",
    "text": "Describe the sample and how data were collected, specifying the variables included in the EFA.\n\n\nParticipants and Data Collection: The study included 250 patients from a mental health clinic. Data were collected through a comprehensive survey on mental health and well-being.\nVariables: The variables included in the EFA were 15 survey items related to mental health.\nProcedure: EFA was conducted using R with varimax rotation to identify the underlying factor structure. The number of factors was determined based on eigenvalues greater than 1 and the scree plot.\n\n\n\n\nSummarize the main findings, report the factor loadings, eigenvalues, and explained variance, and interpret the implications.\n\n\nSummary of Findings: The EFA identified three factors that explained 70% of the total variance.\nStatistical Output: The first factor had an eigenvalue of 5.2 and explained 35% of the variance. The factor loadings indicated that anxiety, depression, and stress loaded highly on the first factor.\nInterpretation: These results suggest that anxiety, depression, and stress are closely related constructs in this patient population. Interventions targeting these factors may be beneficial in improving mental health outcomes.\nLimitations: EFA requires a large sample size for stable results, which may be difficult to achieve in a clinical setting, potentially affecting the reliability of the factor structure identified."
  },
  {
    "objectID": "Lectures/writing.html#confirmatory-factor-analysis-cfa",
    "href": "Lectures/writing.html#confirmatory-factor-analysis-cfa",
    "title": "Guide to Writing Methods and Results for Statistical Analyses",
    "section": "",
    "text": "Describe the sample and how data were collected, specifying the variables included in the CFA.\n\n\nParticipants and Data Collection: Data were collected from 400 respondents through a structured survey assessing various psychological constructs.\nVariables: The variables included in the CFA were items from standardized questionnaires on anxiety, depression, and stress.\nProcedure: CFA was conducted using R with the lavaan package. The factor structure was specified based on theoretical expectations and prior research.\n\n\n\n\nSummarize the main findings, report the goodness-of-fit indices, and interpret the implications.\n\n\nSummary of Findings: The CFA model showed an acceptable fit to the data.\nStatistical Output: The model fit indices were as follows: CFI = 0.95, RMSEA = 0.05, and χ²(120) = 250.34, p &lt; 0.001. The standardized factor loadings ranged from 0.60 to 0.85.\nInterpretation: These results indicate that the specified factor structure is a good representation of the underlying constructs. The high factor loadings suggest that the observed variables are good indicators of their respective latent variables.\nLimitations: CFA requires a large sample size for reliable estimates, which may not always be feasible in psychological research, potentially affecting the robustness of the model fit."
  },
  {
    "objectID": "Lectures/writing.html#path-analysis-pa",
    "href": "Lectures/writing.html#path-analysis-pa",
    "title": "Guide to Writing Methods and Results for Statistical Analyses",
    "section": "",
    "text": "Describe the sample and how data were collected, specifying the variables included in the path analysis.\n\n\nParticipants and Data Collection: The study included 350 participants from a longitudinal health study. Data were collected through annual health assessments and surveys.\nVariables: The variables included in the path analysis were health behaviors (diet, exercise), psychological factors (stress, anxiety), and health outcomes (BMI, blood pressure).\nProcedure: Path analysis was conducted using Mplus to examine the direct and indirect effects of health behaviors and psychological factors on health outcomes.\n\n\n\n\nSummarize the main findings, report the path coefficients, and interpret the implications.\n\n\nSummary of Findings: The path analysis revealed significant direct and indirect effects of health behaviors and psychological factors on health outcomes.\nStatistical Output: The standardized path coefficients were as follows: diet → BMI (β = -0.25, p &lt; 0.01), exercise → blood pressure (β = -0.20, p &lt; 0.05), stress → anxiety (β = 0.30, p &lt; 0.001). The model fit indices indicated a good fit: CFI = 0.97, RMSEA = 0.04.\nInterpretation: These results suggest that health behaviors and psychological factors have significant impacts on health outcomes. Interventions targeting these factors may help improve health metrics in the population.\nLimitations: Path analysis assumes linear relationships between variables, which may not adequately capture the complexity of health behaviors and outcomes, potentially limiting the interpretation of direct and indirect effects."
  },
  {
    "objectID": "Lectures/writing.html#structural-equation-modeling-sem",
    "href": "Lectures/writing.html#structural-equation-modeling-sem",
    "title": "Guide to Writing Methods and Results for Statistical Analyses",
    "section": "",
    "text": "Describe the sample and how data were collected, specifying the variables included in the SEM.\n\n\nParticipants and Data Collection: The sample consisted of 500 participants from a community health survey. Data were collected through self-reported questionnaires and clinical measurements.\nVariables: The SEM included observed variables such as diet, exercise, stress, and health outcomes (BMI, blood pressure) as well as latent variables representing overall health and psychological well-being.\nProcedure: SEM was conducted using R with the lavaan package. The model was specified based on theoretical expectations and prior research.\n\n\n\n\nSummarize the main findings, report the goodness-of-fit indices, and interpret the implications.\n\n\nSummary of Findings: The SEM provided a good fit to the data, with significant path coefficients indicating the relationships among the variables.\nStatistical Output: The model fit indices were as follows: CFI = 0.96, RMSEA = 0.05, and χ²(200) = 300.45, p &lt; 0.001. The standardized path coefficients ranged from 0.20 to 0.45.\nInterpretation: These results support the hypothesized model, suggesting that both health behaviors and psychological well-being significantly influence health outcomes. These findings highlight the importance of integrated health interventions.\nLimitations: SEM requires large sample sizes for accurate estimates, which may not always be available in community health surveys, potentially impacting the stability of the parameter estimates."
  },
  {
    "objectID": "Lectures/writing.html#classification-and-regression-trees-cart",
    "href": "Lectures/writing.html#classification-and-regression-trees-cart",
    "title": "Guide to Writing Methods and Results for Statistical Analyses",
    "section": "",
    "text": "Describe the sample and how data were collected, specifying the variables included in the CART analysis.\n\n\nParticipants and Data Collection: The study involved 600 patients from a diabetes clinic. Data were collected through clinical records and follow-up visits.\nVariables: The CART analysis included variables such as age, BMI, blood sugar levels, and lifestyle factors (diet, physical activity).\nProcedure: CART was conducted using Python with the scikit-learn library. The decision tree was trained to predict diabetes status based on the predictor variables.\n\n\n\n\nSummarize the main findings, report the structure of the decision tree, and interpret the implications.\n\n\nSummary of Findings: The CART model identified key predictors of diabetes status and provided a decision tree for classification.\nStatistical Output: The decision tree had a depth of 4 and included variables such as blood sugar levels, BMI, and age as primary split criteria. The model accuracy was 85%.\nInterpretation: These results suggest that blood sugar levels, BMI, and age are significant predictors of diabetes status. The decision tree model can be used for early identification and intervention in at-risk patients.\nLimitations: CART can be prone to overfitting, especially with small datasets, which may limit its generalizability to new data. Pruning is required to avoid overly complex trees."
  },
  {
    "objectID": "Lectures/writing.html#variable-selection-techniques-in-regression",
    "href": "Lectures/writing.html#variable-selection-techniques-in-regression",
    "title": "Guide to Writing Methods and Results for Statistical Analyses",
    "section": "",
    "text": "Describe the sample and how data were collected, specifying the dependent and independent variables included in the regression analysis.\n\n\nParticipants and Data Collection: The sample included 400 participants from a cardiovascular health study. Data were collected through medical records and lifestyle questionnaires.\nVariables: The dependent variable was recovery time. The independent variables included age, severity of condition, treatment type, gender, and comorbidities.\nProcedure: The variable selection technique (forward selection, backward selection, and stepwise selection) was used to identify the most significant predictors of recovery time.\n\n\n\n\nSummarize the main findings, report the selected variables, and interpret the implications for each technique.\n\n\n\n\nSummary of Findings: Forward selection identified age, severity of condition, and treatment type as significant predictors.\nStatistical Output: The final model included age (β = 0.30, p &lt; 0.01), severity (β = 0.25, p &lt; 0.01), and treatment (β = -0.20, p &lt; 0.05). The model R² was 0.60.\nInterpretation: These results indicate that age, severity of condition, and treatment type are important predictors of recovery time.\nLimitations: Forward selection can miss important variables that only show their effect in combination with others. It may also include variables that are not truly significant due to random chance, leading to potential overfitting.\n\n\n\nSummary of Findings: Backward selection identified severity of condition, age, and comorbidities as significant predictors.\nStatistical Output: The final model included severity (β = 0.35, p &lt; 0.001), age (β = 0.20, p &lt; 0.05), and comorbidities (β = 0.15, p &lt; 0.05). The model R² was 0.65.\nInterpretation: These results highlight the importance of severity, age, and comorbidities in predicting recovery time.\nLimitations: Backward selection can be computationally intensive and may remove variables that are important in combination with others. It can also be sensitive to the initial set of predictors, leading to different final models if starting with a different set.\n\n\n\nSummary of Findings: Stepwise selection identified age, severity, treatment, and comorbidities as significant predictors.\nStatistical Output: The final model included age (β = 0.25, p &lt; 0.01), severity (β = 0.30, p &lt; 0.01), treatment (β = -0.15, p &lt; 0.05), and comorbidities (β = 0.10, p &lt; 0.05). The model R² was 0.70.\nInterpretation: These results suggest that a combination of age, severity, treatment type, and comorbidities are important predictors of recovery time.\nLimitations: Stepwise selection can be prone to overfitting, especially in small datasets. It may also be unstable, meaning small changes in the data can result in different models. Additionally, it can be computationally intensive and may not always provide the most parsimonious model.\n\n\n\nSummary of Findings: LASSO regression identified age, severity of condition, and treatment type as significant predictors.\nStatistical Output: The final model included age (β = 0.28), severity (β = 0.35), and treatment (β = -0.22). The model’s cross-validated mean squared error (MSE) was 0.12.\nInterpretation: These results suggest that age, severity of condition, and treatment type are important predictors of recovery time. The LASSO regression model’s ability to shrink less important variable coefficients to zero highlights its usefulness in handling high-dimensional data with many predictors.\nLimitations: LASSO regression may not perform well if the true relationship between the predictors and the outcome is not linear. It can also be sensitive to the choice of the regularization parameter (lambda), which requires careful selection. Additionally, LASSO may struggle with highly correlated predictors, as it tends to select one and ignore the others."
  },
  {
    "objectID": "Lectures/introduction-to-CFA-PA.html#understanding-the-basics-of-confirmatory-factor-analysis-cfa-and-path-analysis-pa-and-their-application-in-nursing-research",
    "href": "Lectures/introduction-to-CFA-PA.html#understanding-the-basics-of-confirmatory-factor-analysis-cfa-and-path-analysis-pa-and-their-application-in-nursing-research",
    "title": "Introduction to Confirmatory Factor Analysis (CFA) and Path Analysis (PA)",
    "section": "Understanding the Basics of Confirmatory Factor Analysis (CFA) and Path Analysis (PA) and their Application in Nursing Research",
    "text": "Understanding the Basics of Confirmatory Factor Analysis (CFA) and Path Analysis (PA) and their Application in Nursing Research\nConfirmatory Factor Analysis (CFA) and Path Analysis (PA) are essential statistical techniques used to validate measurement models and examine causal relationships between variables, respectively. These methods are crucial in nursing research for understanding complex phenomena and testing theoretical models.\n\nCFA vs. EFA\n\nExploratory Factor Analysis (EFA) is used when you do not have a clear hypothesis about the structure or number of factors.\nConfirmatory Factor Analysis (CFA) is used when you have specific hypotheses or models to test based on theory or prior research.\n\n\n\nExample: One Factor Analysis using SPSS Anxiety Questionnaire\nTo conduct a CFA with an anxiety questionnaire in SPSS:\n\nData Preparation: Ensure data is clean and appropriately coded.\nModel Specification: Define the factor structure based on theoretical expectations.\nRun CFA in Statistical Software: See below for examples.\n\n\n\nInterpreting CFA Symbols, Intro to Path Analysis\nCFA uses various symbols to represent latent variables, observed variables, and the relationships between them. Path analysis extends this by allowing for more complex models, including direct and indirect effects between observed and latent variables.\n\n\n\nDefine Path Analysis, Contrast with CFA and EFA\nPath analysis is a type of structural equation modeling (SEM) that examines the direct relationship among a set of observed variables.\n\n\nDefine Path Diagram\nA path diagram visually represents the hypothesized relationships among variables. It includes: - Observed Variables: Represented by rectangles.\n\n\nSpecial Considerations: No Latent Variables, Correlation vs. Causation\n\nNo Latent Variables: Path analysis can be conducted without latent variables, focusing only on observed variables.\nCorrelation vs. Causation: Path analysis can help infer causality, but it is essential to consider the theoretical basis and experimental design to make causal claims.\n\n\n\nAssessing Goodness of Fit (CFI and RMSEA)\n\nComparative Fit Index (CFI): Values close to 1 indicate a good fit.\nRoot Mean Square Error of Approximation (RMSEA): Values less than 0.06 indicate a good fit.\n\n\n\nExample, Hands-on CFA Exercise in Statistical Software\nYou can use the worland5.csv as an example. This hypothetical dataset examines the effects of student background on academic achievement. It contains 9 observed variables (Motivation, Harmony, Stability, Negative Parental Psychology, SES, Verbal IQ, Reading, Arithmetic and Spelling) and 3 hypothesized latent constructs (Adjustment, Risk, Achievement).\n\nJMP Instructions:\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Multivariate Methods &gt; Multivariate.\n\n\nSelect your variables and click OK.\n\n\nConfigure the model and click Run.\n\n\n\n\n\nR Code Example:\n\n\nR Code Example:\n\n\n\n        # Load necessary library\n        library(lavaan)\n        #load your data\n        mydata3&lt;-read.csv(\"worland5.csv\") # adjust the file path accordingly\n\n        # Define the model\n        model &lt;- 'adjustment  =~ motiv + harm + stabi'\n\n        # Fit the model\n        fit &lt;- cfa(model, data = mydata3)\n\n        # Summarize the results\n        summary(fit, fit.measures = TRUE)\n      \n\n\n\n\n\nPython Code Example:\n\n\nPython Code Example:\n\n\n\n   pip install factor_analyzer\n        import pandas as pd\n        from factor_analyzer import FactorAnalyzer\n        from factor_analyzer import ConfirmatoryFactorAnalyzer\n        from factor_analyzer import ModelSpecificationParser\n        \n        \n        df_features = pd.read_csv('worland5.csv')\n\n        # Define the model\n        model = {\n          'adjustment': ['motiv', 'harm', 'stabi'],\n        }\n        \n      model_spec = ModelSpecificationParser.parse_model_specification_from_dict(df_features,\n                                                                              model)\n        # Fit the model\n        cfa = ConfirmatoryFactorAnalyzer(model_spec)\n        cfa.fit(df_features.values)\n\n        # Display the results\n        print(cfa.loadings_)\n      \n\n\n\n\n\nSPSS Instructions:\n\n\nSPSS Instructions:\n\nCFA can be performed in AMOS, a seperate tool built for conducting SEM for SPSS users see more about the AMOS platform at\nhttp://amosdevelopment.com/\n\n\n\n\n\nStata Code Example:\n\n\nStata Code Example:\n\n\n\n        //  CFA for Single-factor measurement model\n        // Load data, example is the auto data in stata\n        use auto, clear\n         \n\n        // Define and fit the model\n        \n        sem (trunk weight length gear_ratio&lt;-Auto_character)\n\n        //get standardized values\n       \n        sem, standardized\n        \n        //get godness of fit\n        estat gof,stats(all)\n      \n\n\n\n\n\n\nInterpreting Your Results, Refining, Write-up\nAfter running your CFA or path analysis, interpret the results focusing on the goodness of fit indices and the significance of path coefficients. Refining the model may involve adding or removing paths or re-specifying the factor structure based on theoretical considerations and model diagnostics. Document the final model and its implications for nursing research in a clear and structured manner."
  },
  {
    "objectID": "Lectures/introduction-to-CART.html#understanding-the-basics-of-cart-and-its-application-in-research",
    "href": "Lectures/introduction-to-CART.html#understanding-the-basics-of-cart-and-its-application-in-research",
    "title": "Introduction to CART Modeling",
    "section": "Understanding the Basics of CART and Its Application in Research",
    "text": "Understanding the Basics of CART and Its Application in Research\nClassification and Regression Trees (CART) are powerful tools for predictive modeling, capable of handling both categorical and continuous outcomes. CART is used extensively in healthcare research and the basis of more advanced tree methods meant to predict key health outcomes and uncover relationships in complex datasets.\n\nDefine CART Modeling\nCART builds a decision tree to predict the outcome variable based on the input variables. It splits the data into subsets based on the value of input variables, aiming to create the most homogeneous subsets possible.\n\n\nReal-World Examples of Decision Trees\n\nHealthcare Diagnosis: Predicting whether a patient has a disease based on symptoms and test results.\nTreatment Recommendations: Suggesting personalized treatment plans based on patient history and characteristics.\n\n\nPeriodontitis\n\n\nPeriodontitis is a serious gum infection that damages the soft tissue and destroys the bone that supports your teeth. It can cause teeth to loosen or lead to tooth loss. Periodontitis is common but largely preventable. It’s usually the result of poor oral hygiene. Daily brushing and flossing, along with regular dental checkups, can greatly reduce your chance of developing periodontitis.\nThe data: National Health and Nutrition Examination Study (NHANES) from 1999-2004. Approximately 15,000 observations. Primary outcome is whether the subject got periodontitis (yes/no). Explanatory or Predictor Variables: Smoking Status, Age, Total Teeth, and Gender.\n\n\nA simple cross-section of two variables by the outcome (color: blue=no perio, red=perio), looking for tree splits that classify the outcome accurately. Can you find any splits with age and total teeth?\n\nA tree of depth 2 (left) and tree of depth 3 (right).\n\n\n\n\nTypes of Outcomes: Categorical and Continuous\n\nCategorical Outcomes: Use classification trees when predicting a categorical outcome, such as whether a disease is present (yes/no). These trees provide the probability of each category as the outcome.\nContinuous Outcomes: Use regression trees when predicting a continuous outcome, such as blood pressure levels. These trees provide the average outcome value for individuals in the dataset who follow the specific path defined by the tree.\n\n\n\nGraphical Representations\nDecision trees are represented graphically with nodes and branches: - Decision Nodes: Represent decisions or splits based on input variables. - Leaf Nodes: Represent the final outcome or prediction.\n\n\nHow and When to Split\nSplitting in a decision tree is based on criteria such as: - Gini Impurity: Used for classification, aiming to create pure subsets. - Mean Squared Error (MSE): Used for regression, aiming to minimize variance within subsets.\n\n\nPros and Cons of CART Modeling\nPros: - Easy to interpret and visualize. - Can handle both numerical and categorical data. - No need for data normalization or scaling.\nCons: - Can overfit the data, leading to poor generalization. - Sensitive to small changes in the data. - May require pruning to remove unnecessary branches.\n\n\nExample CART Model\nTo illustrate a CART model, consider predicting diabetes based on patient characteristics:\n\nLoad Data: Prepare the dataset containing patient features and diabetes status.\nDefine Model: Specify the CART model parameters.\nTrain Model: Fit the CART model to the data.\nEvaluate Model: Assess the model’s performance using metrics like accuracy and ROC curve.\n\n\n\nHands-on Exercise: Building a CART Model Based on Students’ Own Decisions\nYou can use the Titanic_Passengers dataset for this example. This dataset contains information on the survival status of Titanic passengers.\n\nJMP Instructions:\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Predictive Modeling &gt; Partition.\n\n\nSelect the response variable and predictor variables.\n\n\nClick OK to build the tree.\n\n\n\n\n\nR Code Example:\n\n\nR Code Example:\n\n\n\n        # Load necessary libraries\n        library(rpart)\n        library(rpart.plot)\n\n        # Load the dataset\n        Passengers &lt;- read.csv(\"Titanic_Passengers.csv\") # adust path\n      \n        \n        # Define the CART model\n        model &lt;- rpart(Survived ~ Sex+Age+Passenger.Class+Port, \n        data = Passengers , method = \"class\")\n\n        # Plot the decision tree\n        rpart.plot(model)\n      \n\n\n\n\n\nPython Code Example with Titanic data:\n\n\nPython Code Example:\n\n\n\n        from sklearn.tree import DecisionTreeClassifier\n        from sklearn import tree\n        import pandas as pd\n\n        # Load the dataset\n        passengers = pd.read_csv('Titanic_Passengers.csv')\n        X = passengers[['Sex', 'Age', 'Passenger.Class', 'Port']]\n        y = passengers['Survived']\n\n        # Define the CART model\n        model = DecisionTreeClassifier()\n\n        # Train the model\n        model.fit(X, y)\n\n        # Plot the decision tree\n        tree.plot_tree(model)\n      \n\n\n\n\n\nSPSS Instructions:\n\n\nSPSS Instructions:\n\n\n\nGo to Analyze &gt; Classify &gt; Tree.\n\n\nSelect the dependent variable and independent variables.\n\n\nChoose the type of tree (e.g., CHAID, CRT).\n\n\nClick OK to run the analysis.\n\n\n\n\n\nSPSS Code Instructions:\n\n\n\n* Use SPSS inbuilt customer data as an example (modify the file path)\nGET \n  FILE='C:\\Program Files\\IBM\\SPSS\\Statistics\\22\\Samples\\English\\customer_subset.sav'. \n\n* Decision Tree Dependent: home ownership ,Indep VARS  education age and income \n\nTREE homeown [n] BY ed [s] age [s] inccat [o] \n  /TREE DISPLAY=TOPDOWN NODES=STATISTICS BRANCHSTATISTICS=YES NODEDEFS=YES SCALE=AUTO\n  /DEPCATEGORIES USEVALUES=[0 1]\n  /PRINT MODELSUMMARY CLASSIFICATION RISK\n  /METHOD TYPE=CHAID\n  /GROWTHLIMIT MAXDEPTH=AUTO MINPARENTSIZE=100 MINCHILDSIZE=50\n  /VALIDATION TYPE=NONE OUTPUT=BOTHSAMPLES\n  /CHAID ALPHASPLIT=0.05 ALPHAMERGE=0.05 SPLITMERGED=NO CHISQUARE=PEARSON CONVERGE=0.001 \n    MAXITERATIONS=100 ADJUST=BONFERRONI INTERVALS=10\n  /COSTS EQUAL.\n\n\n\n\n\nStata Code Example:\n\n\nStata Code Example:\n\n\n\n// Load the auto data in stata\nsysuse auto,clear\n        \n// browse to see the cols and rows ofthe data\nbrowse\n        \n// use the help function to review crtrees module\nhelp(crtrees)\n \n// Define and fit the CART model (regression tree) : continuous  outcome\n       crtrees price trunk weight length foreign gear_ratio, seed(12345) detail tree\n\n// Define and fit the CART model (classification tree): categorical outcome\n  crtrees foreign price trunk weight length gear_ratio,classification impurity(gini)tree"
  },
  {
    "objectID": "Lectures/introduction-to-EFA.html",
    "href": "Lectures/introduction-to-EFA.html",
    "title": "Introduction to Exploratory Factor Analysis (EFA)",
    "section": "",
    "text": "Exploratory Factor Analysis (EFA) is a statistical technique used to uncover the underlying structure of a relatively large set of variables. It is often used in research to identify latent variables that cannot be measured directly.\n\n\n\nDefinition: Latent variables are variables that are not directly observed but are inferred from other variables that are observed (measured).\nExamples:\n\nIntelligence inferred from cognitive test scores.\nDepression inferred from responses to psychological questionnaires.\n\nImportance in Research:\n\nHelps in understanding underlying constructs that are not directly measurable.\nEssential in fields like psychology, education, and social sciences where many constructs are abstract.\n\n\n\n\n\n\n\n\nCorrelation Matrix\n\n\n\nR Matrix:\n\nA correlation matrix showing the correlation coefficients between pairs of variables.\nEach cell in the matrix represents the correlation between two continuous variables.\n\nInterpreting Correlations:\n\nCorrelation values range from -1 to 1.\nCorrelation is a measure of the strength of the linear relationship between two continuous variables.\nCloser to 1 or -1 indicates strong positive or negative correlation, while closer to 0 indicates weak correlation.\n\nUses in EFA:\n\nIdentifies patterns of correlations.\nVariables that are correlated are likely being influenced by the same underlying latent factor.\nHelps in determining the suitability of data for factor analysis.\nUse the correlation matrix (R Matrix) to check for clusters of variables that are correlated. If there is a lack of clusters then your data may not be suitable for EFA.\nChecking the correlation matrix (R Matrix) for a large dataset can be difficult.\n\n\n\n\n\n\nDefinition: The study of how to assign numbers to objects and events according to rules.\nComponents:\n\nConstructs: Theoretical concepts being measured (e.g., intelligence, satisfaction).\nIndicators: Observed variables that reflect or are influenced by the constructs.\n\nApplications:\n\nEnsuring that the indicators accurately and reliably measure or are influenced by the constructs.\nValidating the measurement instruments.\n\n\n\n\n\n\nPrincipal Component Analysis (PCA):\n\nAimed at reducing the dimensionality of the data.\nTransforms the variables into a new set of uncorrelated components.\nComponents are linear combinations of the original variables.\n\nExploratory Factor Analysis (EFA):\n\nResearcher uses EFA because they believe there are latent (unobserved) variables.\nSeeks to identify latent factors that explain the correlations among variables.\nFactors are assumed to cause the observed variables.\nFactors can be assumed to be correlated or or uncorrelated.\n\nKey Differences:\n\nGoal: PCA is descriptive, EFA is explanatory.\nComponents vs. Factors: PCA components are not based on underlying constructs, whereas EFA factors are.\n\n\n\n\n\n\nSteps:\n\nCheck for Correlations: Ensure variables are sufficiently correlated for EFA. Check R Matrix.\nDescriptive Statistics: Examine means, standard deviations, and distribution shapes. Floor and Ceiling Effects.\nAssumptions:\n\nAdequate sample size (at least 5-10 times the number of variables).\nNormality of variables.\nLinearity and homoscedasticity.\n\n\n\n\n\n\n\nPrincipal Axis Factoring (PAF):\n\nFocuses on shared variance.\nSuitable for data where normality cannot be assumed.\n\nMaximum Likelihood (ML):\n\nAssumes multivariate normality.\nProvides goodness-of-fit measures.\nUseful for hypothesis testing about the factor structure.\n\nChoosing the Method:\n\nDepends on the nature of the data and research objectives.\nML is preferred for confirmatory purposes, PAF for exploratory purposes.\n\n\n\n\n\n\nDefinition: Estimate of the variance in each variable accounted for by the common factors.\nMethods of Estimation:\n\nSquared multiple correlations.\nSetting to a default value such as 1.\n\nRole in EFA:\n\nHelps in the initial extraction of factors.\nInfluences the factor solution and interpretation.\n\n\n\n\n\n\nEigenvalue:\n\nRepresents the amount of variance explained by each factor.\nCommon rule: retain factors with eigenvalues greater than 1.\n\nRotation Methods:\n\nVarimax (Orthogonal):\n\nSimplifies the loadings to make interpretation easier.\nAssumes factors are uncorrelated.\n\nPromax (Oblique):\n\nAllows for correlated factors.\nProvides a more realistic representation of the data.\n\n\nPurpose of Rotation:\n\nEnhances interpretability of the factor solution.\nClarifies the structure by maximizing high loadings and minimizing low loadings on each factor.\n\n\n\n\n\n\n\n\nEigenvalues Greater Than 1:\n\nUse the Kaiser criterion, which suggests retaining factors with eigenvalues greater than 1.\nFactors with eigenvalues above 1 explain more variance than a single observed variable.\n\nScree Plot:\n\nExamine the scree plot for an “elbow point,” where the slope of eigenvalues levels off.\nRetain factors before the point where the plot starts to flatten.\n\nParallel Analysis:\n\nCompare the observed eigenvalues to those obtained from random data of the same size.\nRetain factors with observed eigenvalues greater than the corresponding random eigenvalues.\n\n\n\n\nCumulative Variance Explained:\n\nConsider the total variance explained by the retained factors.\nAim for a solution that explains a substantial proportion of the total variance (e.g., 70-80%).\n\nTheoretical Considerations:\n\nConsider the theoretical framework and prior research.\nEnsure the number of factors makes sense in the context of the study’s conceptual model.\n\n\n\n\n\n\n\nFactor Loadings:\n\nRepresent the correlation between observed variables and the latent factor.\nLoadings range from -1 to 1, with higher absolute values indicating stronger relationships.\n\nSignificant Loadings:\n\nLoadings greater than ±0.30 are generally considered significant.\nHigher thresholds (e.g., ±0.50 or ±0.60) indicate more robust relationships.\n\nInterpreting Factors:\n\nLabel factors based on the variables with the highest loadings.\nConsider the conceptual meaning and relevance of the variables in the context of the study.\n\nRotation Methods:\n\nUse rotation methods (e.g., varimax, oblimin) to achieve a simpler, more interpretable structure.\nOrthogonal rotations (e.g., varimax) assume factors are uncorrelated, while oblique rotations (e.g., oblimin) allow for correlated factors.\n\nInterpreting Results:\n\nPicking the number of factors: use the scree plot or eigenvalue rule.\nFactor Loadings: Indicates the strength and direction of the relationship between variables and factors.\nCommunalities: Indicates the proportion of each variable’s variance explained by the factors.\nVariance Explained: The total variance accounted for by all the factors.\n\nWriting Up:\n\nDescribe the data and the preliminary analysis.\nDetail the extraction method and rotation used.\nShow scree plot and argue why you are choosing the number of factors you are.\nPresent the factor loadings, communalities, and variance explained via tables and figures.\nDiscuss the theoretical and practical implications of the factors.\n\n\nBy understanding the basics of EFA, latent variables, and their application in research, researchers can effectively use this technique to uncover underlying structures in their data, providing valuable insights and advancing knowledge in their field."
  },
  {
    "objectID": "Lectures/introduction-to-EFA.html#understanding-the-basics-of-latent-variables",
    "href": "Lectures/introduction-to-EFA.html#understanding-the-basics-of-latent-variables",
    "title": "Introduction to Exploratory Factor Analysis (EFA)",
    "section": "",
    "text": "Definition: Latent variables are variables that are not directly observed but are inferred from other variables that are observed (measured).\nExamples:\n\nIntelligence inferred from cognitive test scores.\nDepression inferred from responses to psychological questionnaires.\n\nImportance in Research:\n\nHelps in understanding underlying constructs that are not directly measurable.\nEssential in fields like psychology, education, and social sciences where many constructs are abstract."
  },
  {
    "objectID": "Lectures/introduction-to-EFA.html#understanding-an-r-matrixcorrelations",
    "href": "Lectures/introduction-to-EFA.html#understanding-an-r-matrixcorrelations",
    "title": "Introduction to Exploratory Factor Analysis (EFA)",
    "section": "",
    "text": "Correlation Matrix\n\n\n\nR Matrix:\n\nA correlation matrix showing the correlation coefficients between pairs of variables.\nEach cell in the matrix represents the correlation between two continuous variables.\n\nInterpreting Correlations:\n\nCorrelation values range from -1 to 1.\nCorrelation is a measure of the strength of the linear relationship between two continuous variables.\nCloser to 1 or -1 indicates strong positive or negative correlation, while closer to 0 indicates weak correlation.\n\nUses in EFA:\n\nIdentifies patterns of correlations.\nVariables that are correlated are likely being influenced by the same underlying latent factor.\nHelps in determining the suitability of data for factor analysis.\nUse the correlation matrix (R Matrix) to check for clusters of variables that are correlated. If there is a lack of clusters then your data may not be suitable for EFA.\nChecking the correlation matrix (R Matrix) for a large dataset can be difficult."
  },
  {
    "objectID": "Lectures/introduction-to-EFA.html#measurement-theory",
    "href": "Lectures/introduction-to-EFA.html#measurement-theory",
    "title": "Introduction to Exploratory Factor Analysis (EFA)",
    "section": "",
    "text": "Definition: The study of how to assign numbers to objects and events according to rules.\nComponents:\n\nConstructs: Theoretical concepts being measured (e.g., intelligence, satisfaction).\nIndicators: Observed variables that reflect or are influenced by the constructs.\n\nApplications:\n\nEnsuring that the indicators accurately and reliably measure or are influenced by the constructs.\nValidating the measurement instruments."
  },
  {
    "objectID": "Lectures/introduction-to-EFA.html#differences-between-pca-and-efa",
    "href": "Lectures/introduction-to-EFA.html#differences-between-pca-and-efa",
    "title": "Introduction to Exploratory Factor Analysis (EFA)",
    "section": "",
    "text": "Principal Component Analysis (PCA):\n\nAimed at reducing the dimensionality of the data.\nTransforms the variables into a new set of uncorrelated components.\nComponents are linear combinations of the original variables.\n\nExploratory Factor Analysis (EFA):\n\nResearcher uses EFA because they believe there are latent (unobserved) variables.\nSeeks to identify latent factors that explain the correlations among variables.\nFactors are assumed to cause the observed variables.\nFactors can be assumed to be correlated or or uncorrelated.\n\nKey Differences:\n\nGoal: PCA is descriptive, EFA is explanatory.\nComponents vs. Factors: PCA components are not based on underlying constructs, whereas EFA factors are."
  },
  {
    "objectID": "Lectures/introduction-to-EFA.html#pre-analysis-correlations-descriptives-etc.",
    "href": "Lectures/introduction-to-EFA.html#pre-analysis-correlations-descriptives-etc.",
    "title": "Introduction to Exploratory Factor Analysis (EFA)",
    "section": "",
    "text": "Steps:\n\nCheck for Correlations: Ensure variables are sufficiently correlated for EFA. Check R Matrix.\nDescriptive Statistics: Examine means, standard deviations, and distribution shapes. Floor and Ceiling Effects.\nAssumptions:\n\nAdequate sample size (at least 5-10 times the number of variables).\nNormality of variables.\nLinearity and homoscedasticity."
  },
  {
    "objectID": "Lectures/introduction-to-EFA.html#factoring-methods-principal-axis-maximum-likelihood",
    "href": "Lectures/introduction-to-EFA.html#factoring-methods-principal-axis-maximum-likelihood",
    "title": "Introduction to Exploratory Factor Analysis (EFA)",
    "section": "",
    "text": "Principal Axis Factoring (PAF):\n\nFocuses on shared variance.\nSuitable for data where normality cannot be assumed.\n\nMaximum Likelihood (ML):\n\nAssumes multivariate normality.\nProvides goodness-of-fit measures.\nUseful for hypothesis testing about the factor structure.\n\nChoosing the Method:\n\nDepends on the nature of the data and research objectives.\nML is preferred for confirmatory purposes, PAF for exploratory purposes."
  },
  {
    "objectID": "Lectures/introduction-to-EFA.html#prior-communality",
    "href": "Lectures/introduction-to-EFA.html#prior-communality",
    "title": "Introduction to Exploratory Factor Analysis (EFA)",
    "section": "",
    "text": "Definition: Estimate of the variance in each variable accounted for by the common factors.\nMethods of Estimation:\n\nSquared multiple correlations.\nSetting to a default value such as 1.\n\nRole in EFA:\n\nHelps in the initial extraction of factors.\nInfluences the factor solution and interpretation."
  },
  {
    "objectID": "Lectures/introduction-to-EFA.html#choosing-your-eigenvalue-rotation-method",
    "href": "Lectures/introduction-to-EFA.html#choosing-your-eigenvalue-rotation-method",
    "title": "Introduction to Exploratory Factor Analysis (EFA)",
    "section": "",
    "text": "Eigenvalue:\n\nRepresents the amount of variance explained by each factor.\nCommon rule: retain factors with eigenvalues greater than 1.\n\nRotation Methods:\n\nVarimax (Orthogonal):\n\nSimplifies the loadings to make interpretation easier.\nAssumes factors are uncorrelated.\n\nPromax (Oblique):\n\nAllows for correlated factors.\nProvides a more realistic representation of the data.\n\n\nPurpose of Rotation:\n\nEnhances interpretability of the factor solution.\nClarifies the structure by maximizing high loadings and minimizing low loadings on each factor."
  },
  {
    "objectID": "Lectures/introduction-to-EFA.html#picking-the-number-of-factors-and-understanding-loadings",
    "href": "Lectures/introduction-to-EFA.html#picking-the-number-of-factors-and-understanding-loadings",
    "title": "Introduction to Exploratory Factor Analysis (EFA)",
    "section": "",
    "text": "Eigenvalues Greater Than 1:\n\nUse the Kaiser criterion, which suggests retaining factors with eigenvalues greater than 1.\nFactors with eigenvalues above 1 explain more variance than a single observed variable.\n\nScree Plot:\n\nExamine the scree plot for an “elbow point,” where the slope of eigenvalues levels off.\nRetain factors before the point where the plot starts to flatten.\n\nParallel Analysis:\n\nCompare the observed eigenvalues to those obtained from random data of the same size.\nRetain factors with observed eigenvalues greater than the corresponding random eigenvalues.\n\n\n\n\nCumulative Variance Explained:\n\nConsider the total variance explained by the retained factors.\nAim for a solution that explains a substantial proportion of the total variance (e.g., 70-80%).\n\nTheoretical Considerations:\n\nConsider the theoretical framework and prior research.\nEnsure the number of factors makes sense in the context of the study’s conceptual model.\n\n\n\n\n\n\n\nFactor Loadings:\n\nRepresent the correlation between observed variables and the latent factor.\nLoadings range from -1 to 1, with higher absolute values indicating stronger relationships.\n\nSignificant Loadings:\n\nLoadings greater than ±0.30 are generally considered significant.\nHigher thresholds (e.g., ±0.50 or ±0.60) indicate more robust relationships.\n\nInterpreting Factors:\n\nLabel factors based on the variables with the highest loadings.\nConsider the conceptual meaning and relevance of the variables in the context of the study.\n\nRotation Methods:\n\nUse rotation methods (e.g., varimax, oblimin) to achieve a simpler, more interpretable structure.\nOrthogonal rotations (e.g., varimax) assume factors are uncorrelated, while oblique rotations (e.g., oblimin) allow for correlated factors.\n\nInterpreting Results:\n\nPicking the number of factors: use the scree plot or eigenvalue rule.\nFactor Loadings: Indicates the strength and direction of the relationship between variables and factors.\nCommunalities: Indicates the proportion of each variable’s variance explained by the factors.\nVariance Explained: The total variance accounted for by all the factors.\n\nWriting Up:\n\nDescribe the data and the preliminary analysis.\nDetail the extraction method and rotation used.\nShow scree plot and argue why you are choosing the number of factors you are.\nPresent the factor loadings, communalities, and variance explained via tables and figures.\nDiscuss the theoretical and practical implications of the factors.\n\n\nBy understanding the basics of EFA, latent variables, and their application in research, researchers can effectively use this technique to uncover underlying structures in their data, providing valuable insights and advancing knowledge in their field."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to NPHD9042",
    "section": "",
    "text": "Hello and welcome to NPHD 9042 – Applied Multivariate Analysis!\nI’m Dr. Joshua Lambert, and I’ll be your instructor for this course. I’m excited to guide you through this journey as we explore the fascinating world of advanced statistical methods and their application in nursing and healthcare research.\n\n\n\nEmail: joshua.lambert@uc.edu\n\nFeel free to reach out to me with any questions or concerns. I’m here to help you succeed in this course and deepen your understanding of the topics we will cover.\n\n\n\nNPHD9042 is designed to provide an in-depth exploration of advanced multivariate statistical methods relevant to healthcare research. The course emphasizes practical applications using statistical software. Topics include:\n\n\n\nOverview of multivariate techniques\nApplications in healthcare research\nMultivariate normal distribution\n\n\n\n\n\nPurpose and methodology of EFA\nExtraction methods and factor rotation\nInterpreting factor loadings\n\n\n\n\n\nMultiple regression techniques\nModel selection and validation\nDealing with multicollinearity\n\n\n\n\n\nBasics of SEM and path analysis\nModel specification and identification\nAssessing model fit\n\n\n\n\n\nDistinction between CFA and EFA\nApplication of CFA in healthcare research\nBasics of path analysis and its use in structural models\n\n\n\n\n\nFundamentals of CART modeling\nBuilding and interpreting decision trees\nApplications in clinical decision-making and research"
  },
  {
    "objectID": "index.html#how-to-reach-me",
    "href": "index.html#how-to-reach-me",
    "title": "Welcome to NPHD9042",
    "section": "",
    "text": "Email: joshua.lambert@uc.edu\n\nFeel free to reach out to me with any questions or concerns. I’m here to help you succeed in this course and deepen your understanding of the topics we will cover."
  },
  {
    "objectID": "index.html#overview-of-nphd9042",
    "href": "index.html#overview-of-nphd9042",
    "title": "Welcome to NPHD9042",
    "section": "",
    "text": "NPHD9042 is designed to provide an in-depth exploration of advanced multivariate statistical methods relevant to healthcare research. The course emphasizes practical applications using statistical software. Topics include:\n\n\n\nOverview of multivariate techniques\nApplications in healthcare research\nMultivariate normal distribution\n\n\n\n\n\nPurpose and methodology of EFA\nExtraction methods and factor rotation\nInterpreting factor loadings\n\n\n\n\n\nMultiple regression techniques\nModel selection and validation\nDealing with multicollinearity\n\n\n\n\n\nBasics of SEM and path analysis\nModel specification and identification\nAssessing model fit\n\n\n\n\n\nDistinction between CFA and EFA\nApplication of CFA in healthcare research\nBasics of path analysis and its use in structural models\n\n\n\n\n\nFundamentals of CART modeling\nBuilding and interpreting decision trees\nApplications in clinical decision-making and research"
  },
  {
    "objectID": "Articles/articles.html",
    "href": "Articles/articles.html",
    "title": "Articles",
    "section": "",
    "text": "Modeling and variable selection in epidemiologic analysis\n\nFirst author: Sander Greenland\nThis paper discusses strategies for modeling and variable selection in epidemiologic studies. It covers topics such as categorization of continuous variables, selection of control variables, and the use of stepwise regression methods. The author emphasizes the importance of considering both statistical criteria and subject-matter knowledge when selecting variables and building models in epidemiologic research.\n\nSecondary Data Analysis as an Efficient and Effective Approach to Nursing Research\n\nAuthor: Susan L. Dunn\nThis article discusses the benefits and limitations of secondary data analysis in nursing research. It highlights how secondary data analysis can be a cost-effective method to explore new research questions and hypotheses using existing data sets. The authors provide examples of secondary data analysis in nursing, emphasizing its potential to advance nursing knowledge and practice despite challenges such as data quality and the need for appropriate analytical skills.\n\nVariable Selection Strategies and Its Importance in Clinical Prediction Modelling\n\nAuthor: Mohammad Ziaul Islam Chowdhury\nThis paper provides an overview of variable selection methods in clinical prediction modeling, including backward elimination, forward selection, stepwise selection, and all possible subset selection. The authors discuss the importance of selecting appropriate variables to improve model accuracy and reduce complexity. They also address stopping rules and selection criteria like Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC), emphasizing the balance between model simplicity and fit.\n\nUnderstanding Poisson Regression\n\nAuthor: Matthew J. Hayat\nThe article introduces Poisson regression as a method for analyzing count data in nursing and health research. It explains the assumptions of the Poisson distribution and addresses issues like overdispersion and zero-inflation. The authors use data from the ENSPIRE study to illustrate the application of Poisson regression, comparing it with other regression methods and highlighting its advantages in handling count data accurately.\n\nOn exploratory factor analysis: A review of recent evidence, an assessment of current practice, and recommendations for future use\n\nFirst author: C.J. Gaskin\nThis paper reviews recent literature on exploratory factor analysis (EFA) and assesses current practices in nursing research. It focuses on five key decisions in EFA: determining sample size, choosing between EFA and principal component analysis, selecting the number of factors to retain, choosing data extraction methods, and deciding on factor rotation methods. The authors provide evidence-based recommendations for each decision to improve EFA practices in nursing research.\n\nExploratory factor analysis and principal component analysis in clinical studies: Which one should you use?\n\nFirst author: Mousa Alavi\nThis editorial compares exploratory factor analysis (EFA) and principal component analysis (PCA), highlighting their conceptual and statistical differences. It explains that EFA is used to uncover underlying factors, while PCA is primarily for data reduction. The authors provide guidance on choosing between EFA and PCA based on research objectives and data characteristics, using examples from clinical studies.\n\nTesting and Verifying Nursing Theory by Confirmatory Factor Analysis\n\nAuthor: Maria Kääriäinen\nThis article explores the use of confirmatory factor analysis (CFA) to test and verify nursing theories. The authors explain the phases of CFA, including preparation, model testing, and reporting results. They highlight the importance of CFA in validating theoretical constructs and relationships within nursing theories, providing a structured approach to empirical testing and theory development in nursing science.\n\nInterpreting evidence from structural equation modeling in nursing practice\n\nFirst author: Kathy Newman\nThis article introduces structural equation modeling (SEM) and its applications in nursing research. It explains the key concepts of SEM, including path analysis, latent variables, and model fit indices. The authors provide guidance on interpreting SEM results and discuss the strengths and limitations of this statistical approach for nursing practice.\n\nThe process and utility of classification and regression tree methodology in nursing research\n\nFirst author: Lisa Kuhn\nThis article provides an overview of classification and regression tree (CART) analysis and its applications in nursing research. It explains the key concepts and steps involved in CART methodology, including building baseline models, specifying causal models, and model trimming. The authors discuss the strengths and limitations of CART analysis and provide guidance on interpreting CART results for nursing practice.\n\nThe application of big data and the development of nursing science: A discussion paper\n\nFirst author: Ruifang Zhu\nThis paper examines the importance of big data in advancing nursing science and practice. It discusses the current state of big data research in nursing, including challenges in data sharing, standardization, and talent development. The authors propose establishing big data centers for nursing to facilitate resource sharing, unify standards, and build knowledge systems to support nursing research and practice.\n\nHow artificial intelligence is changing nursing\n\nFirst author: Nancy Robert\nThis article provides an overview of artificial intelligence (AI) and its impact on nursing practice. It discusses various AI technologies, including machine learning, deep learning, and robotics, and their applications in healthcare. The author examines how AI is changing nursing roles, the ethical considerations involved, and the future implications for the nursing profession."
  },
  {
    "objectID": "Other/syl.html",
    "href": "Other/syl.html",
    "title": "NPHD9040 - Syllabus",
    "section": "",
    "text": "NPHD9040 - Applied Multivariable Analysis\n\nClass Information\n\nClass Meeting Time and Location:\n\nTuesdays 9:00 AM - 11:50 AM\nProcter Hall 287 or Online via Microsoft Teams\n\nCredit Hours/Contact Hours:\n\n3 graduate credit hours / 45 contact hours\n\n\n\n\nFaculty Information\n\nFaculty Name:\n\nJoshua W Lambert, PhD, MS, MS\n\nEmail:\n\nJoshua.lambert@uc.edu\n\nStudent First Office Hours:\n\nBy appointment (please email) in person or online via Teams\n\n\n\n\nCourse Pre/Co-Requisites\nTo take this course, you must: - Have taken the following courses: NPHD9000 and 9040 (minimum grade C/Pass)\n\n\nCourse Description\nThis course is the first of two that introduce advanced statistical methods used in doctoral-level nursing research. Method selection, application, results interpretation, and presentation are stressed in a flipped-classroom/workshop format. Methods introduced include partial correlation, multi-way ANOVA, ANCOVA, multiple regression, logistic regression, and multilevel models. An introduction to power and sample size calculation by method is also covered.\n\n\nStudent Learning Outcomes\nUpon completion of the course, students will be able to: 1. Select the appropriate multivariate analysis method based on research objectives and types of variables to be analyzed. 2. Generate required sample size and power for prototypical analyses by multivariate method, concurrently addressing inclusion of under-represented groups in research. 3. Display an introductory level ability to manage data and conduct preliminary analyses to assess and address data quality and potential violations of statistical assumptions by multivariate method. 4. Demonstrate an introductory level ability to generate results, using selected statistical analysis software (e.g., JMP, SPSS, SAS or R), and interpret relevant results generated for each method.\n\n\nTeaching Strategies\n\nWeekly Lectures\nAcademic Methodology Journal Articles\nStatistical Software Workshops\nQuizzes\nGroup Project Proposals\nGroup Presentations\nGroup Papers\nClass Peer Review\n\nActive learning strategies will be used throughout the semester. Examples include faculty and student-led discussions, short paper reviews, creating professional academic documents, and presentations.\nThe textbook readings and additional resources posted in the modules provide background information so you can explore, learn about, discuss, and debate the use of statistical analysis in research.\nGrammar, spelling, and correct statistical formatting are critical components of scholarly written communication and all professional communication. Please include references when referring to the work of others.\n\n\nCourse Activities\nCourse activities will be conducted within the Canvas course management system. All communication should take place through Canvas, email (joshua.lambert@uc.edu), or the Teams page.\n\n\nCourse Format\n\nFlipped-classroom/workshop format\nCombination of lectures, discussions, and practical exercises\n\n\n\nElectronic Communication Policy\n\nAll students are required to register for Canvas for this course. Please check that your correct email address is listed.\nCheck the course site regularly (at least 3 times per week).\nAccess to Microsoft Office files (Word, Excel, Teams) is required.\nYou must check your UC email account regularly, as faculty will use this for communication.\n\n\n\nCourse Materials\n\nResources\nThe main portion of the course content will be through Dr. Lambert’s weekly lecture notes on Canvas. The books listed below are recommended if your preferred learning modality is reading. Lectures will be recorded and posted on Canvas for review.\n\n\nBooks (Optional)\n\nDiscovering Statistics Using IBM SPSS Statistics\n\n4th Edition (Paper) approximately $35 on Amazon\n5th Edition (Paper) approximately $65 on Amazon\n\nNursing Research: Generating and Assessing Evidence for Nursing Practice\n\nTenth, North American Edition (Paper) approximately $60 on Amazon\n\n\n\n\nAdditional Resources\nContact Dr. Lambert if additional resources are needed.\n\n\nWeb-Based Resources\n\nWebsites and videos will be provided as the course progresses\n\n\n\nRequired Technology\n\nMicrosoft Office (Word, Excel, Teams)\nJMP (Free to UC College of Nursing students)\n\nSee Canvas page for download and installation instructions\n\nInternet access\n\n\n\n\nClassroom Procedures/Policies\n\nAcademic Integrity\nThe University Rules, including the Student Code of Conduct, and other documented policies of the department, college, and university related to academic integrity will be enforced. Any violation, including plagiarism or cheating, will be dealt with individually based on severity.\n\nAcademic Integrity\nCode of Conduct\n\n\n\nSpecial Needs and Accommodations Policy\nIf you have special needs related to your participation in the course, meet with the Disability Services Office to arrange reasonable accommodations. Contact them at 513-556-6823 or 210 University Pavilion (main campus).\n\nDisability Services\n\n\n\nAttendance Policy\nActive participation in classroom and online discussions and activities is vital. Plan to spend at least 8-10 hours per week preparing for course activities and completing assignments.\n\nReligious Observances Statement\n\nIf you will be absent, email Dr. Lambert ahead of time.\n\n\nLate Work Policy\nAssignments are due by midnight on the date indicated unless otherwise stated. If you anticipate a conflict with due dates, email Dr. Lambert. Late assignments without prior approval may receive a score as low as 0%.\n\n\nGrade Dispute\nFor grade disputes, first appeal in writing to Dr. Lambert. If not resolved, escalate to the program director, following the university’s grade dispute policy.\n\n\nNetiquette\nMaintain respectful and efficient communication using web-based tools.\n\n\nChanges in Course Syllabus\nThis syllabus is an expected projection of course progress. It may be revised due to circumstances, and changes will be communicated promptly.\n\n\n\nGrading Policy\nThe following scale will be used for grading all course graded activities and deriving the final course grade: - A+: 97-100% - A: 93-96.99% - A-: 90-92.99% - B+: 87-89.99% - B: 83-86.99% - B-: 80-82.99% - C+: 77-79.99% - C: 73-76.99% - C-: 70-72.99% - D+: 67-69.99% - D: 63-66.99% - D-: 60-62.99% - F: Below 60%\nAdditional Notes: - Dr. Lambert may provide additional grading notes during the course."
  },
  {
    "objectID": "Assignments/hw2.html",
    "href": "Assignments/hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Over 7000 participants were surveyed during the 2015-2016 National Health and Nutrition Examination Survey (NHANES) survey collection. NHANES is a “program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations.” A researcher is interested in understanding how the variables within the dataset (HW1_NHANES_Data.csv) explain variation in the number of physical health days that a subject reported in the last 30 days (PhysicalHealthDaysinLast30) using multiple linear regression. The researcher is also interested in using logistic regression to understand what variables explain variation in whether the subject reported no days or more than 1 (PhysicalHealthDaysinLast30_2). After exclusion criteria were considered and missing data removed, a final sample of 11377 subjects remained in the dataset. You were assigned to help this researcher explore these relationships using forward backward selection as well as multi-step (hierarchical regression).\nThe variables included in the dataset are:\n\n\n\n\n\n\n\n\nNHANES Variable\nVariable Description\nOther Info\n\n\n\n\nSubjectID\nSubject ID (not to be used in regression)\n\n\n\nPhysicalHealthDaysinLast30\nNumber of days the subject reported having a physical health day\nDependent Variable (Linear Regression)\n\n\nPhysicalHealthDaysinLast30_2\n0=Subject reported zero days; 1=Subject reported &gt;0 physical health days\nDependent Variable (Logistic Regression)\n\n\nRace\nWhite, Black, MA (Mexican American or Other Hispanic), Other\n\n\n\nEdu\nLTHS=Less than High School; HS=High School; GTHS=Greater Than High School\n\n\n\nSes\nFamily Income to Poverty. Less than 1 indicates that the subject’s family earns less than the poverty line for the area. Greater than 1 indicates that the subject’s family earns more than the poverty line for the area.\n\n\n\nBMXBMI\nBody Mass Index\n\n\n\nsmoke\nNon-Smoker, Current Smoker, Former Smoker\n\n\n\nDiab\nDiabetic, Not Diabetic\n\n\n\nLBXALC\n0 if below median; 1 if above median – Alpha-carotene (ug/dL)\nMedian=2.6\n\n\nLBXBEC\n0 if below median; 1 if above median – trans-b-carotene (ug/dL)\nMedian=11.8\n\n\nLBXCRY\n0 if below median; 1 if above median – b-cryptoxanthin (ug/dL)\nMedian=7.9\n\n\nLBXGTC\n0 if below median; 1 if above median – g-tocopherol (ug/dL)\nMedian=207\n\n\nLBXLUZ\n0 if below median; 1 if above median – Lutein/zeaxanthin (ug/dL)\nMedian=14.4\n\n\nLBXLYC\n0 if below median; 1 if above median – trans-lycopene (ug/dL)\nMedian=20.84\n\n\nLBXVIA\n0 if below median; 1 if above median – Retinol (ug/dL)\nMedian=56\n\n\nLBXVIE\n0 if below median; 1 if above median – Vitamin E (ug/dL)\nMedian=1129.5\n\n\n\nFollow each step and provide tables, figures, and explanations when appropriate. Use the NHANES.sav dataset and answer the following questions using a statistical software of your choice. NHANES\n\n\n\nBegin by creating dummy variables for each character variable (for example: Edu). Make sure your reference groups make sense.\nCreate a demographic table stratified by whether the subject had 0 physical health days or more than 0 physical health days. All 14 independent variables and PhysicalHealthDaysinLast30 should be included. The table should display frequency and percentage for each dummy variable and continuous variables should display the mean, median, and standard deviation. You can make two tables (one for dummy variables and one for continuous) if that is easier.\nMake a histogram for each continuous variable.\nLook at the tables and figures you have created. What do you notice? Are there any variables that have odd distributions?\n\n\n\n\n\nAfter completing the pre-processing step, begin by completing simple linear regression with the outcome PhysicalHealthDaysinLast30 for each of the 14 independent variables (by the time you make your dummy variables you will have more than this). Report your findings in a table like this:\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nβ Estimate\nP-Value\n95% CI\n\n\n\n\nSES (Below 1)\n\n\n\n\n\nBMI\n\n\n\n\n\n…\n\n\n\n\n\n\n\nInterpret the results for BMI.\nInterpret the results for SES (Below 1).\nComplete forward selection using one of the criteria we talked about in class.\n\nDescribe the forward selection method and criteria you chose in 3-4 sentences.\nMake a table which displays your final multivariable model like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nβ Estimate\nP-Value\n95% CI\n\n\n\n\nSES (Below 1)\n\n\n\n\n\nBMI\n\n\n\n\n\n…\n\n\n\n\n\n\n\nInterpret the results for one of the variables retained by forward selection.\nComplete backward selection using one of the criteria we talked about in class.\n\nDescribe the backward selection method and criteria you chose in 3-4 sentences.\nMake a table which displays your final multivariable model like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nβ Estimate\nP-Value\n95% CI\n\n\n\n\nSES (Below 1)\n\n\n\n\n\nBMI\n\n\n\n\n\n…\n\n\n\n\n\n\n\nInterpret the results for one of the variables retained by backward selection.\nThe researcher tells you that current literature overwhelmingly says that both diabetes and BMI affect physical health. They also tell you that there is one paper that supports smoking effects physical health. Then they say that they hypothesize that higher levels of Vitamin E and Alpha-carotene will lower the number of physical health days. Use multi-step (hierarchical regression) to test the researchers’ hypotheses while respecting what the past research says.\n\nMake a table which displays your final multivariable model like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nβ Estimate\nP-Value\n95% CI\n\n\n\n\nStep 1\n\n\n\n\n\nDiabetes\n\n\n\n\n\nBMI\n\n\n\n\n\nStep 2\n\n\n\n\n\nSmoking (Former)\n\n\n\n\n\nSmoking (Current)\n\n\n\n\n\nStep 3\n\n\n\n\n\nVitamin E &gt; Median=1129.5 (compared to &lt;Median)\n\n\n\n\n\nACarotene &gt; Median=2.6 (compared to &lt;Median)\n\n\n\n\n\n\n\nInterpret the results for Vitamin E and Alpha-carotene.\n\n\n\n\n\n\nCompare and contrast the results from forward, backward, and multi-step selection strategies.\n\nWhich method did you like the most?\nWhich method did you trust the most?\nHow did the estimates compare?\nWhat are some pitfalls to each of the three methods?\nOther thoughts?\n\n\n\n\n\n\nUsing PhysicalHealthDaysinLast30_2, complete forward selection using one of the criteria we talked about in class.\n\nDescribe the forward selection method and criteria you chose in 3-4 sentences.\nMake a table which displays your final multivariable model like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nExp(β) Estimate\nP-Value\n95% CI\n\n\n\n\nSES (Below 1)\n\n\n\n\n\nBMI\n\n\n\n\n\n…\n\n\n\n\n\n\n\nInterpret the results for one of the variables retained by forward selection.\n\n\nHomework 1 NPHD 9042 Name: __________________"
  },
  {
    "objectID": "Assignments/hw2.html#data-pre-processing",
    "href": "Assignments/hw2.html#data-pre-processing",
    "title": "Homework 2",
    "section": "",
    "text": "Begin by creating dummy variables for each character variable (for example: Edu). Make sure your reference groups make sense.\nCreate a demographic table stratified by whether the subject had 0 physical health days or more than 0 physical health days. All 14 independent variables and PhysicalHealthDaysinLast30 should be included. The table should display frequency and percentage for each dummy variable and continuous variables should display the mean, median, and standard deviation. You can make two tables (one for dummy variables and one for continuous) if that is easier.\nMake a histogram for each continuous variable.\nLook at the tables and figures you have created. What do you notice? Are there any variables that have odd distributions?"
  },
  {
    "objectID": "Assignments/hw2.html#multiple-linear-regression",
    "href": "Assignments/hw2.html#multiple-linear-regression",
    "title": "Homework 2",
    "section": "",
    "text": "After completing the pre-processing step, begin by completing simple linear regression with the outcome PhysicalHealthDaysinLast30 for each of the 14 independent variables (by the time you make your dummy variables you will have more than this). Report your findings in a table like this:\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nβ Estimate\nP-Value\n95% CI\n\n\n\n\nSES (Below 1)\n\n\n\n\n\nBMI\n\n\n\n\n\n…\n\n\n\n\n\n\n\nInterpret the results for BMI.\nInterpret the results for SES (Below 1).\nComplete forward selection using one of the criteria we talked about in class.\n\nDescribe the forward selection method and criteria you chose in 3-4 sentences.\nMake a table which displays your final multivariable model like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nβ Estimate\nP-Value\n95% CI\n\n\n\n\nSES (Below 1)\n\n\n\n\n\nBMI\n\n\n\n\n\n…\n\n\n\n\n\n\n\nInterpret the results for one of the variables retained by forward selection.\nComplete backward selection using one of the criteria we talked about in class.\n\nDescribe the backward selection method and criteria you chose in 3-4 sentences.\nMake a table which displays your final multivariable model like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nβ Estimate\nP-Value\n95% CI\n\n\n\n\nSES (Below 1)\n\n\n\n\n\nBMI\n\n\n\n\n\n…\n\n\n\n\n\n\n\nInterpret the results for one of the variables retained by backward selection.\nThe researcher tells you that current literature overwhelmingly says that both diabetes and BMI affect physical health. They also tell you that there is one paper that supports smoking effects physical health. Then they say that they hypothesize that higher levels of Vitamin E and Alpha-carotene will lower the number of physical health days. Use multi-step (hierarchical regression) to test the researchers’ hypotheses while respecting what the past research says.\n\nMake a table which displays your final multivariable model like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nβ Estimate\nP-Value\n95% CI\n\n\n\n\nStep 1\n\n\n\n\n\nDiabetes\n\n\n\n\n\nBMI\n\n\n\n\n\nStep 2\n\n\n\n\n\nSmoking (Former)\n\n\n\n\n\nSmoking (Current)\n\n\n\n\n\nStep 3\n\n\n\n\n\nVitamin E &gt; Median=1129.5 (compared to &lt;Median)\n\n\n\n\n\nACarotene &gt; Median=2.6 (compared to &lt;Median)\n\n\n\n\n\n\n\nInterpret the results for Vitamin E and Alpha-carotene."
  },
  {
    "objectID": "Assignments/hw2.html#comparison-of-methods",
    "href": "Assignments/hw2.html#comparison-of-methods",
    "title": "Homework 2",
    "section": "",
    "text": "Compare and contrast the results from forward, backward, and multi-step selection strategies.\n\nWhich method did you like the most?\nWhich method did you trust the most?\nHow did the estimates compare?\nWhat are some pitfalls to each of the three methods?\nOther thoughts?"
  },
  {
    "objectID": "Assignments/hw2.html#multiple-logistic-regression",
    "href": "Assignments/hw2.html#multiple-logistic-regression",
    "title": "Homework 2",
    "section": "",
    "text": "Using PhysicalHealthDaysinLast30_2, complete forward selection using one of the criteria we talked about in class.\n\nDescribe the forward selection method and criteria you chose in 3-4 sentences.\nMake a table which displays your final multivariable model like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nExp(β) Estimate\nP-Value\n95% CI\n\n\n\n\nSES (Below 1)\n\n\n\n\n\nBMI\n\n\n\n\n\n…\n\n\n\n\n\n\n\nInterpret the results for one of the variables retained by forward selection.\n\n\nHomework 1 NPHD 9042 Name: __________________"
  },
  {
    "objectID": "Assignments/hw1.html",
    "href": "Assignments/hw1.html",
    "title": "Homework 1 Exploratory Factor Analysis",
    "section": "",
    "text": "This assignment uses the Williams.sav\nUse the Williams.sav dataset to complete an exploratory factor analysis. A description of the data and the questions and the dataset can be found on Canvas. Answer the following questions using a statistical software of your choice.\n\n\n\n\nConstruct a correlation matrix of the survey items. Put that here and discuss observations that you have.\n\n\n\n\nWhat is the conclusion of Bartlett’s Test for Sphericity?\nWhat does the varimax method assume?\nHow many factors have eigenvalues &gt; 1?\nExtract that many factors. What are the rotated factor loadings? Put the table here.\nWhat percent of variance do these factors cumulatively explain?\nHow would you interpret these factors? What labels would you give them?\n\n\n\n\n\nWhy do we not do Bartlett’s Test for PAF?\nWhat does the varimax method assume?\nHow many factors have eigenvalues &gt; 1?\nExtract that many factors. What are the rotated factor loadings? Put the table here.\nWhat percent of variance do these factors cumulatively explain?\nHow would you interpret these factors? What labels would you give them?\n\n\n\n\n\nWhat does the varimax method assume?\nHow many factors have eigenvalues &gt; 1?\nExtract that many factors. What are the rotated factor loadings? Put the table here.\nWhat percent of variance do these factors cumulatively explain?\nHow would you interpret these factors? What labels would you give them?\n\n\n\n\nCompare and contrast the analyses from questions 2 to 4. Write one to two paragraphs describing their similarities and differences. Which"
  },
  {
    "objectID": "Assignments/hw1.html#questions",
    "href": "Assignments/hw1.html#questions",
    "title": "Homework 1 Exploratory Factor Analysis",
    "section": "",
    "text": "Construct a correlation matrix of the survey items. Put that here and discuss observations that you have.\n\n\n\n\nWhat is the conclusion of Bartlett’s Test for Sphericity?\nWhat does the varimax method assume?\nHow many factors have eigenvalues &gt; 1?\nExtract that many factors. What are the rotated factor loadings? Put the table here.\nWhat percent of variance do these factors cumulatively explain?\nHow would you interpret these factors? What labels would you give them?\n\n\n\n\n\nWhy do we not do Bartlett’s Test for PAF?\nWhat does the varimax method assume?\nHow many factors have eigenvalues &gt; 1?\nExtract that many factors. What are the rotated factor loadings? Put the table here.\nWhat percent of variance do these factors cumulatively explain?\nHow would you interpret these factors? What labels would you give them?\n\n\n\n\n\nWhat does the varimax method assume?\nHow many factors have eigenvalues &gt; 1?\nExtract that many factors. What are the rotated factor loadings? Put the table here.\nWhat percent of variance do these factors cumulatively explain?\nHow would you interpret these factors? What labels would you give them?\n\n\n\n\nCompare and contrast the analyses from questions 2 to 4. Write one to two paragraphs describing their similarities and differences. Which"
  },
  {
    "objectID": "Assignments/hw3.html",
    "href": "Assignments/hw3.html",
    "title": "Homework 3",
    "section": "",
    "text": "This assignment uses the hsb2.csv"
  },
  {
    "objectID": "Assignments/hw3.html#data-dictionary-for-hsb2.csv",
    "href": "Assignments/hw3.html#data-dictionary-for-hsb2.csv",
    "title": "Homework 3",
    "section": "Data Dictionary for hsb2.csv",
    "text": "Data Dictionary for hsb2.csv\nTwo hundred observations were randomly sampled from the High School and Beyond survey, a survey conducted on high school seniors by the National Center of Education Statistics.\n200 observations and 11 variables:\n\nId: unique identifier\nFemale: 0=male, 1=female\nRace: 1=Hispanic, 2=Asian, 3=African-American, 4=Caucasian\nSES: 1=Low, 2=Middle, 3=High\nSchtype: 1=Public, 2=Private\nProg: 1=General, 2=Academic, 3=Vocational\nRead: Reading Score\nWrite: Writing Score\nMath: Math Score\nScience: Science Score\nSocst: Social Studies Score"
  },
  {
    "objectID": "Assignments/hw3.html#questions",
    "href": "Assignments/hw3.html#questions",
    "title": "Homework 3",
    "section": "Questions",
    "text": "Questions\n\n1. Use hsb2.csv to estimate the parameters\n\nEstimate the parameters: a1, a2, b1, b2, b3, e1, e2.\nQuestions:\n\nWhat do they mean?\nWhat seems to have the strongest relationship?\nWhat are the purposes of this path analysis?\nWhat does it tell you?\nWhat doesn’t it tell you?\n\n\nPlease explain your answers.\n\n\n\n2. Use hsb2.csv to estimate the parameters (ignore e1-e5)\n\nEstimate the parameters: a1-a4.\nInstructions:\n\nUse SEM (Structural Equation Modeling) to estimate the parameters.\nYou can use JMP or AMOS to answer this question.\nThe dependent variable is a latent variable Acad with three observed indicators: math, science, and socst.\nThere are two additional observed variables: the independent variable female and a mediator variable read.\n\nQuestions:\n\nWhat do these estimates mean?\nFrom this model, how does read affect Acad?\nDoes Acad affect the three STEM measures?\nDoes female affect read?"
  },
  {
    "objectID": "Assignments/hw4.html",
    "href": "Assignments/hw4.html",
    "title": "Homework 4",
    "section": "",
    "text": "In this homework, you will create a classification tree. This classification tree will classify observations into those that have a science score above the mean and those that have a science score at or below the mean. You will use all available variables (besides Id and the outcome) to create your classification tree. To know whether your tree is good at predicting out-of-sample observations, you will randomly use 75% of the data for training and 25% for testing (or validation). Use the hsb2.csv to create the classification tree to classify whether participants have a science score at or below the mean. You can choose whichever options you would like in SPSS or JMP to create your tree.\nWith your final tree created, please answer the following questions:\n\nWhat criteria did you use for creating your tree?\nPaste a picture of your tree built from the training data.\nHow many splits does the tree have?\nDo the splits make sense? Do any of them not make sense?\nWhat is the classification accuracy in the training data of your tree?\nWhat is the classification accuracy in the testing data of your tree?\nHow does the training and testing classification compare? What does that tell you about your tree?\nDo you trust the tree? Would you feel confident in its ability to predict? Would you trust it with your own prediction?\nHow might this work in the medical field? What are the upsides? Downsides?"
  },
  {
    "objectID": "Assignments/hw4.html#homework-4",
    "href": "Assignments/hw4.html#homework-4",
    "title": "Homework 4",
    "section": "",
    "text": "In this homework, you will create a classification tree. This classification tree will classify observations into those that have a science score above the mean and those that have a science score at or below the mean. You will use all available variables (besides Id and the outcome) to create your classification tree. To know whether your tree is good at predicting out-of-sample observations, you will randomly use 75% of the data for training and 25% for testing (or validation). Use the hsb2.csv to create the classification tree to classify whether participants have a science score at or below the mean. You can choose whichever options you would like in SPSS or JMP to create your tree.\nWith your final tree created, please answer the following questions:\n\nWhat criteria did you use for creating your tree?\nPaste a picture of your tree built from the training data.\nHow many splits does the tree have?\nDo the splits make sense? Do any of them not make sense?\nWhat is the classification accuracy in the training data of your tree?\nWhat is the classification accuracy in the testing data of your tree?\nHow does the training and testing classification compare? What does that tell you about your tree?\nDo you trust the tree? Would you feel confident in its ability to predict? Would you trust it with your own prediction?\nHow might this work in the medical field? What are the upsides? Downsides?"
  },
  {
    "objectID": "Flashcards/VocabJargon.html",
    "href": "Flashcards/VocabJargon.html",
    "title": "Vocabulary & Jargon List",
    "section": "",
    "text": "A Bartlett test is used to check if there are relationships between variables before conducting factor analysis."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#bartletts-test-of-sphericity",
    "href": "Flashcards/VocabJargon.html#bartletts-test-of-sphericity",
    "title": "Vocabulary & Jargon List",
    "section": "",
    "text": "A Bartlett test is used to check if there are relationships between variables before conducting factor analysis."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#bootstrapping",
    "href": "Flashcards/VocabJargon.html#bootstrapping",
    "title": "Vocabulary & Jargon List",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nA resampling technique used to estimate statistics on a population by sampling a dataset with replacement."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#canonical-correlation-analysis",
    "href": "Flashcards/VocabJargon.html#canonical-correlation-analysis",
    "title": "Vocabulary & Jargon List",
    "section": "Canonical Correlation Analysis",
    "text": "Canonical Correlation Analysis\nA method for exploring the relationships between two sets of variables."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#cfi-comparative-fit-index",
    "href": "Flashcards/VocabJargon.html#cfi-comparative-fit-index",
    "title": "Vocabulary & Jargon List",
    "section": "CFI (Comparative Fit Index)",
    "text": "CFI (Comparative Fit Index)\nA fit index that compares the fit of a target model to the fit of an independent, or null, model."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#chi-square",
    "href": "Flashcards/VocabJargon.html#chi-square",
    "title": "Vocabulary & Jargon List",
    "section": "Chi-Square",
    "text": "Chi-Square\nA statistical test that assesses the overall fit of a model by comparing the observed covariance matrix to the model-implied covariance matrix."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#cluster-analysis",
    "href": "Flashcards/VocabJargon.html#cluster-analysis",
    "title": "Vocabulary & Jargon List",
    "section": "Cluster Analysis",
    "text": "Cluster Analysis\nA technique for grouping similar objects or observations into clusters."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#communalities",
    "href": "Flashcards/VocabJargon.html#communalities",
    "title": "Vocabulary & Jargon List",
    "section": "Communalities",
    "text": "Communalities\nThe proportion of each variable’s variance that can be explained by the factors in factor analysis."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#confirmatory-factor-analysis-cfa",
    "href": "Flashcards/VocabJargon.html#confirmatory-factor-analysis-cfa",
    "title": "Vocabulary & Jargon List",
    "section": "Confirmatory Factor Analysis (CFA)",
    "text": "Confirmatory Factor Analysis (CFA)\nA statistical technique used to test how well measured variables represent the number of constructs."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#correlation-matrix",
    "href": "Flashcards/VocabJargon.html#correlation-matrix",
    "title": "Vocabulary & Jargon List",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\nA table showing correlation coefficients between variables in a dataset."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#covariance-matrix",
    "href": "Flashcards/VocabJargon.html#covariance-matrix",
    "title": "Vocabulary & Jargon List",
    "section": "Covariance Matrix",
    "text": "Covariance Matrix\nA square matrix showing the covariances between pairs of variables in a dataset."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#cross-loadings",
    "href": "Flashcards/VocabJargon.html#cross-loadings",
    "title": "Vocabulary & Jargon List",
    "section": "Cross-Loadings",
    "text": "Cross-Loadings\nWhen an indicator variable has a high loading on more than one factor."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#degrees-of-freedom",
    "href": "Flashcards/VocabJargon.html#degrees-of-freedom",
    "title": "Vocabulary & Jargon List",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nThe number of values in a study that are free to vary."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#direct-effect",
    "href": "Flashcards/VocabJargon.html#direct-effect",
    "title": "Vocabulary & Jargon List",
    "section": "Direct Effect",
    "text": "Direct Effect\nThe influence of one variable on another that is not mediated by any other variable in a causal model."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#discriminant-analysis",
    "href": "Flashcards/VocabJargon.html#discriminant-analysis",
    "title": "Vocabulary & Jargon List",
    "section": "Discriminant Analysis",
    "text": "Discriminant Analysis\nA method used to classify a set of observations into predefined classes based on a combination of features."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#eigenvalues",
    "href": "Flashcards/VocabJargon.html#eigenvalues",
    "title": "Vocabulary & Jargon List",
    "section": "Eigenvalues",
    "text": "Eigenvalues\nValues that represent the amount of variance explained by each factor or component in factor analysis or PCA."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#endogenous-variables",
    "href": "Flashcards/VocabJargon.html#endogenous-variables",
    "title": "Vocabulary & Jargon List",
    "section": "Endogenous Variables",
    "text": "Endogenous Variables\nDependent variables in a model that are influenced by other variables within the model."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#exogenous-variables",
    "href": "Flashcards/VocabJargon.html#exogenous-variables",
    "title": "Vocabulary & Jargon List",
    "section": "Exogenous Variables",
    "text": "Exogenous Variables\nIndependent variables in a model that are not influenced by other variables within the model."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#exploratory-factor-analysis-efa",
    "href": "Flashcards/VocabJargon.html#exploratory-factor-analysis-efa",
    "title": "Vocabulary & Jargon List",
    "section": "Exploratory Factor Analysis (EFA)",
    "text": "Exploratory Factor Analysis (EFA)\nA statistical method used to uncover the underlying structure of a set of variables and identify latent (unobserved) factors."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#factor-loadings",
    "href": "Flashcards/VocabJargon.html#factor-loadings",
    "title": "Vocabulary & Jargon List",
    "section": "Factor Loadings",
    "text": "Factor Loadings\nCorrelation coefficients between variables and factors in factor analysis."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#factor-scores",
    "href": "Flashcards/VocabJargon.html#factor-scores",
    "title": "Vocabulary & Jargon List",
    "section": "Factor Scores",
    "text": "Factor Scores\nComposite variables which provide information about an individual’s placement on the factor(s)."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#generalized-least-squares-gls",
    "href": "Flashcards/VocabJargon.html#generalized-least-squares-gls",
    "title": "Vocabulary & Jargon List",
    "section": "Generalized Least Squares (GLS)",
    "text": "Generalized Least Squares (GLS)\nAn alternative estimation method in SEM that is less sensitive to non-normality."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#goodness-of-fit-measures",
    "href": "Flashcards/VocabJargon.html#goodness-of-fit-measures",
    "title": "Vocabulary & Jargon List",
    "section": "Goodness-of-Fit Measures",
    "text": "Goodness-of-Fit Measures\nStatistical indicators that assess how well a model fits the observed data."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#homoscedasticity",
    "href": "Flashcards/VocabJargon.html#homoscedasticity",
    "title": "Vocabulary & Jargon List",
    "section": "Homoscedasticity",
    "text": "Homoscedasticity\nA condition where the variability of a variable is equal across the range of values of a second variable that predicts it."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#indirect-effect",
    "href": "Flashcards/VocabJargon.html#indirect-effect",
    "title": "Vocabulary & Jargon List",
    "section": "Indirect Effect",
    "text": "Indirect Effect\nThe influence of one variable on another that is mediated by at least one other variable in a causal model."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#kaiser-meyer-olkin-kmo-test",
    "href": "Flashcards/VocabJargon.html#kaiser-meyer-olkin-kmo-test",
    "title": "Vocabulary & Jargon List",
    "section": "Kaiser-Meyer-Olkin (KMO) Test",
    "text": "Kaiser-Meyer-Olkin (KMO) Test\nA measure of sampling adequacy for factor analysis."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#latent-variables",
    "href": "Flashcards/VocabJargon.html#latent-variables",
    "title": "Vocabulary & Jargon List",
    "section": "Latent Variables",
    "text": "Latent Variables\nVariables that are not directly observed but are inferred from other observed variables."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#manova-multivariate-analysis-of-variance",
    "href": "Flashcards/VocabJargon.html#manova-multivariate-analysis-of-variance",
    "title": "Vocabulary & Jargon List",
    "section": "MANOVA (Multivariate Analysis of Variance)",
    "text": "MANOVA (Multivariate Analysis of Variance)\nAn extension of ANOVA that allows for the comparison of multivariate sample means across groups."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#maximum-likelihood-estimation",
    "href": "Flashcards/VocabJargon.html#maximum-likelihood-estimation",
    "title": "Vocabulary & Jargon List",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\nA common method for estimating parameters in SEM and factor analysis."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#measurement-model",
    "href": "Flashcards/VocabJargon.html#measurement-model",
    "title": "Vocabulary & Jargon List",
    "section": "Measurement Model",
    "text": "Measurement Model\nThe part of an SEM that deals with the relationships between latent variables and their indicators."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#mediation",
    "href": "Flashcards/VocabJargon.html#mediation",
    "title": "Vocabulary & Jargon List",
    "section": "Mediation",
    "text": "Mediation\nAn analysis that explores whether the effect of an independent variable on a dependent variable is transmitted through a third variable."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#model-identification",
    "href": "Flashcards/VocabJargon.html#model-identification",
    "title": "Vocabulary & Jargon List",
    "section": "Model Identification",
    "text": "Model Identification\nThe condition where a unique solution can be found for all of the parameters in an SEM model."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#modification-indices",
    "href": "Flashcards/VocabJargon.html#modification-indices",
    "title": "Vocabulary & Jargon List",
    "section": "Modification Indices",
    "text": "Modification Indices\nSuggested changes to an SEM model that could improve its fit."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#moderation",
    "href": "Flashcards/VocabJargon.html#moderation",
    "title": "Vocabulary & Jargon List",
    "section": "Moderation",
    "text": "Moderation\nAn analysis that examines whether the relationship between two variables changes at different levels of a third variable."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#multicollinearity",
    "href": "Flashcards/VocabJargon.html#multicollinearity",
    "title": "Vocabulary & Jargon List",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nA condition where two or more predictor variables in a model are highly correlated, leading to unreliable estimates."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#multidimensional-scaling",
    "href": "Flashcards/VocabJargon.html#multidimensional-scaling",
    "title": "Vocabulary & Jargon List",
    "section": "Multidimensional Scaling",
    "text": "Multidimensional Scaling\nA technique for visualizing the level of similarity of individual cases in a dataset."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#multivariate-analysis",
    "href": "Flashcards/VocabJargon.html#multivariate-analysis",
    "title": "Vocabulary & Jargon List",
    "section": "Multivariate Analysis",
    "text": "Multivariate Analysis\nThe observation and analysis of more than one statistical outcome variable at a time."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#oblique-rotation",
    "href": "Flashcards/VocabJargon.html#oblique-rotation",
    "title": "Vocabulary & Jargon List",
    "section": "Oblique Rotation",
    "text": "Oblique Rotation\nA method in factor analysis that allows factors to be correlated with each other."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#observed-variables-manifest-variables",
    "href": "Flashcards/VocabJargon.html#observed-variables-manifest-variables",
    "title": "Vocabulary & Jargon List",
    "section": "Observed Variables (Manifest Variables)",
    "text": "Observed Variables (Manifest Variables)\nVariables that are directly measured or observed in a study."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#orthogonal-rotation",
    "href": "Flashcards/VocabJargon.html#orthogonal-rotation",
    "title": "Vocabulary & Jargon List",
    "section": "Orthogonal Rotation",
    "text": "Orthogonal Rotation\nA method in factor analysis that produces uncorrelated factors."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#parsimony",
    "href": "Flashcards/VocabJargon.html#parsimony",
    "title": "Vocabulary & Jargon List",
    "section": "Parsimony",
    "text": "Parsimony\nThe principle of using the simplest possible statistical model that still explains the data well."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#partial-least-squares-pls",
    "href": "Flashcards/VocabJargon.html#partial-least-squares-pls",
    "title": "Vocabulary & Jargon List",
    "section": "Partial Least Squares (PLS)",
    "text": "Partial Least Squares (PLS)\nA method for constructing predictive models when factors are many and highly collinear."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#path-analysis",
    "href": "Flashcards/VocabJargon.html#path-analysis",
    "title": "Vocabulary & Jargon List",
    "section": "Path Analysis",
    "text": "Path Analysis\nA statistical technique that examines direct and indirect relationships between variables using only observed variables."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#path-coefficients",
    "href": "Flashcards/VocabJargon.html#path-coefficients",
    "title": "Vocabulary & Jargon List",
    "section": "Path Coefficients",
    "text": "Path Coefficients\nValues that represent the strength and direction of relationships between variables in a model."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#path-diagrams",
    "href": "Flashcards/VocabJargon.html#path-diagrams",
    "title": "Vocabulary & Jargon List",
    "section": "Path Diagrams",
    "text": "Path Diagrams\nGraphical representations of the hypothesized relationships among variables in a model."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#principal-components-analysis-pca",
    "href": "Flashcards/VocabJargon.html#principal-components-analysis-pca",
    "title": "Vocabulary & Jargon List",
    "section": "Principal Components Analysis (PCA)",
    "text": "Principal Components Analysis (PCA)\nA technique used to emphasize variation and bring out strong patterns in a dataset by reducing its dimensionality."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#promax-rotation",
    "href": "Flashcards/VocabJargon.html#promax-rotation",
    "title": "Vocabulary & Jargon List",
    "section": "Promax Rotation",
    "text": "Promax Rotation\nA type of oblique rotation that allows factors to be correlated."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#rmsea-root-mean-square-error-of-approximation",
    "href": "Flashcards/VocabJargon.html#rmsea-root-mean-square-error-of-approximation",
    "title": "Vocabulary & Jargon List",
    "section": "RMSEA (Root Mean Square Error of Approximation)",
    "text": "RMSEA (Root Mean Square Error of Approximation)\nA fit index that measures the discrepancy between the hypothesized model and the population covariance matrix."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#scree-plot",
    "href": "Flashcards/VocabJargon.html#scree-plot",
    "title": "Vocabulary & Jargon List",
    "section": "Scree Plot",
    "text": "Scree Plot\nA graph used in factor analysis and PCA to determine the optimal number of factors or components to retain."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#structural-equation-modeling-sem",
    "href": "Flashcards/VocabJargon.html#structural-equation-modeling-sem",
    "title": "Vocabulary & Jargon List",
    "section": "Structural Equation Modeling (SEM)",
    "text": "Structural Equation Modeling (SEM)\nA comprehensive statistical approach that models complex relationships among both observed and latent variables."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#structural-model",
    "href": "Flashcards/VocabJargon.html#structural-model",
    "title": "Vocabulary & Jargon List",
    "section": "Structural Model",
    "text": "Structural Model\nThe part of an SEM that specifies the relationships among latent variables."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#suppressor-variable",
    "href": "Flashcards/VocabJargon.html#suppressor-variable",
    "title": "Vocabulary & Jargon List",
    "section": "Suppressor Variable",
    "text": "Suppressor Variable\nA variable that increases the predictive validity of another variable when included in a regression equation."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#tli-tucker-lewis-index",
    "href": "Flashcards/VocabJargon.html#tli-tucker-lewis-index",
    "title": "Vocabulary & Jargon List",
    "section": "TLI (Tucker-Lewis Index)",
    "text": "TLI (Tucker-Lewis Index)\nA fit index similar to CFI that penalizes for model complexity."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#total-effect",
    "href": "Flashcards/VocabJargon.html#total-effect",
    "title": "Vocabulary & Jargon List",
    "section": "Total Effect",
    "text": "Total Effect\nThe sum of the direct and indirect effects of one variable on another in a causal model."
  },
  {
    "objectID": "Flashcards/VocabJargon.html#varimax-rotation",
    "href": "Flashcards/VocabJargon.html#varimax-rotation",
    "title": "Vocabulary & Jargon List",
    "section": "Varimax Rotation",
    "text": "Varimax Rotation\nA specific type of orthogonal rotation that maximizes the variance of squared loadings for each factor. ```\nThe list is now in alphabetical order, making it easier to find specific terms."
  },
  {
    "objectID": "Other/aboutDrL.html",
    "href": "Other/aboutDrL.html",
    "title": "About Dr. Joshua Lambert",
    "section": "",
    "text": "Dr. Joshua Lambert holds a Ph.D. in Biostatistics and Epidemiology from the University of Kentucky (2017).\nHe has also earned master’s degrees in Statistics and Mathematics from University of Kentucky and Murray State University.\nCurrently, Dr. Lambert serves as an Associate Professor at the College of Nursing, University of Cincinnati."
  },
  {
    "objectID": "Other/aboutDrL.html#education-and-academic-positions",
    "href": "Other/aboutDrL.html#education-and-academic-positions",
    "title": "About Dr. Joshua Lambert",
    "section": "",
    "text": "Dr. Joshua Lambert holds a Ph.D. in Biostatistics and Epidemiology from the University of Kentucky (2017).\nHe has also earned master’s degrees in Statistics and Mathematics from University of Kentucky and Murray State University.\nCurrently, Dr. Lambert serves as an Associate Professor at the College of Nursing, University of Cincinnati."
  },
  {
    "objectID": "Other/aboutDrL.html#research-support-and-grants",
    "href": "Other/aboutDrL.html#research-support-and-grants",
    "title": "About Dr. Joshua Lambert",
    "section": "Research Support and Grants",
    "text": "Research Support and Grants\n\nDr. Lambert’s research endeavors have been supported by various grants, including prestigious federal and private funding agencies.\nNotably, he serves as Principal Investigator (PI) on projects funded by the National Library of Medicine and Co-Investigator (Co-I) on grants from the National Institute of Diabetes and Digestive and Kidney Diseases, among others.\nHis research spans diverse topics, from identifying clinically protective effects of existing drugs against COVID-19 to exploring outcomes in patients with acute kidney injury.\nMethodologically he works on subset selection for regression models, interaction identification when exhuastive search is not possible, and Pareto Frontier based solutions in Machine Learning and Statistics."
  },
  {
    "objectID": "Other/aboutDrL.html#publications-and-presentations",
    "href": "Other/aboutDrL.html#publications-and-presentations",
    "title": "About Dr. Joshua Lambert",
    "section": "Publications and Presentations",
    "text": "Publications and Presentations\n\nDr. Lambert has authored numerous peer-reviewed publications covering a wide range of topics in healthcare statistics including predictive modeling, opioid exposure, and acute kidney injury.\nHis scholarly work extends beyond publications to include invited presentations at esteemed conferences and institutions and showcasing his expertise and leadership in the field.\nAdditionally, he has contributed technical reports addressing critical public health issues such as drug overdose deaths and suicide attempts in Kentucky.\nResearchGate\nGoogle Scholar\nORCiD"
  },
  {
    "objectID": "Other/aboutDrL.html#teaching-and-academic-engagement",
    "href": "Other/aboutDrL.html#teaching-and-academic-engagement",
    "title": "About Dr. Joshua Lambert",
    "section": "Teaching and Academic Engagement",
    "text": "Teaching and Academic Engagement\n\nIn addition to his research pursuits, Dr. Lambert is actively involved in teaching and mentoring future healthcare professionals.\nHe has held positions as a lecturer in Mathematics and Statistics at various universities, fostering a passion for statistical literacy among students.\nDr. Lambert’s commitment to education is further evidenced by his presentations on statistical methodologies at academic conferences and workshops.\nDr. Lambert teaches NPHD9040 (Multivariate Analysis) and NPHD9042 (Multivariate Analysis) for the PhD program in the College of Nursing at the University of Cincinnati."
  },
  {
    "objectID": "Other/aboutDrL.html#contact-information",
    "href": "Other/aboutDrL.html#contact-information",
    "title": "About Dr. Joshua Lambert",
    "section": "Contact Information",
    "text": "Contact Information\n\nDr. Joshua Lambert can be reached at his academic office in Procter Hall.\nEmail: joshua.lambert@uc.edu"
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "Exploratory factor analysis and principal component analysis in clinical studies: Which one should you use?\nVariable selection strategies and its importance in clinical prediction modelling\nSecondary data analysis as an efficient and effective approach to nursing research\nOn exploratory factor analysis: A review of recent evidence, an assessment of current practice, and recommendations for future use\nModeling and variable selection in epidemiologic analysis.\nUnderstanding poisson regression. \nZero-inflated Poisson modeling of fall risk factors in community-dwelling older adults.\nTesting and verifying nursing theory by confirmatory factor analysis\nThe process and utility of classification and regression tree methodology in nursing research.\nInterpreting evidence from structural equation modeling in nursing practice.\nHow artificial intelligence is changing nursing\nA systematic review of structural equation modelling in nursing research\nThe application of big data and the development of nursing science_ A discussion paper."
  },
  {
    "objectID": "articles.html#articles",
    "href": "articles.html#articles",
    "title": "Articles",
    "section": "",
    "text": "Exploratory factor analysis and principal component analysis in clinical studies: Which one should you use?\nVariable selection strategies and its importance in clinical prediction modelling\nSecondary data analysis as an efficient and effective approach to nursing research\nOn exploratory factor analysis: A review of recent evidence, an assessment of current practice, and recommendations for future use\nModeling and variable selection in epidemiologic analysis.\nUnderstanding poisson regression. \nZero-inflated Poisson modeling of fall risk factors in community-dwelling older adults.\nTesting and verifying nursing theory by confirmatory factor analysis\nThe process and utility of classification and regression tree methodology in nursing research.\nInterpreting evidence from structural equation modeling in nursing practice.\nHow artificial intelligence is changing nursing\nA systematic review of structural equation modelling in nursing research\nThe application of big data and the development of nursing science_ A discussion paper."
  },
  {
    "objectID": "Lectures/introduction-to-multivariate.html",
    "href": "Lectures/introduction-to-multivariate.html",
    "title": "Introduction to Multivariate Analysis",
    "section": "",
    "text": "Multivariate analysis involves the observation and analysis of more than one statistical outcome variable at a time. This type of analysis is particularly powerful in healthcare research, where it allows for the examination of multiple outcome variables simultaneously, providing a more comprehensive understanding of complex phenomena.\n\nObservation and Analysis of Multiple Outcome Variables:\n\nMultivariate analysis considers multiple outcome variables simultaneously rather than analyzing them separately.\nThis approach helps to capture the interrelationships and interactions between different variables.\nExample: In a study examining the effectiveness of a new drug, researchers might look at how multiple outcomes such as blood pressure, cholesterol levels, and heart rate simultaneously differ collectively between a new drug group and a placebo group.\n\nApplication in Healthcare Research:\n\nIn healthcare, patient and clinical outcomes are often happening in a collective rather than independently.\nMultivariate analysis enables researchers to study these outcomes together, offering insights into how they collectively change based on independent variables.\nExample: Analyze how patient recovery (yes/no) impact lifestyle (diet, exercise) provides a more accurate picture than studying each factor in isolation.\n\nComprehensive Understanding of Complex Phenomena:\n\nBy examining multiple outcome variables at once, researchers can uncover patterns and relationships that may not be evident in uni/bivariate analysis.\n\n\n\n\n\nDiscriminant analysis is used to classify a set of observations into predefined classes. It works by finding a combination of features that best separates two or more classes of objects or events. This method is often used in healthcare for diagnostic purposes.\n\nPurpose of Discriminant Analysis:\n\nTo classify observations into predefined groups.\nTo identify the variables that differentiate between groups.\nExample: Classifying patients into different risk categories based on clinical measurements.\n\nProcess:\n\nCollect data on multiple predictor variables.\nDefine the group membership for each observation.\nApply discriminant function to identify the combination of variables that best separate the groups.\nExample: Using blood pressure, cholesterol, and BMI to predict whether a patient is at low, medium, or high risk for heart disease.\n\nApplications in Healthcare:\n\nUsed for diagnostic purposes, predicting outcomes, and identifying risk factors.\nHelps in making informed clinical decisions.\nExample: Determining whether a patient should undergo a specific treatment based on their risk classification.\n\n\nUse the Owl Diet.sav dataset to complete the example Discriminant Analysis. The Owl Diet dataset contains fictional information about types of owl diets.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Multivariate Methods  Discriminant.\n\n\nSelect your continuous outcome measurements as the Y, response, and the categorical predictor as X.\n\n\nClick OK.\n\n\n\n\n\nR Code Example:\n\n\n\n        #Install Packages\ninstall.packages(\"MASS\")\ninstall.packages(\"readr\")\nlibrary(readr)\nlibrary(MASS)\n\n# Read the data file\ndata &lt;-read_csv(\"Owl Diet.csv\")\nstr(data)\n\n#Perform the Discriminant Analysis\nlda_model &lt;- lda(`species` ~ `skull length` + `teeth row` + `palatine foramen` + `jaw length`, data = data)\nprint(lda_model)\nsummary(lda_model)\n\n      \n\n\n\n\n\nPython Code Example:\n\n\n\nSee details at https://medium.com/@glennlenormand/complete-guide-to-linear-discriminant-analysis-lda-in-python-d1255a5983b0\n# Demonstraton of LDA with IRIS data in python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_lda = lda.fit_transform(X, y)\n\nplt.xlabel('LD1')\nplt.ylabel('LD2')\nplt.scatter(X_lda[:,0], X_lda[:,1], c=y, cmap='rainbow', alpha=0.7, edgecolors='b')\nplt.show()\n\n\n\n\n\n\n\n\n\nMultivariate Analysis of Variance (MANOVA) is an extension of ANOVA that allows for the comparison of multivariate sample means. It is used when there are two or more continuous dependent variables.\n\nPurpose of MANOVA:\n\nTo test hypotheses about the differences in multiple dependent variables across groups.\nTo understand how independent variables affect combinations of dependent variables.\nExample: Examining the effect of different treatments (example: placebo/drug) on a set of health outcomes like blood pressure, cholesterol levels, and heart rate.\n\nProcess:\n\nCollect data on multiple dependent variables.\nDefine independent variables or groups.\nUse MANOVA to determine if there are significant differences in the multivariate means across groups.\nExample: Comparing the effectiveness of different medications on reducing multiple symptoms of a disease.\n\nApplications in Healthcare:\n\nUsed in clinical trials and healthcare studies to evaluate the effect of interventions on multiple outcomes.\nProvides a comprehensive analysis of treatment effects.\nExample: Assessing the overall impact of a lifestyle intervention on continuous physical and mental health outcomes.\n\n\n\n\nUse the Blood Pressure.csv dataset to complete the example MANOVA. This dataset contains fictional data. There are 20 subjects who received either Dose A, Dose B, Placebo, or Control group assignments for a blood pressure medication. There are also repeated BP measurements over time.\n\n\nJMP Instructions:\n\n\n\nGo to  File  &gt;  Open  and select the “Blood Pressure.csv file..\n\n\nGo to Analyze &gt; Fit Model.\n\n\nMove all the blood pressure measurements into the the “Y, Responses” box.\n\n\nMove Dose into the “Construct Model Effects” box.\n\n\nIn the “Personality” dropdown, select MANOVA\n\n\nClick Run and under “Response Specification”, click Compound from the red triangle menu.\n\n\n\n\n\nR Code Example:\n\n\n\n       # Load necessary libraries\nlibrary(tidyverse)\n\n# Load the dataset\ndata_1 &lt;-read_csv(\"Blood Pressure.csv\")\n\n#Perform the MANOVA\nmanova_model &lt;- manova(cbind(`BP 8M`, `BP 12M`, `BP 6M`, `BP 8W`) ~ `Dose`, data = data_1)\n\n#Print and summarize the results\nprint(manova_model)\nsummary(manova_model)\n\n      \n\n\n\n\n\nPython Code Example:\n\n\n\nSee other examples at https://www.geeksforgeeks.org/manova-multivariate-analysis-of-variance/\n# Importing necessary libraries\n\nimport pandas as pd\nfrom statsmodels.multivariate.manova import MANOVA\nfrom sklearn.datasets import load_iris\n\n# Use iris as sample data\ndata = load_iris()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf['species'] = data.target\nprint(df.head())\n\n\n# Rename columns to remove spaces in col names\ndf.columns = ['sepal_length', 'sepal_width',\n              'petal_length', 'petal_width', 'species']\n\n# Replace target numbers with their respective names for clarity in results\ndf['species'] = df['species'].map(\n    {0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n# Apply MANOVA with the renamed cols\nmanova = MANOVA.from_formula('sepal_length + sepal_width + petal_length + petal_width ~ species', data=df)\nresult = manova.mv_test()\nprint(result)\n\n\n\n\n\n\n\n\n\n\n\nPCA is a technique used to emphasize variation and bring out strong patterns in a dataset. It is particularly useful in reducing the dimensionality (number of variables) of large datasets, simplifying the complexity without losing significant information.\n\nPurpose of PCA:\n\nTo reduce the number of variables in a dataset while retaining most of the variation.\nTo identify the principal components that explain the most variance in the data.\nExample: Simplifying a dataset with dozens of health indicators to a few new variables (principal components) that capture the most significant patterns.\n\nProcess:\n\nStandardize the data if the variables are measured on different scales.\nCompute the covariance matrix or correlation matrix.\nExtract the eigenvalues and eigenvectors to identify the principal components.\nMore on PCA in our next lecture.\nExample: Using PCA to reduce a large number of survey questions into a few key factors that summarize patient satisfaction.\n\nApplications in Nursing Research:\n\nIdentifying key factors that influence patient outcomes.\nSimplifying complex datasets to facilitate analysis and interpretation.\nExample: Determining the main factors contributing to patient recovery in a rehabilitation study.\n\n\nUse the Body Measurements.csv dataset to complete the example PCA. This dataset contains the weight and physical measurements of 22 male subjects between 16-30 years old.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Multivariate Methods &gt; Principal Components.\n\n\nSelect the variables you want to include in the analysis and click OK.\n\n\nClick the red triangle and select Scree Plot and Eigenvalues.\n\n\nReview the output, including the principal components and their loadings.\n\n\n\n\n\nR Code Example:\n\n\n\n        #Load Data\nyour_data&lt;-read.csv(\"Body Measurements.csv\")\n#Run PCA\npca_model &lt;- prcomp(your_data, scale = TRUE)\n#Print and view results\nprint(pca_model)\nsummary(pca_model)\nplot(pca_model)\n#Scree Plot\nplot(pca_model, type = \"l\")\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n        # Import necessary libraries\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndata = pd.read_csv(\"/path/to/Body Measurements.csv\")\n\n# Drop any non-numeric columns if present, assuming all numeric columns are for PCA\nnumeric_data = data.select_dtypes(include=[float, int])\n\n# Standardize the data\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(numeric_data)\n\n# Perform PCA\npca = PCA(n_components=2)  # Adjust n_components as needed\nprincipal_components = pca.fit_transform(scaled_data)\n\n# Create a DataFrame with the principal components\npc_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n\n# Output the explained variance ratio\nprint(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n\n# Plot the principal components\nplt.figure(figsize=(8, 6))\nplt.scatter(pc_df['PC1'], pc_df['PC2'], alpha=0.5)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA of Body Measurements')\nplt.show()\n\n      \n\n\n\n\n\n\n\n\nEFA is used to identify the underlying relationships between measured variables. It helps in understanding the structure of a set of variables and in identifying the underlying unobserved (latent) variables. In nursing research, EFA can uncover hidden patterns in survey data or clinical assessments.\n\nPurpose of EFA:\n\nTo explore the underlying structure of a set of variables.\nTo identify unobserved (latent) factors that explain the correlations among observed variables.\nExample: Exploring the underlying dimensions of patient-reported outcomes in a quality of life survey.\n\nProcess:\n\nCollect data on multiple observed variables.\nCompute the correlation matrix.\nExtract factors using methods such as principal axis factoring or maximum likelihood.\nRotate the factors to achieve a simpler and more interpretable structure.\nExample: Using EFA to identify latent factors like physical health, mental health, and social functioning from a set of health survey items.\n\nApplications in Nursing Research:\n\nIdentifying latent constructs that are not directly observable.\nUnderstanding the dimensions of complex constructs such as patient satisfaction or quality of care.\nExample: Uncovering the underlying factors that contribute to nurse burnout.\n\n\nUse the Williams.sav dataset to complete the example PCA. This dataset contains an 28-item questionnaire (measured on a 7-point Likert scale) on organizational ability. It contains 5 theoretical dimensions: (1) preference for organization; (2) goal achievement; (3) planning approach; (4) acceptance of delays; and (5) preference for routine.\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Multivariate Methods &gt; Factor Analysis.\n\n\nSelect the variables you want to include and place them into the  Y, columns.\n\nSelect the variance estimation (e.g., REML) you want to use from the  Variance Estimationdropdown. Click OK.\n\n\nReview the output, including the factor loadings and the scree plot.\n\n\n\n\n\nR Code Example:\n\n\n\n        #Install and load the libraries\ninstall.packages(haven)\ninstall.packages(psych)\nlibrary(haven)\nlibrary(psych)\n#Read the file\nefa_data&lt;-read.csv(\"Williams.sav\")\n#Run the EFA\nefa_result &lt;- fa(efa_data, nfactors = 3, rotate = \"varimax\")\nprint(efa_result)\nfa.diagram(efa_result)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n        import pandas as pd\nimport pyreadstat\nfrom factor_analyzer import FactorAnalyzer\n\n# Read SPSS file\nmy_data, metadata = pyreadstat.read_sav(\"Williams.sav\")\n\n# Convert to pandas DataFrame\nmy_data = pd.DataFrame(my_data)\n\n# Display the first few rows of the DataFrame\nprint(my_data.head())\n\n# Perform EFA\nefa = FactorAnalyzer(n_factors=2, rotation=\"varimax\")\nefa.fit(my_data)\n\n# View results\nprint(efa.get_factor_variance())\nprint(efa.loadings_)\n\n\n\n\n\n\n\n\nWhile both EFA and PCA can be used for data reduction, and latent variable or composite variable creation, they have different purposes. PCA is a technique for reducing the dimensionality of data while preserving as much variance as possible. EFA, on the other hand, is used to uncover the underlying unobserved (latent) structure of a set of variables.\n\nPCA:\n\nFocuses on reducing dimensionality by transforming variables into principal components.\nEach principal component is a linear combination of the original variables.\nExample: Reducing the number of health indicators to a few principal components that capture the most variance.\n\nEFA:\n\nAims to identify the latent structure among variables.\nFactors are assumed to cause the observed variables and are estimated based on the shared variance.\nExample: Identifying underlying factors such as stress, fatigue, and job satisfaction from a set of survey questions on nurse well-being.\n\n\n\n\n\n\nUnderstanding multivariate analysis and its applications is crucial for advanced nursing research. Techniques like Discriminant Analysis, MANOVA, PCA, and EFA allow researchers to analyze complex datasets and uncover valuable insights that can improve patient outcomes and healthcare practices.\n\nImportance of Multivariate Analysis:\n\nProvides a comprehensive understanding of complex healthcare phenomena.\nEnables the simultaneous examination of multiple variables and their interactions.\nExample: Improving patient care by understanding the combined effects of various treatment factors.\n\nTechniques and Applications:\n\nDiscriminant Analysis: Classifying patients into risk categories.\nMANOVA: Comparing treatment effects on multiple health outcomes.\nPCA: Simplifying large datasets to identify key factors.\nEFA: Exploring underlying dimensions in survey data.\nExample: Using these techniques to inform clinical decision-making and healthcare policies."
  },
  {
    "objectID": "Lectures/introduction-to-multivariate.html#goal-understand-the-basics-of-multivariate-analysis-including-its-purpose-and-applications",
    "href": "Lectures/introduction-to-multivariate.html#goal-understand-the-basics-of-multivariate-analysis-including-its-purpose-and-applications",
    "title": "Introduction to Multivariate Analysis",
    "section": "",
    "text": "Multivariate analysis involves the observation and analysis of more than one statistical outcome variable at a time. This type of analysis is particularly powerful in healthcare research, where it allows for the examination of multiple outcome variables simultaneously, providing a more comprehensive understanding of complex phenomena.\n\nObservation and Analysis of Multiple Outcome Variables:\n\nMultivariate analysis considers multiple outcome variables simultaneously rather than analyzing them separately.\nThis approach helps to capture the interrelationships and interactions between different variables.\nExample: In a study examining the effectiveness of a new drug, researchers might look at how multiple outcomes such as blood pressure, cholesterol levels, and heart rate simultaneously differ collectively between a new drug group and a placebo group.\n\nApplication in Healthcare Research:\n\nIn healthcare, patient and clinical outcomes are often happening in a collective rather than independently.\nMultivariate analysis enables researchers to study these outcomes together, offering insights into how they collectively change based on independent variables.\nExample: Analyze how patient recovery (yes/no) impact lifestyle (diet, exercise) provides a more accurate picture than studying each factor in isolation.\n\nComprehensive Understanding of Complex Phenomena:\n\nBy examining multiple outcome variables at once, researchers can uncover patterns and relationships that may not be evident in uni/bivariate analysis.\n\n\n\n\n\nDiscriminant analysis is used to classify a set of observations into predefined classes. It works by finding a combination of features that best separates two or more classes of objects or events. This method is often used in healthcare for diagnostic purposes.\n\nPurpose of Discriminant Analysis:\n\nTo classify observations into predefined groups.\nTo identify the variables that differentiate between groups.\nExample: Classifying patients into different risk categories based on clinical measurements.\n\nProcess:\n\nCollect data on multiple predictor variables.\nDefine the group membership for each observation.\nApply discriminant function to identify the combination of variables that best separate the groups.\nExample: Using blood pressure, cholesterol, and BMI to predict whether a patient is at low, medium, or high risk for heart disease.\n\nApplications in Healthcare:\n\nUsed for diagnostic purposes, predicting outcomes, and identifying risk factors.\nHelps in making informed clinical decisions.\nExample: Determining whether a patient should undergo a specific treatment based on their risk classification.\n\n\nUse the Owl Diet.sav dataset to complete the example Discriminant Analysis. The Owl Diet dataset contains fictional information about types of owl diets.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Multivariate Methods  Discriminant.\n\n\nSelect your continuous outcome measurements as the Y, response, and the categorical predictor as X.\n\n\nClick OK.\n\n\n\n\n\nR Code Example:\n\n\n\n        #Install Packages\ninstall.packages(\"MASS\")\ninstall.packages(\"readr\")\nlibrary(readr)\nlibrary(MASS)\n\n# Read the data file\ndata &lt;-read_csv(\"Owl Diet.csv\")\nstr(data)\n\n#Perform the Discriminant Analysis\nlda_model &lt;- lda(`species` ~ `skull length` + `teeth row` + `palatine foramen` + `jaw length`, data = data)\nprint(lda_model)\nsummary(lda_model)\n\n      \n\n\n\n\n\nPython Code Example:\n\n\n\nSee details at https://medium.com/@glennlenormand/complete-guide-to-linear-discriminant-analysis-lda-in-python-d1255a5983b0\n# Demonstraton of LDA with IRIS data in python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_lda = lda.fit_transform(X, y)\n\nplt.xlabel('LD1')\nplt.ylabel('LD2')\nplt.scatter(X_lda[:,0], X_lda[:,1], c=y, cmap='rainbow', alpha=0.7, edgecolors='b')\nplt.show()\n\n\n\n\n\n\n\n\n\nMultivariate Analysis of Variance (MANOVA) is an extension of ANOVA that allows for the comparison of multivariate sample means. It is used when there are two or more continuous dependent variables.\n\nPurpose of MANOVA:\n\nTo test hypotheses about the differences in multiple dependent variables across groups.\nTo understand how independent variables affect combinations of dependent variables.\nExample: Examining the effect of different treatments (example: placebo/drug) on a set of health outcomes like blood pressure, cholesterol levels, and heart rate.\n\nProcess:\n\nCollect data on multiple dependent variables.\nDefine independent variables or groups.\nUse MANOVA to determine if there are significant differences in the multivariate means across groups.\nExample: Comparing the effectiveness of different medications on reducing multiple symptoms of a disease.\n\nApplications in Healthcare:\n\nUsed in clinical trials and healthcare studies to evaluate the effect of interventions on multiple outcomes.\nProvides a comprehensive analysis of treatment effects.\nExample: Assessing the overall impact of a lifestyle intervention on continuous physical and mental health outcomes.\n\n\n\n\nUse the Blood Pressure.csv dataset to complete the example MANOVA. This dataset contains fictional data. There are 20 subjects who received either Dose A, Dose B, Placebo, or Control group assignments for a blood pressure medication. There are also repeated BP measurements over time.\n\n\nJMP Instructions:\n\n\n\nGo to  File  &gt;  Open  and select the “Blood Pressure.csv file..\n\n\nGo to Analyze &gt; Fit Model.\n\n\nMove all the blood pressure measurements into the the “Y, Responses” box.\n\n\nMove Dose into the “Construct Model Effects” box.\n\n\nIn the “Personality” dropdown, select MANOVA\n\n\nClick Run and under “Response Specification”, click Compound from the red triangle menu.\n\n\n\n\n\nR Code Example:\n\n\n\n       # Load necessary libraries\nlibrary(tidyverse)\n\n# Load the dataset\ndata_1 &lt;-read_csv(\"Blood Pressure.csv\")\n\n#Perform the MANOVA\nmanova_model &lt;- manova(cbind(`BP 8M`, `BP 12M`, `BP 6M`, `BP 8W`) ~ `Dose`, data = data_1)\n\n#Print and summarize the results\nprint(manova_model)\nsummary(manova_model)\n\n      \n\n\n\n\n\nPython Code Example:\n\n\n\nSee other examples at https://www.geeksforgeeks.org/manova-multivariate-analysis-of-variance/\n# Importing necessary libraries\n\nimport pandas as pd\nfrom statsmodels.multivariate.manova import MANOVA\nfrom sklearn.datasets import load_iris\n\n# Use iris as sample data\ndata = load_iris()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf['species'] = data.target\nprint(df.head())\n\n\n# Rename columns to remove spaces in col names\ndf.columns = ['sepal_length', 'sepal_width',\n              'petal_length', 'petal_width', 'species']\n\n# Replace target numbers with their respective names for clarity in results\ndf['species'] = df['species'].map(\n    {0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n# Apply MANOVA with the renamed cols\nmanova = MANOVA.from_formula('sepal_length + sepal_width + petal_length + petal_width ~ species', data=df)\nresult = manova.mv_test()\nprint(result)\n\n\n\n\n\n\n\n\n\n\n\nPCA is a technique used to emphasize variation and bring out strong patterns in a dataset. It is particularly useful in reducing the dimensionality (number of variables) of large datasets, simplifying the complexity without losing significant information.\n\nPurpose of PCA:\n\nTo reduce the number of variables in a dataset while retaining most of the variation.\nTo identify the principal components that explain the most variance in the data.\nExample: Simplifying a dataset with dozens of health indicators to a few new variables (principal components) that capture the most significant patterns.\n\nProcess:\n\nStandardize the data if the variables are measured on different scales.\nCompute the covariance matrix or correlation matrix.\nExtract the eigenvalues and eigenvectors to identify the principal components.\nMore on PCA in our next lecture.\nExample: Using PCA to reduce a large number of survey questions into a few key factors that summarize patient satisfaction.\n\nApplications in Nursing Research:\n\nIdentifying key factors that influence patient outcomes.\nSimplifying complex datasets to facilitate analysis and interpretation.\nExample: Determining the main factors contributing to patient recovery in a rehabilitation study.\n\n\nUse the Body Measurements.csv dataset to complete the example PCA. This dataset contains the weight and physical measurements of 22 male subjects between 16-30 years old.\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Multivariate Methods &gt; Principal Components.\n\n\nSelect the variables you want to include in the analysis and click OK.\n\n\nClick the red triangle and select Scree Plot and Eigenvalues.\n\n\nReview the output, including the principal components and their loadings.\n\n\n\n\n\nR Code Example:\n\n\n\n        #Load Data\nyour_data&lt;-read.csv(\"Body Measurements.csv\")\n#Run PCA\npca_model &lt;- prcomp(your_data, scale = TRUE)\n#Print and view results\nprint(pca_model)\nsummary(pca_model)\nplot(pca_model)\n#Scree Plot\nplot(pca_model, type = \"l\")\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n        # Import necessary libraries\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndata = pd.read_csv(\"/path/to/Body Measurements.csv\")\n\n# Drop any non-numeric columns if present, assuming all numeric columns are for PCA\nnumeric_data = data.select_dtypes(include=[float, int])\n\n# Standardize the data\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(numeric_data)\n\n# Perform PCA\npca = PCA(n_components=2)  # Adjust n_components as needed\nprincipal_components = pca.fit_transform(scaled_data)\n\n# Create a DataFrame with the principal components\npc_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n\n# Output the explained variance ratio\nprint(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n\n# Plot the principal components\nplt.figure(figsize=(8, 6))\nplt.scatter(pc_df['PC1'], pc_df['PC2'], alpha=0.5)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA of Body Measurements')\nplt.show()\n\n      \n\n\n\n\n\n\n\n\nEFA is used to identify the underlying relationships between measured variables. It helps in understanding the structure of a set of variables and in identifying the underlying unobserved (latent) variables. In nursing research, EFA can uncover hidden patterns in survey data or clinical assessments.\n\nPurpose of EFA:\n\nTo explore the underlying structure of a set of variables.\nTo identify unobserved (latent) factors that explain the correlations among observed variables.\nExample: Exploring the underlying dimensions of patient-reported outcomes in a quality of life survey.\n\nProcess:\n\nCollect data on multiple observed variables.\nCompute the correlation matrix.\nExtract factors using methods such as principal axis factoring or maximum likelihood.\nRotate the factors to achieve a simpler and more interpretable structure.\nExample: Using EFA to identify latent factors like physical health, mental health, and social functioning from a set of health survey items.\n\nApplications in Nursing Research:\n\nIdentifying latent constructs that are not directly observable.\nUnderstanding the dimensions of complex constructs such as patient satisfaction or quality of care.\nExample: Uncovering the underlying factors that contribute to nurse burnout.\n\n\nUse the Williams.sav dataset to complete the example PCA. This dataset contains an 28-item questionnaire (measured on a 7-point Likert scale) on organizational ability. It contains 5 theoretical dimensions: (1) preference for organization; (2) goal achievement; (3) planning approach; (4) acceptance of delays; and (5) preference for routine.\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Multivariate Methods &gt; Factor Analysis.\n\n\nSelect the variables you want to include and place them into the  Y, columns.\n\nSelect the variance estimation (e.g., REML) you want to use from the  Variance Estimationdropdown. Click OK.\n\n\nReview the output, including the factor loadings and the scree plot.\n\n\n\n\n\nR Code Example:\n\n\n\n        #Install and load the libraries\ninstall.packages(haven)\ninstall.packages(psych)\nlibrary(haven)\nlibrary(psych)\n#Read the file\nefa_data&lt;-read.csv(\"Williams.sav\")\n#Run the EFA\nefa_result &lt;- fa(efa_data, nfactors = 3, rotate = \"varimax\")\nprint(efa_result)\nfa.diagram(efa_result)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n        import pandas as pd\nimport pyreadstat\nfrom factor_analyzer import FactorAnalyzer\n\n# Read SPSS file\nmy_data, metadata = pyreadstat.read_sav(\"Williams.sav\")\n\n# Convert to pandas DataFrame\nmy_data = pd.DataFrame(my_data)\n\n# Display the first few rows of the DataFrame\nprint(my_data.head())\n\n# Perform EFA\nefa = FactorAnalyzer(n_factors=2, rotation=\"varimax\")\nefa.fit(my_data)\n\n# View results\nprint(efa.get_factor_variance())\nprint(efa.loadings_)\n\n\n\n\n\n\n\n\nWhile both EFA and PCA can be used for data reduction, and latent variable or composite variable creation, they have different purposes. PCA is a technique for reducing the dimensionality of data while preserving as much variance as possible. EFA, on the other hand, is used to uncover the underlying unobserved (latent) structure of a set of variables.\n\nPCA:\n\nFocuses on reducing dimensionality by transforming variables into principal components.\nEach principal component is a linear combination of the original variables.\nExample: Reducing the number of health indicators to a few principal components that capture the most variance.\n\nEFA:\n\nAims to identify the latent structure among variables.\nFactors are assumed to cause the observed variables and are estimated based on the shared variance.\nExample: Identifying underlying factors such as stress, fatigue, and job satisfaction from a set of survey questions on nurse well-being."
  },
  {
    "objectID": "Lectures/introduction-to-multivariate.html#summary",
    "href": "Lectures/introduction-to-multivariate.html#summary",
    "title": "Introduction to Multivariate Analysis",
    "section": "",
    "text": "Understanding multivariate analysis and its applications is crucial for advanced nursing research. Techniques like Discriminant Analysis, MANOVA, PCA, and EFA allow researchers to analyze complex datasets and uncover valuable insights that can improve patient outcomes and healthcare practices.\n\nImportance of Multivariate Analysis:\n\nProvides a comprehensive understanding of complex healthcare phenomena.\nEnables the simultaneous examination of multiple variables and their interactions.\nExample: Improving patient care by understanding the combined effects of various treatment factors.\n\nTechniques and Applications:\n\nDiscriminant Analysis: Classifying patients into risk categories.\nMANOVA: Comparing treatment effects on multiple health outcomes.\nPCA: Simplifying large datasets to identify key factors.\nEFA: Exploring underlying dimensions in survey data.\nExample: Using these techniques to inform clinical decision-making and healthcare policies."
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#goal",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#goal",
    "title": "Introduction to Multivariate Analysis",
    "section": "Goal",
    "text": "Goal\n\nUnderstand the basics of multivariate analysis\nLearn its purpose and applications"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#definition-of-multivariate-analysis",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#definition-of-multivariate-analysis",
    "title": "Introduction to Multivariate Analysis",
    "section": "Definition of Multivariate Analysis",
    "text": "Definition of Multivariate Analysis\n\nObservation and analysis of more than one statistical outcome variable at a time\nParticularly powerful in healthcare research"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#purpose",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#purpose",
    "title": "Introduction to Multivariate Analysis",
    "section": "Purpose",
    "text": "Purpose\n\nClassify observations into predefined groups\nIdentify variables that differentiate between groups"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#process",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#process",
    "title": "Introduction to Multivariate Analysis",
    "section": "Process",
    "text": "Process\n\nCollect data on multiple predictor variables\nDefine group membership for each observation"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#applications-in-healthcare",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#applications-in-healthcare",
    "title": "Introduction to Multivariate Analysis",
    "section": "Applications in Healthcare",
    "text": "Applications in Healthcare\n\nDiagnostic purposes\nPredicting outcomes and identifying risk factors"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#example-discriminant-analysis",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#example-discriminant-analysis",
    "title": "Introduction to Multivariate Analysis",
    "section": "Example Discriminant Analysis",
    "text": "Example Discriminant Analysis\n\nUse the Owl Diet.sav dataset\nFictional information about types of owl diets"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#purpose-1",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#purpose-1",
    "title": "Introduction to Multivariate Analysis",
    "section": "Purpose",
    "text": "Purpose\n\nCompare multivariate sample means\nTest hypotheses about differences in multiple dependent variables across groups"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#process-1",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#process-1",
    "title": "Introduction to Multivariate Analysis",
    "section": "Process",
    "text": "Process\n\nCollect data on multiple dependent variables\nDefine independent variables or groups"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#applications-in-healthcare-1",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#applications-in-healthcare-1",
    "title": "Introduction to Multivariate Analysis",
    "section": "Applications in Healthcare",
    "text": "Applications in Healthcare\n\nClinical trials and healthcare studies\nEvaluate intervention effects on multiple outcomes"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#example-manova",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#example-manova",
    "title": "Introduction to Multivariate Analysis",
    "section": "Example MANOVA",
    "text": "Example MANOVA\n\nUse the Blood Pressure.csv dataset\nFictional data on blood pressure medication and repeated BP measurements"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#purpose-2",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#purpose-2",
    "title": "Introduction to Multivariate Analysis",
    "section": "Purpose",
    "text": "Purpose\n\nReduce the number of variables in a dataset\nRetain most of the variation"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#process-2",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#process-2",
    "title": "Introduction to Multivariate Analysis",
    "section": "Process",
    "text": "Process\n\nStandardize the data\nCompute covariance or correlation matrix\nExtract eigenvalues and eigenvectors"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#applications-in-nursing-research",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#applications-in-nursing-research",
    "title": "Introduction to Multivariate Analysis",
    "section": "Applications in Nursing Research",
    "text": "Applications in Nursing Research\n\nIdentify key factors influencing patient outcomes\nSimplify complex datasets for analysis"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#example-of-pca",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#example-of-pca",
    "title": "Introduction to Multivariate Analysis",
    "section": "Example of PCA",
    "text": "Example of PCA\n\nUse the Body Measurements.csv dataset\nWeight and physical measurements of 22 male subjects"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#purpose-3",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#purpose-3",
    "title": "Introduction to Multivariate Analysis",
    "section": "Purpose",
    "text": "Purpose\n\nIdentify underlying structure of a set of variables\nIdentify unobserved (latent) factors"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#process-3",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#process-3",
    "title": "Introduction to Multivariate Analysis",
    "section": "Process",
    "text": "Process\n\nCollect data on multiple observed variables\nCompute correlation matrix\nExtract factors using methods like principal axis factoring or maximum likelihood\nRotate factors for simpler interpretation"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#applications-in-nursing-research-1",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#applications-in-nursing-research-1",
    "title": "Introduction to Multivariate Analysis",
    "section": "Applications in Nursing Research",
    "text": "Applications in Nursing Research\n\nIdentify latent constructs\nUnderstand dimensions of complex constructs like patient satisfaction"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#example-of-efa",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#example-of-efa",
    "title": "Introduction to Multivariate Analysis",
    "section": "Example of EFA",
    "text": "Example of EFA\n\nUse the Williams.sav dataset\n28-item questionnaire on organizational ability"
  },
  {
    "objectID": "Lectures/Slides/Lecture 1/L1_V1.html#importance-of-multivariate-analysis",
    "href": "Lectures/Slides/Lecture 1/L1_V1.html#importance-of-multivariate-analysis",
    "title": "Introduction to Multivariate Analysis",
    "section": "Importance of Multivariate Analysis",
    "text": "Importance of Multivariate Analysis\n\nComprehensive understanding of complex phenomena\nExamine multiple outcome variables and interactions simultaneously"
  },
  {
    "objectID": "Lectures/advanced_regression.html",
    "href": "Lectures/advanced_regression.html",
    "title": "Model Building and Advanced Linear and Logistic Regression",
    "section": "",
    "text": "Understanding the fundamentals and advanced topics of multiple linear and logistic regression analysis is crucial for nursing research. This page will cover the definitions, key concepts, assumptions, and techniques required to build robust models in these contexts.\n\n\n\n\n\nDefinition: A statistical technique that models the relationship between a dependent variable and two or more independent variables by fitting a linear equation.\nPurpose: To predict the value of the dependent variable based on the values of the independent variables and to understand the relationship between them.\n\n\n\n\n\nDefinition: A regression model where the dependent variable is categorical, typically binary.\nPurpose: To model the probability of a binary outcome based on one or more predictor variables.\n\n\n\n\n\n\n\n\nLinearity: The relationship between the dependent and independent variables should be linear.\nIndependence: Observations should be independent of each other.\nHomoscedasticity: The variance of error terms should be constant across all levels of the independent variables.\nNormality: The residuals (errors) should be approximately normally distributed.\nAbsence of Multicollinearity: Predictors should not be too highly correlated with each other.\nIndependence: Observations should be independent. ### Logistic Regression Assumptions\nLinearity of Logit: The logit of the outcome should have a linear relationship with the predictor variables.\nIndependence: Observations should be independent.\nAbsence of Multicollinearity: Predictors should not be too highly correlated with each other.\n\n\n\n\n\n\nMultiple Linear Regression: A continuous variable (e.g., blood pressure, cholesterol level).\nLogistic Regression: A categorical variable (e.g., presence or absence of a disease).\n\n\n\n\n\nDefinition: Variables that predict or explain the dependent variable. Can be continuous or categorical.\nExamples in Nursing Research: Age, weight, treatment type, comorbidities.\n\n\n\n\n\n\n\n\nMethod: Starts with no predictors and adds them one by one based on a specified criterion (e.g., p-value).\nProcess:\n\nBegin with an empty model.\nAdd the predictor with the best criterion (e.g. lowest p-value).\nContinue adding predictors one at a time, based on criterion, until no additional predictors meet the criterion.\n\nAdvantages: Simple and easy to understand.\nDisadvantages: Can miss important variables that only show their effect in combination with others. Using p-values to add variables can lead to incorrect model specification. Other methods like AIC, or BIC can perform better.\n\n\n\n\n\n\nMethod: Starts with all candidate predictors and removes them one by one based on a specified criterion.\nProcess:\n\nBegin with the full model.\nRemove the predictor with the worst criterion.\nContinue removing predictors until all remaining predictors meet criterion.\n\nAdvantages: Considers the full model from the start.\nDisadvantages: Computationally intensive for large set of candidate predictors.\n\n\n\n\n\nMethod: A combination of forward and backward selection.\nProcess:\n\nBegin with an empty model or a model with a subset of predictors.\nAdd predictors based on criterion and remove predictors whose criterion no longer meets standard.\nContinue until no predictors can be added or removed.\n\nAdvantages: More flexible and can result in a better model.\nDisadvantages: Prone to overfitting and can be unstable.\n\n\n\n\n\n\nMethod: Performs both variable selection and regularization to enhance the prediction accuracy and interpretability.\nProcess:\n\nAdds a penalty to the size of the coefficients, shrinking some to zero and thus performing variable selection.\nThe amount of shrinkage is controlled by a tuning parameter (lambda).\n\nAdvantages: Can handle large sets of predictors and reduces overfitting.\nDisadvantages: Requires careful selection of the regularization parameter. Use cross validation to select.\n\n\n\n\n\n\nMethod: Involves entering predictors into the regression model in steps based on theoretical justification.\nProcess:\n\nEnter variables in a pre-specified order, typically based on theoretical importance.\nAssess the contribution of each variable or set of variables at each step.\n\nAdvantages: Allows testing the incremental value of adding new predictors.\nDisadvantages: Can be complex to implement and interpret.\nConfusion with Hierarchical Models:\n\nHierarchical regression involves adding variables in steps to assess their incremental value.\nHierarchical models (also known as multilevel models) account for data that is nested within higher-level units (e.g., patients within hospitals).\nDespite similar names, hierarchical regression and hierarchical models address different analytical needs.\n\n\n\n\n\n\n\nInterpretation: Coefficients represent the change in the dependent variable for a one-unit change in the predictor variable, holding other variables constant.\nLogistic Regression: Coefficients are in terms of the log odds of the outcome. Once exponentiated, the coefficientsare in terms of change in odds of outcome.\n\n\n\n\n\nP-Values: Indicate whether there is statistical evidence that the population parameter is significantly different from zero.\nConfidence Intervals: Provide a range of values within which the true population parameter is expected to fall. Based on a defined confidence level (e.g. 95%)\n\n\n\n\n\n\n\nR-Squared: Measures the proportion of variability in the dependent variable that is explained by the independent variables. Problem is that as the number of variables increase the R-Squared will always increase.\nAdjusted R-Squared: Adjusts R-squared for the number of predictors in the model. Addresses problem previously mentioned.\nAIC/BIC (Akaike Information Criterion/Bayesian Information Criterion): Used for model comparison, with lower values indicating a better fit and quality.\n\n\n\n\n\nPseudo R-Squared: Analogous to R-squared in linear regression, but measures the goodness of fit for logistic models. Problem is that as the number of variables increase the R-Squared will always increase. Adjusted Pseudo R-Squared do exist.\nAIC/BIC (Akaike Information Criterion/Bayesian Information Criterion): Used for model comparison, with lower values indicating a better fit and quality.\n\n\n\n\n\n\n\n\nImportance: Ensures the validity of hypothesis tests and confidence intervals.\nAssessment: Histogram, Q-Q plot.\n\n\n\n\n\nImportance: Ensures that the residuals have constant variance, which is an assumption of linear regression.\nAssessment: Residuals vs. fitted values plot.\n\n\n\n\n\n\nResidual Plots: Help assess the assumptions of linearity, homoscedasticity, and independence.\nInfluence Plots: Identify influential data points that can disproportionately affect the model.\n\n\n\n\n\n\n\nScenario: Predicting calories of common breakfast cereal based on nutritional information.\nUse the Cereal datatset to run the multiple linear regression.\n\n\n\n\n\nScenario: Predicting the likelihood of domestic vehicle manufacture based on vehicle characteristics.\nUse the  Cars 1993.csv dataset to run the logistic regression. This dataset contains information on 93 cars from the year 1993.\n\n\n\n\n\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your dependent variable and move it to the Y box.\n\n\nSelect your independent variables and move them to the Construct Model Effects box.\n\n\nClick on the Stepwise button.\n\n\nIn the Stepwise Options dialog, set the Direction to Forward.\n\n\nSpecify the stopping criterion (e.g., p-value threshold.\n\n\nClick Run Model to perform the forward selection.\n\n\n\n\n\nR Code Example:\n\n\n\n        #Forward selection for MLR\n# Load necessary libraries\nlibrary(MASS)  \n\n# Load dataset\ncars1993&lt;-read_csv(\"Cars 1993.csv\")\n\n#Build the full model\nfull_model &lt;- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`+ `Passenger Capacity`, data = cars1993)\n\n# Forward stepwise selection\nstep_model &lt;- stepAIC(lm(`Minimum Price ($1000)` ~ 1, data = cars1993), \n                      direction = \"forward\", \n                      scope = list(lower = ~1, upper = full_model))\n\n# Print summary of the final model selected by forward selection\nsummary(step_model)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Load iris data\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Feature selection using Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=0)\nsfm = SelectFromModel(rf, threshold='median')\nX_train_selected = sfm.fit_transform(X_train, y_train)\n\n# Get the selected feature indices\nselected_feature_indices = sfm.get_support(indices=True)\n\n# Get the selected feature names\nselected_features = X_train.columns[selected_feature_indices]\n\ndef forward_selection(X_train, y_train, selected_features):\n    remaining_features = set(selected_features)\n    current_score = float('inf')\n    best_new_score = float('inf')\n    selected_features_list = [] # Use a list to store selected feature names\n    \n    while remaining_features:\n        scores_with_candidates = []\n        for candidate in remaining_features:\n            # Add the candidate feature to the selected features\n            selected_features_list.append(candidate)\n            \n            # Fit a model with the selected features\n            # Use the list of selected feature names to index X_train\n            model = LinearRegression().fit(X_train[selected_features_list], y_train) \n            \n            # Calculate the AIC score\n            RSS = sum((model.predict(X_train[selected_features_list]) - y_train) ** 2)\n            n = len(X_train)\n            p = len(selected_features_list)\n            score = n * np.log(RSS / n) + 2 * p\n            \n            # Store the score and the candidate feature\n            scores_with_candidates.append((score, candidate))\n            \n            # Remove the candidate feature from the selected features\n            selected_features_list.pop()\n        \n        # Sort the scores and select the best candidate\n        scores_with_candidates.sort()\n        best_new_score, best_candidate = scores_with_candidates[0]\n        \n        # If the new score is better than the current score, update the current score and add the best candidate to the selected features\n        if current_score &gt; best_new_score:\n            current_score = best_new_score\n            selected_features_list.append(best_candidate) # Append to the list\n            remaining_features.remove(best_candidate)\n    \n    # Fit a model with the selected features\n    # Index X_train with the list of selected feature names\n    model = LinearRegression().fit(X_train[selected_features_list], y_train) \n    \n    return model, selected_features_list # Return both the model and the list of selected features\n\n# Run forward selection\n# Pass the list of selected feature names to the function\nmodel, selected_features_list = forward_selection(X_train, y_train, selected_features) \n\n# Print the summary of the model\nprint(\"Selected Features:\", model.coef_)\n\n# Print the selected feature column names\nprint(\"Selected Features:\")\nfor feature in selected_features_list: # Iterate over the list of selected features\n    print(feature)\n      \n\n\n\n\n\nSTATA Code Example:\n\n\n\n        // Forward Selection in STATA\n        stepwise, pr(0.05): regress recovery_time age severity treatment gender comorbidities\n      \n\n\n\n\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your dependent variable and move it to the Y box.\n\n\nSelect your independent variables and move them to the Construct Model Effects box.\n\n\nClick on the Stepwise button.\n\n\nIn the Stepwise Options dialog, set the Direction to Backward.\n\n\nSpecify the stopping criterion (e.g., p-value threshold) and click OK.\n\n\nClick Run to perform the backward selection.\n\n\n\n\n\nR Code Example:\n\n\n\n        #Backward selection with MLR\n# Load necessary libraries\nlibrary(MASS)  \n\n# Load dataset\ncars1993&lt;-read_csv(\"Cars 1993.csv\")\n\n#Build the full model\nfull_model &lt;- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`+ `Passenger Capacity`, data = cars1993)\n\n# Backward stepwise selection\nstep_model &lt;- stepAIC(full_model, direction = \"backward\")\n\n# Print summary of the final model selected by backward selection\nsummary(step_model)\n  \n      \n\n\n\n\n\nPython Code Example:\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nfrom sklearn.linear_model import Lasso\n\n# Load iris data\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n\ndef backward_selection(X_train, y_train):\n    remaining_features = set(X_train.columns)\n    current_score = float('inf')\n    selected_features_list = list(X_train.columns)\n    \n    # Change the model to Lasso for feature selection\n    model = Lasso(alpha=0.01)\n    \n    while len(remaining_features) &gt; 1:\n        scores_with_candidates = []\n        for candidate in remaining_features:\n            # Temporarily remove the candidate feature from the selected features\n            selected_features_list.remove(candidate)\n            \n            # Fit the Lasso model with the remaining features\n            model.fit(X_train[selected_features_list], y_train)\n            \n            # Calculate the AIC score\n            RSS = sum((model.predict(X_train[selected_features_list]) - y_train) ** 2)\n            n = len(X_train)\n            p = len(selected_features_list)\n            score = n * np.log(RSS / n) + 2 * p\n            \n            # Store the score and the candidate feature\n            scores_with_candidates.append((score, candidate))\n            \n            # Add the candidate feature back to the selected features\n            selected_features_list.append(candidate)\n        \n        # Sort the scores in ascending order\n        scores_with_candidates.sort()\n        \n        # Select the candidate feature with the highest score\n        _, worst_candidate = scores_with_candidates.pop(0)\n        \n        # If removing the worst candidate improves the score, remove it\n        if scores_with_candidates[-1][0] &lt; current_score: \n            selected_features_list.remove(worst_candidate)\n            remaining_features.remove(worst_candidate)\n            current_score = scores_with_candidates[-1][0]\n        else:\n            break  # Early stopping if improvement becomes negligible\n    \n    # Fit the final Lasso model with the remaining features\n    model.fit(X_train[selected_features_list], y_train)\n    \n    return model, selected_features_list\n\n# Run backward selection\nmodel, selected_features_list = backward_selection(X_train, y_train)\n\n# Get the names of the selected features\nselected_feature_names = selected_features_list\n\n# Get the coefficients of the selected features\nselected_feature_coefficients = model.coef_\n\n# Print the summary of the selected features and their coefficients\nfor feature_name, coefficient in zip(selected_feature_names, selected_feature_coefficients):\n    print(\"Feature:\", feature_name, \"| Coefficient:\", coefficient)\n      \n\n\n\n\n\nSTATA Code Example:\n\n\n\n        // Backward Selection in STATA\n        stepwise, pr(0.05) backward: regress recovery_time age severity treatment gender comorbidities\n      \n\n\n\n\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your dependent variable and move it to the Y box.\n\n\nSelect your independent variables and move them to the Construct Model Effects box.\n\n\nClick on the Stepwise button.\n\n\nIn the Stepwise Options dialog, set the Direction to Mixed (for both forward and backward steps).\n\n\nSpecify the stopping criterion (e.g., p-value threshold) and click OK.\n\n\nClick Run to perform the stepwise selection.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Stepwise Selection in R\n        library(MASS)\n        \n        # Full model\n        full_model &lt;- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`+ `Passenger Capacity`, data = cars1993)\n        \n        # Stepwise selection\n        step_model &lt;- stepAIC(full_model, direction = \"both\")\n        \n        summary(step_model)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n      \n # Stepwise Selection in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load Iris data\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Add the target variable to the training data\ntrain_data = X_train.copy()\ntrain_data['target'] = y_train\n\n# Stepwise selection\ndef stepwise_selected(data, response):\n    def calculate_aic(model):\n        return sm.OLS(data[response], model).fit().aic\n\n    predictors = list(data.columns)\n    predictors.remove(response)\n    selected = []\n    aic_current = float('inf')\n    while predictors:\n        aic_with_candidates = []\n        for candidate in predictors:\n            model = sm.add_constant(data[selected + [candidate]])\n            aic = calculate_aic(model)\n            aic_with_candidates.append((aic, candidate))\n        aic_with_candidates.sort()\n        best_aic, best_candidate = aic_with_candidates[0]\n        if aic_current &gt; best_aic:\n            predictors.remove(best_candidate)\n            selected.append(best_candidate)\n            aic_current = best_aic\n        else:\n            break\n\n        # Check if there are selected features before attempting to remove any\n        if selected:\n            remaining = list(set(data.columns) - set(selected) - {response})\n            for candidate in remaining:\n                # Check if the candidate is still in the selected list\n                if candidate in selected: \n                    model = sm.add_constant(data[selected].drop([candidate], axis=1))\n                    aic = calculate_aic(model)\n                    if aic &lt; aic_current:\n                        selected.remove(candidate)\n                        aic_current = aic\n\n    final_model = sm.OLS(data[response], sm.add_constant(data[selected])).fit()\n    return final_model\n\n# Apply stepwise selection to Iris dataset\nselected_model = stepwise_selected(train_data, 'target')\nprint(selected_model.summary())\n      \n\n\n\n\n\nSTATA Code Example:\n\n\n\n        // Stepwise Selection in STATA\n        stepwise, pr(0.05): regress recovery_time age severity treatment gender comorbidities\n      \n\n\n\n\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your dependent variable and move it to the Y box.\n\n\nSelect your independent variables and move them to the Construct Model Effects box.\n\n\nClick on the Personality dropdown and select Lasso.\n\n\nSpecify the tuning parameter (lambda) or use cross-validation to select it.\n\n\nClick Run to fit the lasso model.\n\n\n\n\n\nR Code Example:\n\n\n\n       # Install glmnet package if not already installed\ninstall.packages(\"glmnet\")\n\n# Lasso in R\nlibrary(glmnet)\n        \n# Prepare data\n  data_2&lt;-read.csv(\"Cars 1993.csv\")\nX &lt;- model.matrix(`Minimum.Price...1000.` ~ `Maximum.Horsepower` + `Fuel.Tank.Capacity` + `Passenger.Capacity`, data = data_2)[, -1]\ny &lt;- data_2$`Minimum.Price...1000.`\n        \n#Fit LASSO using cross-validation\nset.seed(123) #For reproducibility\ncv_fit &lt;- cv.glmnet(X, y, alpha = 1)\n\n#Plot the cross-validation\nplot(cv_fit)\n\n#Extract the coefficients of the best model\nbest_lambda &lt;- cv_fit$lambda.min\nlasso_coefficients &lt;- coef(cv_fit, s = \"lambda.min\")\nprint(lasso_coefficients)\n\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n        \n        \n        # Lasso in Python\n       import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import Lasso\n\n# Load Iris data\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Perform LASSO with cross-validation to find the best alpha\nlasso = Lasso(max_iter=10000)\nparam_grid = {'alpha': np.logspace(-4, 0, 100)}  # Test alpha values from 0.0001 to 1\nlasso_cv = GridSearchCV(lasso, param_grid, cv=5)\nlasso_cv.fit(X_train, y_train)\n\n# Get the best alpha value and the corresponding model\nbest_alpha = lasso_cv.best_params_['alpha']\nbest_model = lasso_cv.best_estimator_\n\n# Get the selected features based on the best model\nselected_features = X_train.columns[best_model.coef_ != 0]\n\n# Print the best alpha value and selected features\nprint(\"Best alpha:\", best_alpha)\nprint(\"Selected features:\", selected_features)\n      \n\n\n\n\n\nSTATA Code Example:\n\n\n\n        // Lasso in STATA\n        lassoreg recovery_time age severity treatment gender comorbidities\n      \n\n\n\n\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your dependent variable and move it to the Y box.\n\n\nSelect the first set of independent variables and move them to the Construct Model Effects box.\n\n\nClick Run to fit the first model.\n\n\nTo add more variables, go to Redo &gt; Modeling &gt; Add Predictors.\n\n\nAdd the next set of variables and click Run again.\n\n\nRepeat the process until all variables are included, fitting the model in steps.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Multi-Step (Hierarchical Regression) in R\n        # Step 1\n        model1&lt;- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower`, data = cars1993)\n        \n        # Step 2\n        model2 &lt;- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`, data = cars1993)\n        \n        # Step 3\n        model3 &lt;- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`+ `Passenger Capacity`, data = cars1993)\n        \n        # Compare models\n        summary(model1)\n        summary(model2)\n        summary(model3)\n        \n        #ANOVA of models\n        anova(model1, model2, model3)\n\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n    \n# Multi-Step (Hierarchical Regression) in Python\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Load Iris data\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Step 1: Regression on a subset of predictors\npredictors_step1 = ['sepal length (cm)', 'sepal width (cm)']\nX_train_step1 = sm.add_constant(X_train[predictors_step1])\nmodel_step1 = sm.OLS(y_train, X_train_step1)\nresults_step1 = model_step1.fit()\ny_pred_step1 = results_step1.predict(sm.add_constant(X_test[predictors_step1]))\nmse_step1 = mean_squared_error(y_test, y_pred_step1)\nprint(\"Model Summary - Step 1:\")\nprint(results_step1.summary())\nprint(\"Mean Squared Error - Step 1:\", mse_step1)\n\n# Step 2: Regression with additional predictors\npredictors_step2 = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)']\nX_train_step2 = sm.add_constant(X_train[predictors_step2])\nmodel_step2 = sm.OLS(y_train, X_train_step2)\nresults_step2 = model_step2.fit()\ny_pred_step2 = results_step2.predict(sm.add_constant(X_test[predictors_step2]))\nmse_step2 = mean_squared_error(y_test, y_pred_step2)\nprint(\"Model Summary - Step 2:\")\nprint(results_step2.summary())\nprint(\"Mean Squared Error - Step 2:\", mse_step2)\n\n# Step 3: Regression with all predictors\nX_train_step3 = sm.add_constant(X_train)\nmodel_step3 = sm.OLS(y_train, X_train_step3)\nresults_step3 = model_step3.fit()\ny_pred_step3 = results_step3.predict(sm.add_constant(X_test))\nmse_step3 = mean_squared_error(y_test, y_pred_step3)\nprint(\"Model Summary - Step 3:\")\nprint(results_step3.summary())\nprint(\"Mean Squared Error - Step 3:\", mse_step3)\n\n\n# COMPARING MSE\nprint(\"Mean Squared Error - Step 1:\", mse_step1)\nprint(\"Mean Squared Error - Step 2:\", mse_step2)\nprint(\"Mean Squared Error - Step 3:\", mse_step3)\n      \n\n\n\n\n\nSTATA Code Example:\n\n\n\n        // Multi-Step (Hierarchical Regression) in STATA\n        // Step 1\n        regress recovery_time age gender\n        \n        // Step 2\n        regress recovery_time age gender severity\n        \n        // Step 3\n        regress recovery_time age gender severity treatment comorbidities"
  },
  {
    "objectID": "Lectures/advanced_regression.html#definition-and-purpose-of-multiple-linear-and-logistic-regression-analysis",
    "href": "Lectures/advanced_regression.html#definition-and-purpose-of-multiple-linear-and-logistic-regression-analysis",
    "title": "Model Building and Advanced Linear and Logistic Regression",
    "section": "",
    "text": "Definition: A statistical technique that models the relationship between a dependent variable and two or more independent variables by fitting a linear equation.\nPurpose: To predict the value of the dependent variable based on the values of the independent variables and to understand the relationship between them.\n\n\n\n\n\nDefinition: A regression model where the dependent variable is categorical, typically binary.\nPurpose: To model the probability of a binary outcome based on one or more predictor variables."
  },
  {
    "objectID": "Lectures/advanced_regression.html#key-assumptions-and-limitations",
    "href": "Lectures/advanced_regression.html#key-assumptions-and-limitations",
    "title": "Model Building and Advanced Linear and Logistic Regression",
    "section": "",
    "text": "Linearity: The relationship between the dependent and independent variables should be linear.\nIndependence: Observations should be independent of each other.\nHomoscedasticity: The variance of error terms should be constant across all levels of the independent variables.\nNormality: The residuals (errors) should be approximately normally distributed.\nAbsence of Multicollinearity: Predictors should not be too highly correlated with each other.\nIndependence: Observations should be independent. ### Logistic Regression Assumptions\nLinearity of Logit: The logit of the outcome should have a linear relationship with the predictor variables.\nIndependence: Observations should be independent.\nAbsence of Multicollinearity: Predictors should not be too highly correlated with each other."
  },
  {
    "objectID": "Lectures/advanced_regression.html#dependent-variable",
    "href": "Lectures/advanced_regression.html#dependent-variable",
    "title": "Model Building and Advanced Linear and Logistic Regression",
    "section": "",
    "text": "Multiple Linear Regression: A continuous variable (e.g., blood pressure, cholesterol level).\nLogistic Regression: A categorical variable (e.g., presence or absence of a disease)."
  },
  {
    "objectID": "Lectures/advanced_regression.html#independent-variables",
    "href": "Lectures/advanced_regression.html#independent-variables",
    "title": "Model Building and Advanced Linear and Logistic Regression",
    "section": "",
    "text": "Definition: Variables that predict or explain the dependent variable. Can be continuous or categorical.\nExamples in Nursing Research: Age, weight, treatment type, comorbidities."
  },
  {
    "objectID": "Lectures/advanced_regression.html#variable-selection-techniques",
    "href": "Lectures/advanced_regression.html#variable-selection-techniques",
    "title": "Model Building and Advanced Linear and Logistic Regression",
    "section": "",
    "text": "Method: Starts with no predictors and adds them one by one based on a specified criterion (e.g., p-value).\nProcess:\n\nBegin with an empty model.\nAdd the predictor with the best criterion (e.g. lowest p-value).\nContinue adding predictors one at a time, based on criterion, until no additional predictors meet the criterion.\n\nAdvantages: Simple and easy to understand.\nDisadvantages: Can miss important variables that only show their effect in combination with others. Using p-values to add variables can lead to incorrect model specification. Other methods like AIC, or BIC can perform better.\n\n\n\n\n\n\nMethod: Starts with all candidate predictors and removes them one by one based on a specified criterion.\nProcess:\n\nBegin with the full model.\nRemove the predictor with the worst criterion.\nContinue removing predictors until all remaining predictors meet criterion.\n\nAdvantages: Considers the full model from the start.\nDisadvantages: Computationally intensive for large set of candidate predictors.\n\n\n\n\n\nMethod: A combination of forward and backward selection.\nProcess:\n\nBegin with an empty model or a model with a subset of predictors.\nAdd predictors based on criterion and remove predictors whose criterion no longer meets standard.\nContinue until no predictors can be added or removed.\n\nAdvantages: More flexible and can result in a better model.\nDisadvantages: Prone to overfitting and can be unstable.\n\n\n\n\n\n\nMethod: Performs both variable selection and regularization to enhance the prediction accuracy and interpretability.\nProcess:\n\nAdds a penalty to the size of the coefficients, shrinking some to zero and thus performing variable selection.\nThe amount of shrinkage is controlled by a tuning parameter (lambda).\n\nAdvantages: Can handle large sets of predictors and reduces overfitting.\nDisadvantages: Requires careful selection of the regularization parameter. Use cross validation to select.\n\n\n\n\n\n\nMethod: Involves entering predictors into the regression model in steps based on theoretical justification.\nProcess:\n\nEnter variables in a pre-specified order, typically based on theoretical importance.\nAssess the contribution of each variable or set of variables at each step.\n\nAdvantages: Allows testing the incremental value of adding new predictors.\nDisadvantages: Can be complex to implement and interpret.\nConfusion with Hierarchical Models:\n\nHierarchical regression involves adding variables in steps to assess their incremental value.\nHierarchical models (also known as multilevel models) account for data that is nested within higher-level units (e.g., patients within hospitals).\nDespite similar names, hierarchical regression and hierarchical models address different analytical needs."
  },
  {
    "objectID": "Lectures/advanced_regression.html#understanding-coefficients",
    "href": "Lectures/advanced_regression.html#understanding-coefficients",
    "title": "Model Building and Advanced Linear and Logistic Regression",
    "section": "",
    "text": "Interpretation: Coefficients represent the change in the dependent variable for a one-unit change in the predictor variable, holding other variables constant.\nLogistic Regression: Coefficients are in terms of the log odds of the outcome. Once exponentiated, the coefficientsare in terms of change in odds of outcome."
  },
  {
    "objectID": "Lectures/advanced_regression.html#significance-and-confidence-intervals",
    "href": "Lectures/advanced_regression.html#significance-and-confidence-intervals",
    "title": "Model Building and Advanced Linear and Logistic Regression",
    "section": "",
    "text": "P-Values: Indicate whether there is statistical evidence that the population parameter is significantly different from zero.\nConfidence Intervals: Provide a range of values within which the true population parameter is expected to fall. Based on a defined confidence level (e.g. 95%)"
  },
  {
    "objectID": "Lectures/advanced_regression.html#common-criterion-r-squared-and-adjusted-r-squared-and-other-model-fit-statistics",
    "href": "Lectures/advanced_regression.html#common-criterion-r-squared-and-adjusted-r-squared-and-other-model-fit-statistics",
    "title": "Model Building and Advanced Linear and Logistic Regression",
    "section": "",
    "text": "R-Squared: Measures the proportion of variability in the dependent variable that is explained by the independent variables. Problem is that as the number of variables increase the R-Squared will always increase.\nAdjusted R-Squared: Adjusts R-squared for the number of predictors in the model. Addresses problem previously mentioned.\nAIC/BIC (Akaike Information Criterion/Bayesian Information Criterion): Used for model comparison, with lower values indicating a better fit and quality.\n\n\n\n\n\nPseudo R-Squared: Analogous to R-squared in linear regression, but measures the goodness of fit for logistic models. Problem is that as the number of variables increase the R-Squared will always increase. Adjusted Pseudo R-Squared do exist.\nAIC/BIC (Akaike Information Criterion/Bayesian Information Criterion): Used for model comparison, with lower values indicating a better fit and quality."
  },
  {
    "objectID": "Lectures/advanced_regression.html#residual-analysis",
    "href": "Lectures/advanced_regression.html#residual-analysis",
    "title": "Model Building and Advanced Linear and Logistic Regression",
    "section": "",
    "text": "Importance: Ensures the validity of hypothesis tests and confidence intervals.\nAssessment: Histogram, Q-Q plot.\n\n\n\n\n\nImportance: Ensures that the residuals have constant variance, which is an assumption of linear regression.\nAssessment: Residuals vs. fitted values plot."
  },
  {
    "objectID": "Lectures/advanced_regression.html#diagnostic-plots",
    "href": "Lectures/advanced_regression.html#diagnostic-plots",
    "title": "Model Building and Advanced Linear and Logistic Regression",
    "section": "",
    "text": "Residual Plots: Help assess the assumptions of linearity, homoscedasticity, and independence.\nInfluence Plots: Identify influential data points that can disproportionately affect the model."
  },
  {
    "objectID": "Lectures/advanced_regression.html#examples-of-model-building-in-multiple-linear-and-logistic-regression",
    "href": "Lectures/advanced_regression.html#examples-of-model-building-in-multiple-linear-and-logistic-regression",
    "title": "Model Building and Advanced Linear and Logistic Regression",
    "section": "",
    "text": "Scenario: Predicting calories of common breakfast cereal based on nutritional information.\nUse the Cereal datatset to run the multiple linear regression.\n\n\n\n\n\nScenario: Predicting the likelihood of domestic vehicle manufacture based on vehicle characteristics.\nUse the  Cars 1993.csv dataset to run the logistic regression. This dataset contains information on 93 cars from the year 1993."
  },
  {
    "objectID": "Lectures/advanced_regression.html#software-examples",
    "href": "Lectures/advanced_regression.html#software-examples",
    "title": "Model Building and Advanced Linear and Logistic Regression",
    "section": "",
    "text": "JMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your dependent variable and move it to the Y box.\n\n\nSelect your independent variables and move them to the Construct Model Effects box.\n\n\nClick on the Stepwise button.\n\n\nIn the Stepwise Options dialog, set the Direction to Forward.\n\n\nSpecify the stopping criterion (e.g., p-value threshold.\n\n\nClick Run Model to perform the forward selection.\n\n\n\n\n\nR Code Example:\n\n\n\n        #Forward selection for MLR\n# Load necessary libraries\nlibrary(MASS)  \n\n# Load dataset\ncars1993&lt;-read_csv(\"Cars 1993.csv\")\n\n#Build the full model\nfull_model &lt;- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`+ `Passenger Capacity`, data = cars1993)\n\n# Forward stepwise selection\nstep_model &lt;- stepAIC(lm(`Minimum Price ($1000)` ~ 1, data = cars1993), \n                      direction = \"forward\", \n                      scope = list(lower = ~1, upper = full_model))\n\n# Print summary of the final model selected by forward selection\nsummary(step_model)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Load iris data\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Feature selection using Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=0)\nsfm = SelectFromModel(rf, threshold='median')\nX_train_selected = sfm.fit_transform(X_train, y_train)\n\n# Get the selected feature indices\nselected_feature_indices = sfm.get_support(indices=True)\n\n# Get the selected feature names\nselected_features = X_train.columns[selected_feature_indices]\n\ndef forward_selection(X_train, y_train, selected_features):\n    remaining_features = set(selected_features)\n    current_score = float('inf')\n    best_new_score = float('inf')\n    selected_features_list = [] # Use a list to store selected feature names\n    \n    while remaining_features:\n        scores_with_candidates = []\n        for candidate in remaining_features:\n            # Add the candidate feature to the selected features\n            selected_features_list.append(candidate)\n            \n            # Fit a model with the selected features\n            # Use the list of selected feature names to index X_train\n            model = LinearRegression().fit(X_train[selected_features_list], y_train) \n            \n            # Calculate the AIC score\n            RSS = sum((model.predict(X_train[selected_features_list]) - y_train) ** 2)\n            n = len(X_train)\n            p = len(selected_features_list)\n            score = n * np.log(RSS / n) + 2 * p\n            \n            # Store the score and the candidate feature\n            scores_with_candidates.append((score, candidate))\n            \n            # Remove the candidate feature from the selected features\n            selected_features_list.pop()\n        \n        # Sort the scores and select the best candidate\n        scores_with_candidates.sort()\n        best_new_score, best_candidate = scores_with_candidates[0]\n        \n        # If the new score is better than the current score, update the current score and add the best candidate to the selected features\n        if current_score &gt; best_new_score:\n            current_score = best_new_score\n            selected_features_list.append(best_candidate) # Append to the list\n            remaining_features.remove(best_candidate)\n    \n    # Fit a model with the selected features\n    # Index X_train with the list of selected feature names\n    model = LinearRegression().fit(X_train[selected_features_list], y_train) \n    \n    return model, selected_features_list # Return both the model and the list of selected features\n\n# Run forward selection\n# Pass the list of selected feature names to the function\nmodel, selected_features_list = forward_selection(X_train, y_train, selected_features) \n\n# Print the summary of the model\nprint(\"Selected Features:\", model.coef_)\n\n# Print the selected feature column names\nprint(\"Selected Features:\")\nfor feature in selected_features_list: # Iterate over the list of selected features\n    print(feature)\n      \n\n\n\n\n\nSTATA Code Example:\n\n\n\n        // Forward Selection in STATA\n        stepwise, pr(0.05): regress recovery_time age severity treatment gender comorbidities\n      \n\n\n\n\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your dependent variable and move it to the Y box.\n\n\nSelect your independent variables and move them to the Construct Model Effects box.\n\n\nClick on the Stepwise button.\n\n\nIn the Stepwise Options dialog, set the Direction to Backward.\n\n\nSpecify the stopping criterion (e.g., p-value threshold) and click OK.\n\n\nClick Run to perform the backward selection.\n\n\n\n\n\nR Code Example:\n\n\n\n        #Backward selection with MLR\n# Load necessary libraries\nlibrary(MASS)  \n\n# Load dataset\ncars1993&lt;-read_csv(\"Cars 1993.csv\")\n\n#Build the full model\nfull_model &lt;- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`+ `Passenger Capacity`, data = cars1993)\n\n# Backward stepwise selection\nstep_model &lt;- stepAIC(full_model, direction = \"backward\")\n\n# Print summary of the final model selected by backward selection\nsummary(step_model)\n  \n      \n\n\n\n\n\nPython Code Example:\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nfrom sklearn.linear_model import Lasso\n\n# Load iris data\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n\ndef backward_selection(X_train, y_train):\n    remaining_features = set(X_train.columns)\n    current_score = float('inf')\n    selected_features_list = list(X_train.columns)\n    \n    # Change the model to Lasso for feature selection\n    model = Lasso(alpha=0.01)\n    \n    while len(remaining_features) &gt; 1:\n        scores_with_candidates = []\n        for candidate in remaining_features:\n            # Temporarily remove the candidate feature from the selected features\n            selected_features_list.remove(candidate)\n            \n            # Fit the Lasso model with the remaining features\n            model.fit(X_train[selected_features_list], y_train)\n            \n            # Calculate the AIC score\n            RSS = sum((model.predict(X_train[selected_features_list]) - y_train) ** 2)\n            n = len(X_train)\n            p = len(selected_features_list)\n            score = n * np.log(RSS / n) + 2 * p\n            \n            # Store the score and the candidate feature\n            scores_with_candidates.append((score, candidate))\n            \n            # Add the candidate feature back to the selected features\n            selected_features_list.append(candidate)\n        \n        # Sort the scores in ascending order\n        scores_with_candidates.sort()\n        \n        # Select the candidate feature with the highest score\n        _, worst_candidate = scores_with_candidates.pop(0)\n        \n        # If removing the worst candidate improves the score, remove it\n        if scores_with_candidates[-1][0] &lt; current_score: \n            selected_features_list.remove(worst_candidate)\n            remaining_features.remove(worst_candidate)\n            current_score = scores_with_candidates[-1][0]\n        else:\n            break  # Early stopping if improvement becomes negligible\n    \n    # Fit the final Lasso model with the remaining features\n    model.fit(X_train[selected_features_list], y_train)\n    \n    return model, selected_features_list\n\n# Run backward selection\nmodel, selected_features_list = backward_selection(X_train, y_train)\n\n# Get the names of the selected features\nselected_feature_names = selected_features_list\n\n# Get the coefficients of the selected features\nselected_feature_coefficients = model.coef_\n\n# Print the summary of the selected features and their coefficients\nfor feature_name, coefficient in zip(selected_feature_names, selected_feature_coefficients):\n    print(\"Feature:\", feature_name, \"| Coefficient:\", coefficient)\n      \n\n\n\n\n\nSTATA Code Example:\n\n\n\n        // Backward Selection in STATA\n        stepwise, pr(0.05) backward: regress recovery_time age severity treatment gender comorbidities\n      \n\n\n\n\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your dependent variable and move it to the Y box.\n\n\nSelect your independent variables and move them to the Construct Model Effects box.\n\n\nClick on the Stepwise button.\n\n\nIn the Stepwise Options dialog, set the Direction to Mixed (for both forward and backward steps).\n\n\nSpecify the stopping criterion (e.g., p-value threshold) and click OK.\n\n\nClick Run to perform the stepwise selection.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Stepwise Selection in R\n        library(MASS)\n        \n        # Full model\n        full_model &lt;- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`+ `Passenger Capacity`, data = cars1993)\n        \n        # Stepwise selection\n        step_model &lt;- stepAIC(full_model, direction = \"both\")\n        \n        summary(step_model)\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n      \n # Stepwise Selection in Python\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load Iris data\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Add the target variable to the training data\ntrain_data = X_train.copy()\ntrain_data['target'] = y_train\n\n# Stepwise selection\ndef stepwise_selected(data, response):\n    def calculate_aic(model):\n        return sm.OLS(data[response], model).fit().aic\n\n    predictors = list(data.columns)\n    predictors.remove(response)\n    selected = []\n    aic_current = float('inf')\n    while predictors:\n        aic_with_candidates = []\n        for candidate in predictors:\n            model = sm.add_constant(data[selected + [candidate]])\n            aic = calculate_aic(model)\n            aic_with_candidates.append((aic, candidate))\n        aic_with_candidates.sort()\n        best_aic, best_candidate = aic_with_candidates[0]\n        if aic_current &gt; best_aic:\n            predictors.remove(best_candidate)\n            selected.append(best_candidate)\n            aic_current = best_aic\n        else:\n            break\n\n        # Check if there are selected features before attempting to remove any\n        if selected:\n            remaining = list(set(data.columns) - set(selected) - {response})\n            for candidate in remaining:\n                # Check if the candidate is still in the selected list\n                if candidate in selected: \n                    model = sm.add_constant(data[selected].drop([candidate], axis=1))\n                    aic = calculate_aic(model)\n                    if aic &lt; aic_current:\n                        selected.remove(candidate)\n                        aic_current = aic\n\n    final_model = sm.OLS(data[response], sm.add_constant(data[selected])).fit()\n    return final_model\n\n# Apply stepwise selection to Iris dataset\nselected_model = stepwise_selected(train_data, 'target')\nprint(selected_model.summary())\n      \n\n\n\n\n\nSTATA Code Example:\n\n\n\n        // Stepwise Selection in STATA\n        stepwise, pr(0.05): regress recovery_time age severity treatment gender comorbidities\n      \n\n\n\n\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your dependent variable and move it to the Y box.\n\n\nSelect your independent variables and move them to the Construct Model Effects box.\n\n\nClick on the Personality dropdown and select Lasso.\n\n\nSpecify the tuning parameter (lambda) or use cross-validation to select it.\n\n\nClick Run to fit the lasso model.\n\n\n\n\n\nR Code Example:\n\n\n\n       # Install glmnet package if not already installed\ninstall.packages(\"glmnet\")\n\n# Lasso in R\nlibrary(glmnet)\n        \n# Prepare data\n  data_2&lt;-read.csv(\"Cars 1993.csv\")\nX &lt;- model.matrix(`Minimum.Price...1000.` ~ `Maximum.Horsepower` + `Fuel.Tank.Capacity` + `Passenger.Capacity`, data = data_2)[, -1]\ny &lt;- data_2$`Minimum.Price...1000.`\n        \n#Fit LASSO using cross-validation\nset.seed(123) #For reproducibility\ncv_fit &lt;- cv.glmnet(X, y, alpha = 1)\n\n#Plot the cross-validation\nplot(cv_fit)\n\n#Extract the coefficients of the best model\nbest_lambda &lt;- cv_fit$lambda.min\nlasso_coefficients &lt;- coef(cv_fit, s = \"lambda.min\")\nprint(lasso_coefficients)\n\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n        \n        \n        # Lasso in Python\n       import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import Lasso\n\n# Load Iris data\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Perform LASSO with cross-validation to find the best alpha\nlasso = Lasso(max_iter=10000)\nparam_grid = {'alpha': np.logspace(-4, 0, 100)}  # Test alpha values from 0.0001 to 1\nlasso_cv = GridSearchCV(lasso, param_grid, cv=5)\nlasso_cv.fit(X_train, y_train)\n\n# Get the best alpha value and the corresponding model\nbest_alpha = lasso_cv.best_params_['alpha']\nbest_model = lasso_cv.best_estimator_\n\n# Get the selected features based on the best model\nselected_features = X_train.columns[best_model.coef_ != 0]\n\n# Print the best alpha value and selected features\nprint(\"Best alpha:\", best_alpha)\nprint(\"Selected features:\", selected_features)\n      \n\n\n\n\n\nSTATA Code Example:\n\n\n\n        // Lasso in STATA\n        lassoreg recovery_time age severity treatment gender comorbidities\n      \n\n\n\n\n\n\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Fit Model.\n\n\nSelect your dependent variable and move it to the Y box.\n\n\nSelect the first set of independent variables and move them to the Construct Model Effects box.\n\n\nClick Run to fit the first model.\n\n\nTo add more variables, go to Redo &gt; Modeling &gt; Add Predictors.\n\n\nAdd the next set of variables and click Run again.\n\n\nRepeat the process until all variables are included, fitting the model in steps.\n\n\n\n\n\nR Code Example:\n\n\n\n        # Multi-Step (Hierarchical Regression) in R\n        # Step 1\n        model1&lt;- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower`, data = cars1993)\n        \n        # Step 2\n        model2 &lt;- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`, data = cars1993)\n        \n        # Step 3\n        model3 &lt;- lm(`Minimum Price ($1000)` ~ `Maximum Horsepower` + `Fuel Tank Capacity`+ `Passenger Capacity`, data = cars1993)\n        \n        # Compare models\n        summary(model1)\n        summary(model2)\n        summary(model3)\n        \n        #ANOVA of models\n        anova(model1, model2, model3)\n\n      \n\n\n\n\n\nPython Code Example:\n\n\n\n    \n# Multi-Step (Hierarchical Regression) in Python\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Load Iris data\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Step 1: Regression on a subset of predictors\npredictors_step1 = ['sepal length (cm)', 'sepal width (cm)']\nX_train_step1 = sm.add_constant(X_train[predictors_step1])\nmodel_step1 = sm.OLS(y_train, X_train_step1)\nresults_step1 = model_step1.fit()\ny_pred_step1 = results_step1.predict(sm.add_constant(X_test[predictors_step1]))\nmse_step1 = mean_squared_error(y_test, y_pred_step1)\nprint(\"Model Summary - Step 1:\")\nprint(results_step1.summary())\nprint(\"Mean Squared Error - Step 1:\", mse_step1)\n\n# Step 2: Regression with additional predictors\npredictors_step2 = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)']\nX_train_step2 = sm.add_constant(X_train[predictors_step2])\nmodel_step2 = sm.OLS(y_train, X_train_step2)\nresults_step2 = model_step2.fit()\ny_pred_step2 = results_step2.predict(sm.add_constant(X_test[predictors_step2]))\nmse_step2 = mean_squared_error(y_test, y_pred_step2)\nprint(\"Model Summary - Step 2:\")\nprint(results_step2.summary())\nprint(\"Mean Squared Error - Step 2:\", mse_step2)\n\n# Step 3: Regression with all predictors\nX_train_step3 = sm.add_constant(X_train)\nmodel_step3 = sm.OLS(y_train, X_train_step3)\nresults_step3 = model_step3.fit()\ny_pred_step3 = results_step3.predict(sm.add_constant(X_test))\nmse_step3 = mean_squared_error(y_test, y_pred_step3)\nprint(\"Model Summary - Step 3:\")\nprint(results_step3.summary())\nprint(\"Mean Squared Error - Step 3:\", mse_step3)\n\n\n# COMPARING MSE\nprint(\"Mean Squared Error - Step 1:\", mse_step1)\nprint(\"Mean Squared Error - Step 2:\", mse_step2)\nprint(\"Mean Squared Error - Step 3:\", mse_step3)\n      \n\n\n\n\n\nSTATA Code Example:\n\n\n\n        // Multi-Step (Hierarchical Regression) in STATA\n        // Step 1\n        regress recovery_time age gender\n        \n        // Step 2\n        regress recovery_time age gender severity\n        \n        // Step 3\n        regress recovery_time age gender severity treatment comorbidities"
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html",
    "href": "Lectures/introduction-to-SEM.html",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "Structural Equation Modeling (SEM) is a comprehensive statistical approach used to model complex relationships among variables. This page will cover the fundamentals of SEM, including how to build, analyze, and interpret SEM models accurately.\n\n\n\nDefinition: SEM is a statistical technique that allows for the analysis of complex relationships between observed and latent variables.\nPurpose: To test theoretical models that specify relationships among multiple variables, including both direct and indirect effects.\n\n\n\n\n\nKey Concepts:\n\nObserved Variables: Variables that are measured directly.\nLatent Variables: Variables that are not directly observed but inferred from observed variables.\nPath Diagrams: Graphical representations of the hypothesized relationships among variables.\n\nTheoretical Foundations:\n\nSEM integrates factor analysis and multiple regression analysis.\nBased on the theory-driven approach to model specification and hypothesis testing.\n\n\n\n\n\n\nComponents:\n\nExogenous Variables: Independent variables that are not influenced by other variables in the model.\nEndogenous Variables: Dependent variables that are influenced by other variables in the model.\nPaths: Arrows in path diagrams representing hypothesized relationships.\n\nPath Models: Illustrate the relationships among variables using arrows and lines.\n\n\n\n\n\n\n\n\nObserved Variables (Manifest Variables):\n\nRepresented by rectangles or squares.\nThese are directly measured variables in the study (e.g., survey items, test scores).\n\nLatent Variables:\n\nRepresented by circles or ovals.\nThese are unobserved constructs inferred from observed variables (e.g., intelligence, satisfaction).\n\nError Terms:\n\nRepresented by small circles or ovals, often with an arrow pointing to the observed variable.\nThese represent measurement error or unexplained variance in the observed variables.\n\n\n\n\n\n\nDirect Paths:\n\nRepresented by single-headed arrows.\nIndicate a directional relationship between two variables (e.g., the effect of one variable on another).\n\nBidirectional Paths:\n\nRepresented by double-headed arrows.\nIndicate a correlational relationship between two variables, without specifying directionality.\n\nLatent Variable Indicators:\n\nObserved variables that load onto a latent variable.\nRepresented by single-headed arrows pointing from the latent variable (circle/ovals) to the observed variables (squares/rectangles).\n\n\n\n\n\n\n\n\n\nPath Analysis: Simpler, involving only observed variables and focusing on direct and indirect effects (mediation/moderation).\nSEM: More complex, incorporating both observed and latent variables and allowing for the modeling of measurement errors.\n\n\n\n\n\nPath Analysis: Typically does not include latent variables or measurement errors. Observed variables only.\nSEM: Includes latent variables, measurement models, and can model measurement errors. Both unobserved and observed variables can be used.\n\n\n\n\n\nPath Analysis: Limited to simpler models with observed variables.\nSEM: More flexible, capable of modeling complex relationships and structures among both observed and latent variables.\n\n\n\n\n\nPath Analysis: Assumes no measurement error in the observed variables.\nSEM: Can explicitly model measurement error, providing more accurate estimates of relationships.\n\n\n\n\n\n\nEstimation Methods:\n\nMaximum Likelihood (ML): The most common method for estimating parameters in SEM.\nGeneralized Least Squares (GLS): Another estimation method that is less sensitive to non-normality.\n\nModel Identification:\n\nA model must be identified (i.e., have a unique solution) to be estimated.\nDegrees of Freedom: The difference between the number of observed data points and the number of estimated parameters.\nIf a model has more parameters (p) than observed data points (N) then the model will not be identifiable. Also, it can lead to a problem known as overfitting, where the model becomes too complex and fits the noise in the data rather than the underlying trend. Colinearity may also be a problem.\n\n\n\n\n\n\nLatent Variables: Variables that represent underlying constructs not directly observed.\nIndicators: Observed variables that are used to measure latent variables.\n\n\n\n\n\nCFA: A technique used to test the measurement model, ensuring that the indicators accurately measure the latent constructs.\nSteps in CFA:\n\nSpecify the measurement model.\nEstimate the model parameters.\nAssess the model fit.\n\n\n\n\n\n\nFormulating Hypotheses: Develop hypotheses based on theory, specifying the expected relationships among variables.\nTesting Hypotheses: Use SEM to test the specified relationships and assess the overall model fit.\n\n\n\n\n\nPath Coefficients: Represent the strength and direction of relationships between variables.\nModel Fit: Indicates how well the model fits the observed data.\n\n\n\n\n\nCommon Fit Indices:\n\nChi-Square: Assesses the discrepancy between the observed and model-implied covariance matrices.\nRMSEA (Root Mean Square Error of Approximation): Measures the goodness of fit per degree of freedom.\nCFI (Comparative Fit Index): Compares the fit of the target model to an independent baseline model.\nTLI (Tucker-Lewis Index): Similar to CFI, but penalizes model complexity.\n\n\n\n\n\n\nInterpreting Fit Indices:\n\nChi-Square: A non-significant value indicates a good fit.\nRMSEA: Values less than 0.06 indicate a good fit.\nCFI and TLI: Values greater than 0.95 indicate a good fit.\n\nAssessing Model Adequacy: Evaluate multiple fit indices to determine the overall model adequacy.\n\n\n\n\n\n\nMediation: Explores whether the effect of an independent variable on a dependent variable is mediated by another variable. See Mediation Analysis or (Baron & Kenny 1986, Holmbeck 1997, Kraemer et al. 2001).\nModeration: Examines whether the relationship between two variables changes at different levels of a third variable. Moderation Analysis\n\n\n\n\n\n\n\n\nScenario: Using SEM to model the relationships between patient characteristics, treatment variables, and health outcomes.\n\n\n\n\n\nScenario: Exploring whether patient satisfaction mediates the relationship between nurse communication and patient recovery.\n\n\n\n\n\n\n\nYou can use the worland5.csv as an example. This hypothetical dataset examines the effects of student background on academic achievement. It contains 9 observed variables (Motivation, Harmony, Stability, Negative Parental Psychology, SES, Verbal IQ, Reading, Arithmetic and Spelling) and 3 hypothesized latent constructs (Adjustment, Risk, Achievement).\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Multivariate Methods &gt; Structural Equation Modeling.\n\n\nSelect your variables and specify the model structure in the diagram window.\n\n\nUse the Add Path tool to draw paths between variables as hypothesized in your model.\n\n\nSpecify latent variables by selecting observed variables and defining them as indicators.\n\n\nClick Run to estimate the model.\n\n\nReview the output for path coefficients, model fit indices, and other diagnostics.\n\n\n\n\n\n\n\n\nR Code Example:\n\n\n\n        # Load necessary libraries\n        library(lavaan)\n        \n        mydata2&lt;-read.csv(\"worland5.csv\") # adjust the file path accordingly\n        \n        # Specify the SEM model\n        \n        model &lt;- '\n          # Measurement model\n          adjustment  =~ motiv + harm + stabi\n          risk =~ verbal + ppsych + ses\n          achievement =~ read + arith + spell\n          \n          # Structural model\n          achievement ~ adjustment+ risk\n        '\n        \n        # Fit the model\n        MySEM_model &lt;- sem(model, data=mydata2)\n        \n        # Summarize the results\n        summary(MySEM_model, fit.measures = TRUE)\n        \n        # visualize the SEM model\n        library(semPlot)\n\n         # Visualize the path diagram\n        semPaths(MySEM_model, what = \"std\", edge.label.cex = 0.8, layout = \"tree\")"
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#definition-and-purpose-of-sem",
    "href": "Lectures/introduction-to-SEM.html#definition-and-purpose-of-sem",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "Definition: SEM is a statistical technique that allows for the analysis of complex relationships between observed and latent variables.\nPurpose: To test theoretical models that specify relationships among multiple variables, including both direct and indirect effects."
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#key-concepts-and-theoretical-foundations",
    "href": "Lectures/introduction-to-SEM.html#key-concepts-and-theoretical-foundations",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "Key Concepts:\n\nObserved Variables: Variables that are measured directly.\nLatent Variables: Variables that are not directly observed but inferred from observed variables.\nPath Diagrams: Graphical representations of the hypothesized relationships among variables.\n\nTheoretical Foundations:\n\nSEM integrates factor analysis and multiple regression analysis.\nBased on the theory-driven approach to model specification and hypothesis testing."
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#understanding-the-basic-components-of-path-models",
    "href": "Lectures/introduction-to-SEM.html#understanding-the-basic-components-of-path-models",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "Components:\n\nExogenous Variables: Independent variables that are not influenced by other variables in the model.\nEndogenous Variables: Dependent variables that are influenced by other variables in the model.\nPaths: Arrows in path diagrams representing hypothesized relationships.\n\nPath Models: Illustrate the relationships among variables using arrows and lines."
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#path-diagrams",
    "href": "Lectures/introduction-to-SEM.html#path-diagrams",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "Observed Variables (Manifest Variables):\n\nRepresented by rectangles or squares.\nThese are directly measured variables in the study (e.g., survey items, test scores).\n\nLatent Variables:\n\nRepresented by circles or ovals.\nThese are unobserved constructs inferred from observed variables (e.g., intelligence, satisfaction).\n\nError Terms:\n\nRepresented by small circles or ovals, often with an arrow pointing to the observed variable.\nThese represent measurement error or unexplained variance in the observed variables.\n\n\n\n\n\n\nDirect Paths:\n\nRepresented by single-headed arrows.\nIndicate a directional relationship between two variables (e.g., the effect of one variable on another).\n\nBidirectional Paths:\n\nRepresented by double-headed arrows.\nIndicate a correlational relationship between two variables, without specifying directionality.\n\nLatent Variable Indicators:\n\nObserved variables that load onto a latent variable.\nRepresented by single-headed arrows pointing from the latent variable (circle/ovals) to the observed variables (squares/rectangles)."
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#path-vs-sem-analysis-key-differences",
    "href": "Lectures/introduction-to-SEM.html#path-vs-sem-analysis-key-differences",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "Path Analysis: Simpler, involving only observed variables and focusing on direct and indirect effects (mediation/moderation).\nSEM: More complex, incorporating both observed and latent variables and allowing for the modeling of measurement errors.\n\n\n\n\n\nPath Analysis: Typically does not include latent variables or measurement errors. Observed variables only.\nSEM: Includes latent variables, measurement models, and can model measurement errors. Both unobserved and observed variables can be used.\n\n\n\n\n\nPath Analysis: Limited to simpler models with observed variables.\nSEM: More flexible, capable of modeling complex relationships and structures among both observed and latent variables.\n\n\n\n\n\nPath Analysis: Assumes no measurement error in the observed variables.\nSEM: Can explicitly model measurement error, providing more accurate estimates of relationships."
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#estimation-methods-and-model-identification",
    "href": "Lectures/introduction-to-SEM.html#estimation-methods-and-model-identification",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "Estimation Methods:\n\nMaximum Likelihood (ML): The most common method for estimating parameters in SEM.\nGeneralized Least Squares (GLS): Another estimation method that is less sensitive to non-normality.\n\nModel Identification:\n\nA model must be identified (i.e., have a unique solution) to be estimated.\nDegrees of Freedom: The difference between the number of observed data points and the number of estimated parameters.\nIf a model has more parameters (p) than observed data points (N) then the model will not be identifiable. Also, it can lead to a problem known as overfitting, where the model becomes too complex and fits the noise in the data rather than the underlying trend. Colinearity may also be a problem."
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#latent-variables-and-indicators",
    "href": "Lectures/introduction-to-SEM.html#latent-variables-and-indicators",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "Latent Variables: Variables that represent underlying constructs not directly observed.\nIndicators: Observed variables that are used to measure latent variables."
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#confirmatory-factor-analysis-cfa",
    "href": "Lectures/introduction-to-SEM.html#confirmatory-factor-analysis-cfa",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "CFA: A technique used to test the measurement model, ensuring that the indicators accurately measure the latent constructs.\nSteps in CFA:\n\nSpecify the measurement model.\nEstimate the model parameters.\nAssess the model fit."
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#formulating-and-testing-structural-hypotheses",
    "href": "Lectures/introduction-to-SEM.html#formulating-and-testing-structural-hypotheses",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "Formulating Hypotheses: Develop hypotheses based on theory, specifying the expected relationships among variables.\nTesting Hypotheses: Use SEM to test the specified relationships and assess the overall model fit."
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#path-coefficients-and-model-fit",
    "href": "Lectures/introduction-to-SEM.html#path-coefficients-and-model-fit",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "Path Coefficients: Represent the strength and direction of relationships between variables.\nModel Fit: Indicates how well the model fits the observed data."
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#goodness-of-fit-measures",
    "href": "Lectures/introduction-to-SEM.html#goodness-of-fit-measures",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "Common Fit Indices:\n\nChi-Square: Assesses the discrepancy between the observed and model-implied covariance matrices.\nRMSEA (Root Mean Square Error of Approximation): Measures the goodness of fit per degree of freedom.\nCFI (Comparative Fit Index): Compares the fit of the target model to an independent baseline model.\nTLI (Tucker-Lewis Index): Similar to CFI, but penalizes model complexity."
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#interpreting-fit-indices-and-assessing-model-adequacy",
    "href": "Lectures/introduction-to-SEM.html#interpreting-fit-indices-and-assessing-model-adequacy",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "Interpreting Fit Indices:\n\nChi-Square: A non-significant value indicates a good fit.\nRMSEA: Values less than 0.06 indicate a good fit.\nCFI and TLI: Values greater than 0.95 indicate a good fit.\n\nAssessing Model Adequacy: Evaluate multiple fit indices to determine the overall model adequacy."
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#mediation-and-moderation-analysis",
    "href": "Lectures/introduction-to-SEM.html#mediation-and-moderation-analysis",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "Mediation: Explores whether the effect of an independent variable on a dependent variable is mediated by another variable. See Mediation Analysis or (Baron & Kenny 1986, Holmbeck 1997, Kraemer et al. 2001).\nModeration: Examines whether the relationship between two variables changes at different levels of a third variable. Moderation Analysis"
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#examples-of-sem-in-nursing-research",
    "href": "Lectures/introduction-to-SEM.html#examples-of-sem-in-nursing-research",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "Scenario: Using SEM to model the relationships between patient characteristics, treatment variables, and health outcomes.\n\n\n\n\n\nScenario: Exploring whether patient satisfaction mediates the relationship between nurse communication and patient recovery."
  },
  {
    "objectID": "Lectures/introduction-to-SEM.html#software-examples",
    "href": "Lectures/introduction-to-SEM.html#software-examples",
    "title": "Introduction to Structural Equation Modeling (SEM)",
    "section": "",
    "text": "You can use the worland5.csv as an example. This hypothetical dataset examines the effects of student background on academic achievement. It contains 9 observed variables (Motivation, Harmony, Stability, Negative Parental Psychology, SES, Verbal IQ, Reading, Arithmetic and Spelling) and 3 hypothesized latent constructs (Adjustment, Risk, Achievement).\n\n\nJMP Instructions:\n\n\n\nGo to Analyze &gt; Multivariate Methods &gt; Structural Equation Modeling.\n\n\nSelect your variables and specify the model structure in the diagram window.\n\n\nUse the Add Path tool to draw paths between variables as hypothesized in your model.\n\n\nSpecify latent variables by selecting observed variables and defining them as indicators.\n\n\nClick Run to estimate the model.\n\n\nReview the output for path coefficients, model fit indices, and other diagnostics.\n\n\n\n\n\n\n\n\nR Code Example:\n\n\n\n        # Load necessary libraries\n        library(lavaan)\n        \n        mydata2&lt;-read.csv(\"worland5.csv\") # adjust the file path accordingly\n        \n        # Specify the SEM model\n        \n        model &lt;- '\n          # Measurement model\n          adjustment  =~ motiv + harm + stabi\n          risk =~ verbal + ppsych + ses\n          achievement =~ read + arith + spell\n          \n          # Structural model\n          achievement ~ adjustment+ risk\n        '\n        \n        # Fit the model\n        MySEM_model &lt;- sem(model, data=mydata2)\n        \n        # Summarize the results\n        summary(MySEM_model, fit.measures = TRUE)\n        \n        # visualize the SEM model\n        library(semPlot)\n\n         # Visualize the path diagram\n        semPaths(MySEM_model, what = \"std\", edge.label.cex = 0.8, layout = \"tree\")"
  }
]